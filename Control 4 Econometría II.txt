Universidad de Lima
Facultad de Ciencias Empresariales y Económicas
Carrera de Economía

Resumen Control 4: Lecturas semanas 10-13
Curso: Econometría 2

“He insistido en que la econometría debe tener relevancia en realidades
concretas, de lo contrario degenera en algo que no merece el nombre de
econometría, sino que más bien debería llamarse 'jugometría.”
-Ragnar Frisch.

Profesor
Jose Luis Nolazco Cama
Julio de 2024

Lima – Perú

INTRODUCTION (BALTAGI CAP 1)
1.1. PANEL DATA: ALGUNOS EJEMPLOS
os datos de panel se refieren a la combinación de observaciones sobre un mismo grupo de individuos,
países, empresas, etc., a lo largo de varios periodos de tiempo. Este tipo de datos permite seguir a los
mismos sujetos a lo largo del tiempo, proporcionando una visión más rica y detallada de los fenómenos
estudiados en comparación con estudios transversales o de series temporales puras. A continuación, se
comentarán ciertos estudios de ejemplo.
En Estados Unidos, los datos de panel incluyen estudios significativos como el Panel Study of Income
Dynamics (PSID), iniciado en 1968, que sigue a familias y sus descendientes para recoger datos sobre
ingresos, empleo y salud, cubriendo más de 65,000 individuos hasta 2003 con entrevistas anuales hasta
1996 y bianuales desde 1997. El National Longitudinal Surveys (NLS) documenta transiciones
importantes en la vida de jóvenes, incluyendo grupos como NLSY97 y NLSY79, y ha estudiado a hijos
biológicos de mujeres desde 1967. La Current Population Survey (CPS), realizada mensualmente por la
Oficina del Censo, tiene una muestra más grande y representativa, aunque con menos variables y menor
cobertura temporal comparada con PSID y NLS, proporcionando datos extensivos durante más de 50
años.
En Europa, ejemplos destacados de datos de panel incluyen el German Socio-Economic Panel (GSOEP),
iniciado en 1984, que recopila datos demográficos y socioeconómicos en Alemania Occidental y
Oriental desde 1990, con una baja tasa de desgaste. El British Household Panel Survey (BHPS),
comenzado en 1991, recoge anualmente datos sobre demografía, mercado laboral y salud en alrededor
de 5500 hogares británicos. El European Community Household Panel (ECHP), coordinado por Eurostat
desde 1994, cubre varios países de la UE y proporciona información comparable sobre ingresos, empleo
y pobreza, vinculándose a paneles nacionales existentes o corriendo en paralelo a ellos para obtener
datos homogéneos entre los países miembros.
1.2 ¿POR QUÉ DEBERIAMOS USAR PANEL DATA? SUS BENEFICIOS Y LIMITACIONES
Beneficios de los Datos de Panel
1. Control de Heterogeneidad Individual: Permiten controlar efectos específicos de individuos,
empresas o países que no varían en el tiempo, evitando sesgos. Por ejemplo, en un estudio sobre la
demanda de cigarrillos, se pueden controlar variables como la religión o el nivel educativo.
2. Datos Más Informativos: Ofrecen mayor variabilidad y menos colinealidad entre las variables, con
más grados de libertad y eficiencia en las estimaciones. Permiten descomponer la variabilidad en
variación entre individuos y dentro de los individuos a lo largo del tiempo.
3. Estudio de Dinámicas de Ajuste: Permiten analizar la duración de estados económicos como el
desempleo y la pobreza, evaluando la duración y velocidad de los cambios económicos. Adecuados para
estudiar la duración del desempleo y la movilidad residencial e ingresos.
4. Identificación de Efectos No Detectables en Datos Transversales o Temporales Puros: Permiten
observar cambios individuales en el tiempo, como determinar si la membresía sindical afecta los salarios
o diferenciar patrones de participación laboral.
5. Modelos Comportamentales Más Complejos: Facilitan la construcción y prueba de modelos de
comportamiento más complejos que los datos transversales o de series temporales puras, como estudiar
la eficiencia técnica de las empresas a lo largo del tiempo.
6. Medición Más Precisa a Nivel Micro: Los datos de panel a nivel micro son más precisos que las
variables medidas a nivel macro, reduciendo o eliminando sesgos de agregación de datos.
7. Pruebas de Raíces Unitarias y Cointegración: Permiten realizar pruebas de raíces unitarias y
cointegración con distribuciones asintóticas estándar, una ventaja sobre las pruebas en series temporales
puras.

Limitaciones de los Datos de Panel
1. Problemas de Diseño y Recopilación de Datos: Incluyen problemas de cobertura, no respuesta,
errores de memoria, frecuencia de entrevistas y manejo de datos.
2. Errores de Medición: Respuestas inexactas debido a preguntas poco claras, distorsión deliberada,
informantes inapropiados y errores de registro por parte de los entrevistadores.
3. Problemas de Selectividad: Autoselección, donde los individuos eligen no participar o trabajar,
introduciendo sesgos en las muestras. La falta de cooperación de los encuestados resulta en no respuesta
y pérdida de datos, mientras que la atrición, o pérdida de participantes a lo largo del tiempo, puede
afectar la representatividad del panel.
4. Dimensión Temporal Corta: Los paneles de microdatos suelen cubrir periodos cortos, limitando los
análisis a largo plazo. Aumentar el periodo de tiempo puede incrementar la atrición y la dificultad
computacional para modelos con variables dependientes limitadas.
5. Dependencia Transversal: Los paneles macro que no consideran la dependencia entre países pueden
conducir a inferencias erróneas. Es crucial considerar la dependencia transversal en paneles macro para
obtener conclusiones válidas.

THE ONE-WAY ERROR COMPONENT REGRESSION MODEL (BALTAGI CAP 2)
2.1. INTRODUCCIÓN
Una regresión con datos de panel se diferencia de una regresión típica de series temporales o de corte
transversal porque tiene un doble subíndice en sus variables:

Aquí, 𝑖 denota hogares, individuos, empresas, países, etc., y 𝑡 denota tiempo. Por lo tanto, 𝑖 representa
la dimensión de corte transversal, mientras que t representa la dimensión de series temporales. En esta
ecuación, 𝛼 es un escalar, 𝛽 es un vector de 𝐾 × 1𝐾, y 𝑋𝑖𝑡 es la observación 𝑖𝑡 sobre 𝐾 variables
explicativas.
Modelo de Error de Componente Único
La mayoría de las aplicaciones de datos de panel utilizan un modelo de error de componente único para
las perturbaciones, como en el caso de 𝑦𝑖𝑡 de arriba, este error decomponente único se expresa así:
𝒖𝒊𝒕 = 𝝁𝒊 + 𝝂𝒊𝒕 (𝟐. 𝟐)
Donde 𝜇𝑖 denota el efecto específico no observable de cada individuo y 𝑣𝑖𝑡 denota la perturbación
residual.
Por ejemplo, en una ecuación de ingresos en economía laboral, 𝑦𝑖𝑡 medirá los ingresos del jefe del hogar,
mientras que 𝑋𝑖𝑡 puede contener variables como experiencia, educación, sexo, raza, etc. En este caso,
𝜇𝑖 sería invariante en el tiempo y representa cualquier efecto específico del individuo que no esté
incluido en la regresión, como la habilidad no observada del individuo. Mientras que la perturbación
residual 𝜈𝑖𝑡 varía entre individuos y tiempo y puede ser vista como la perturbación usual en la regresión.
Representación en Forma Vectorial
La ecuación (2.1) puede ser escrita en forma vectorial como:

Donde 𝑦 es un vector de dimensión 𝑁𝑇 × 1, 𝑋 es una matriz de dimensión 𝑁𝑇 × 𝐾, 𝑍 = [𝜄𝑁𝑇, 𝑋], 𝛿´ =
(𝛼´, 𝛽´) y 𝜄𝑁𝑇 es un vector de unos de dimensión 𝑁𝑇.
La ecuación (2.2) puede ser reescrita como:

Con las ecuaciones anteriores (2.4 y 2.3) se puede encontrar P y Q, los cuales se utilizan para manejar
los datos de manera efectiva:
•
•

−1

La matriz P: definida como: 𝑃 = 𝑍𝜇 (𝑍𝜇′ 𝑍𝜇 ) 𝑍𝜇′ (dónde 𝑍𝜇 es la matriz de dummys.) Promedia
las observaciones a lo largo del tiempo para cada individuo.
La matriz Q: definida como: 𝑄 = 𝐼𝑁𝑇 − 𝑃, obtiene las desviaciones de las observaciones
respecto a sus medias individuales.

Ambas matrices son idempotentes y simétricas (si se multiplica por la misma, sale la misma matriz) y
la suma de sus rangos es igual a su traza (𝑟𝑎𝑛𝑘(𝑃) = 𝑡𝑟(𝑃) = 𝑁) y 𝑟𝑎𝑛𝑘(𝑄) = 𝑡𝑟(𝑄) = 𝑁(𝑇 − 1)
Además, 𝑃 y 𝑄 son ortogonales (𝑃𝑄 = 0), y su suma es la matriz identidad (𝑃 + 𝑄 = 𝐼𝑁𝑇).
2.2 EL MODELO DE EFECTOS FIJOS
En el análisis de datos de panel, el modelo de efectos fijos asume que los 𝜇𝑖 son parámetros fijos para
estimar y que las perturbaciones residuales 𝜈𝑖𝑡 son independientes e idénticamente distribuidas (IID)
con media cero y varianza 𝜎 2 . Las variables 𝑋𝑖𝑡 se asumen independientes de 𝜈𝑖𝑡 para todos los 𝑖 y 𝑡.

Este modelo es adecuado cuando se analiza un conjunto específico de 𝑁 unidades (como empresas o
países) y el análisis se limita a este conjunto particular.
Se menciona la siguiente regresión:

y promediando en el tiempo se obtiene:

Por lo tanto, restando (2.9) de (2.8) se obtiene:

Además, el promedio de todas las observaciones en (2.8) da como resultado:

Se impone una restricción ∑𝑁
𝑖=1 𝜇𝑖 = 0 para evitar la multicolinealidad perfecta al incluir variables
dummy. Esta restricción permite estimar 𝛽 y (𝛼 + 𝜇𝑖 ) pero no 𝛼 y 𝜇𝑖 por separado. El estimador de
efectos fijos (FE), también conocido como mínimos cuadrados con variables dummy (LSDV), puede
tener problemas cuando el número de individuos N es grande, debido a la pérdida de grados de libertad
y la multicolinealidad entre los regresores.
Para estimar los parámetros del modelo, se realiza una regresión de mínimos cuadrados ordinarios (OLS)
en un modelo transformado que elimina los efectos individuales, conocida como la transformación
“within”, que permite obtener estimaciones consistentes de los parámetros de interés (𝛼 𝑦 𝛽) al reducir
la dimensionalidad del problema.
El modelo también permite la obtención de estimaciones robustas de los errores estándar utilizando
métodos como los propuestos por Arellano, que consiste en apilar el panel como una ecuación para cada
individuo y luego realizar la transformación Within en estas ecuaciones, obteniendo el estimador Within
de 𝛽 con distribución asintótica.
El modelo de efectos fijos es especialmente útil cuando se trata de paneles grandes, ya que puede
manejar la presencia de muchos efectos individuales (𝜇𝑖 ) y proporciona estimaciones consistentes
siempre que las perturbaciones sigan las suposiciones clásicas. Sin embargo, no puede estimar el efecto
de variables invariantes en el tiempo, como el sexo o la raza, ya que estas variables son eliminadas por
la transformación dentro, y si se aplica un MCO con un modelo de efectos fijos sin incluir las dummies
individuales se tendrían estimaciones sesgadas e inconsistentes por omisión de variables relevantes
Para verificar la presencia de efectos fijos, se puede realizar una prueba F de Chow para la significancia
conjunta de las variables ficticias individuales, que compara modelos con suma de cuadrados
restringidas (RRSS) y no restringidas (URSS) para determinar la importancia de los efectos fijos.

Asimismo, se debe de considerar que Al usar la regresión Within (2.10), es necesario ajustar las
varianzas obtenidas multiplicando la matriz de varianza-covarianza por: 𝑠 ∗ 2/𝑠 2
2.3 EL MODELO DE EFECTOS ALEATORIOS
El modelo de efectos aleatorios es una alternativa al modelo de efectos fijos, especialmente útil cuando
se trabaja con grandes paneles de datos. En lugar de tratar los efectos individuales (𝜇𝑖 ) como constantes,
se asumen como aleatorios y se distribuyen como IID, mientras que las perturbaciones residuales (𝜈𝑖𝑡)
se distribuyen como IID, y ambos son independientes entre sí. Además, las variables explicativas (𝑋𝑖𝑡 )
son independientes de 𝜇𝑖 y 𝜈𝑖𝑡 para todos los individuos y períodos de tiempo.

Este modelo es adecuado cuando los individuos se seleccionan aleatoriamente de una población grande,
lo cual es común en estudios de panel de hogares. En estos estudios, el número de individuos (N) es
generalmente grande, y usar un modelo de efectos fijos podría resultar en una significativa pérdida de
grados de libertad. El modelo de efectos aleatorios permite hacer inferencias sobre la población de la
cual se ha extraído la muestra.
Según Nerlove y Balestra (1996), la población no consiste en un número infinito de individuos, sino en
un número infinito de decisiones que cada individuo podría tomar, por lo que las perturbaciones de un
mismo individuo están correlacionadas a lo largo del tiempo, pero no entre diferentes individuos.
Asimismo, se puede usar el método simplificado de Wansbeek y Kapteyn, que se basa en simplificar los
cálculos usando una descomposición de la matriz de varianza-covarianza en componentes más
manejables, descomponiéndola en dos partes: Una que captura la varianza de los efectos aleatorios y
otra que captura la varianza de los errores residuales.
Varianza y Covarianza: En el modelo de efectos aleatorios, la varianza de las perturbaciones es la
misma para todos los individuos y períodos de tiempo. Además, hay una correlación entre las
perturbaciones de un mismo individuo a lo largo del tiempo. Esta estructura de correlación permite
descomponer la matriz de varianza-covarianza en dos componentes principales, lo que facilita su
inversión.
Fuller y Battese sugieren transformar la ecuación de regresión original y luego aplicar mínimos
cuadrados ordinarios (OLS) a la ecuación transformada. Esto facilita la obtención de estimaciones
precisas de los coeficientes de regresión. Los estimadores cuadráticos insesgados de las varianzas
permiten calcular eficientemente la varianza de los efectos aleatorios y de los errores residuales.
Comparado con otros métodos, el estimador GLS basado en componentes de varianza es el mejor
estimador lineal insesgado (BLUE) y es más eficiente en grandes paneles de datos. Estudios como los
de Maddala y Mount, y experimentos de Monte Carlo han demostrado que los distintos estimadores
GLS factibles tienen un desempeño comparable y son recomendables para su uso práctico.
Taylor (1980) comparó el estimador Within con el estimador GLS factible de Swamy-Arora y encontró
que el GLS factible es generalmente más eficiente que los mínimos cuadrados con variables dummy
(LSDV) para casi todos los grados de libertad, y su varianza nunca supera en más del 17% al límite
inferior de Cramer-Rao. Sin embargo, estimadores más eficientes de los componentes de varianza no
siempre conducen a estimadores GLS factibles más eficientes. Estos resultados fueron confirmados por
experimentos de Monte Carlo realizados por Maddala y Mount (1973) y Baltagi (1981a). Bellmann,
Breitung y Wagner (1989) investigaron el sesgo en la estimación de componentes de varianza y
coeficientes de regresión, encontrando un sesgo considerable en la estimación de 𝜎𝜇2 en un panel a nivel
de industria, pero no afectando significativamente los coeficientes de regresión.
2.3.1 Fijo vs Aleatorio
La elección entre los modelos de efectos fijos y aleatorios ha sido un tema de debate en biometría,
estadísticas y econometría de datos de panel. Mundlak (1961) y Wallace y Hussain (1969) apoyaron los
efectos fijos, mientras que Balestra y Nerlove (1966) prefirieron los efectos aleatorios. Hausman (1978)
propuso una prueba para diferenciar entre ambos modelos, pero se ha malinterpretado, ya que algunos
investigadores creen que un rechazo implica adoptar el modelo de efectos fijos y no rechazo, el de
efectos aleatorios.
Chamberlain (1984) demostró que los efectos fijos imponen restricciones testables en los parámetros,
que deben verificarse. Mundlak (1978) señaló que los efectos aleatorios asumen exogeneidad de todos
los regresores con los efectos individuales, mientras que los efectos fijos permiten endogeneidad.
Hausman y Taylor (1981) sugirieron un enfoque intermedio, donde algunos regresores pueden estar
correlacionados con los efectos individuales. Para los investigadores aplicados, es esencial no solo
realizar pruebas básicas, sino también verificar las restricciones del modelo de efectos fijos y considerar
la especificación de Hausman y Taylor como una alternativa viable.

2.4 MAXIMUM LIKELIHOOD ESTIMATION
Bajo la normalidad de las perturbaciones, se puede escribir la función de verosimilitud como:

Donde
En la función de probabilidad encontramos

:

Breusch: Muestra que este parámetro tiene una propiedad notable de formar una secuencia monótona.
A partir del estimador “within” se consigue una secuencia monótona creciente. Con el estimador
“Between” se consigue una secuencia monótona decreciente. Por ello debemos protegernos de un
máximo, para esto comenzamos con 𝛽̃𝑊𝑖𝑡ℎ𝑖𝑛 y 𝛽̂𝐵𝑒𝑡𝑤𝑒𝑒𝑛 e iteramos.
Maddala: Encuentra que hay como máximo dos “máximos” para la probabilidad 0 < 𝜙 2 ≤ 1, por ello
debemos protegernos de un máximo local.
2.5 PREDICTION
Para predecir 𝑆 periodos futuros en un modelo de efectos aleatorios, Goldberger (1962) desarrolló un
método llamado Mejor Predictor Lineal Insesgado (BLUP). Este método utiliza la estructura de
varianza-covarianza de las perturbaciones y ajusta las predicciones con una fracción de los residuales
de los modelos GLS. Baillie y Baltagi (1999) analizaron diferentes métodos de predicción cuando los
componentes de varianza no son conocidos y evaluaron cuatro enfoques:
1.
2.
3.
4.

Un predictor ordinario que usa estimaciones de máxima verosimilitud (MLE).
Un predictor truncado que ignora la corrección del error.
Un predictor mal especificado que usa estimaciones OLS.
Un predictor de efectos fijos que asume que los efectos individuales son parámetros fijos.

Sus hallazgos indican que el predictor ordinario es superior en términos de error cuadrático medio
(MSE) en comparación con los predictores truncado y mal especificado, especialmente cuando la
proporción de varianza de los efectos individuales es alta. El predictor de efectos fijos también mostró
un buen desempeño, siendo casi tan efectivo como el predictor ordinario. En general, tanto el predictor
ordinario como el de efectos fijos superaron a los otros métodos y se recomiendan en las investigaciones.
En la práctica, es crucial considerar los efectos individuales al hacer predicciones.
2.6. EJEMPLOS
Ejemplo 1: Ecuación de inversión de Grunfeld
Se consideró la siguiente ecuación:

𝐼𝑖𝑡 = 𝐼𝑛𝑣𝑒𝑟𝑠𝑖ó𝑛 𝑏𝑟𝑢𝑡𝑎 𝑟𝑒𝑎𝑙 𝑑𝑒 𝑙𝑎 𝑒𝑚𝑝𝑟𝑒𝑠𝑎 𝑖 𝑒𝑛 𝑒𝑙 𝑎ñ𝑜 𝑡
𝐹𝑖𝑡 = 𝑉𝑎𝑙𝑜𝑟 𝑟𝑒𝑎𝑙 𝑑𝑒 𝑙𝑎 𝑒𝑚𝑝𝑟𝑒𝑠𝑎 (𝑎𝑐𝑐𝑖𝑜𝑛𝑒𝑠 𝑒𝑛 𝑐𝑖𝑟𝑐𝑢𝑙𝑎𝑐𝑖ó𝑛)
𝐶𝑖𝑡 = 𝑉𝑎𝑙𝑜𝑟 𝑟𝑒𝑎𝑙 𝑑𝑒𝑙 𝑐𝑎𝑝𝑖𝑡𝑎𝑙 𝑠𝑜𝑐𝑖𝑎𝑙
Los datos son de 10 empresas grandes manufactureras durante 20 años (1935-1954) La tabla 2.1 muestra
los OLS, Between y Whitin estimadores.

Los resultados de OLS y GLS son combinaciones ponderadas por matriz de estos estimadores. Los GLS
factibles de la regresión son WALHUS, AMEMIYA y SWAR junto sus estimaciones correspondientes
𝜌 , 𝜎𝑢 , 𝜎𝑣 . Por otro lado, tenemos la estimación de máxima verosimilitud iterativa de Breusch (1987)
(IMLE) donde converge a un máximo global de 3 o cuatro iteraciones dependiendo del estimador.
La tabla 2.2 muestra al estimador Wallace and Hussain (1969) como una opción dentro del
procedimiento de datos de panel de efectos aleatorios.

La tabla 2.3 muestra el procedimiento Amemiya (1971), que se denomina Wansbeek and Kapteyn (1989)
por tratar con paneles desequilibrados o incompletos. Asimismo, la tabla 2.4 presenta el procedimiento
de Swamy y Arora (1972). Donde 𝜎𝑣 es 52.77 y ρ es 0.72 por ello θ es 0.86

La estimación de máxima verosimilitud iterativa de Breusch (1987) se realizó, convergiendo a un
máximo global en tres a cuatro iteraciones dependiendo del punto de inicio (estimadores Between o
Within). No hay mucha diferencia entre las estimaciones GLS factibles y la MLE iterativa; todas están
cerca de las estimaciones Within, lo cual es comprensible dado que θ para estos estimadores es cercano
a 1.
Ejemplo 2: Demanda de Gasolina
Baltagi y Griffin (1983) consideraron la siguiente ecuación de demanda de gasolina

Donde Gas/car es el consumo de gasolina de motor por automóvil, Y/N es el ingreso real percapita,
PMG/PGDP es el precio real de la gasolina para automóviles y Car/N denota la cantidad de

automóviles per cápita. La muestra son observaciones anuales en 18 países de la OCDE (1960-1978).
La Tabla 2.5 presenta las estimaciones de parámetros para OLS, Between, Within y tres estimaciones
GLS factibles de los coeficientes y las estimaciones correspondientes de ρ, σµ y σν.

Además, la máxima verosimilitud iterativa de Breusch (1987) convergió a un máximo global en cuatro
a seis iteraciones. El procedimiento SWAR, σµ = 0.196, σν = 0.092, ρ = 0.82 y θ = 0.89. Una vez más,
las estimaciones de θ están más cerca de 1 que de 0, lo que explica por qué GLS factible está más
cerca del estimador Within que del estimador OLS.
Ejemplo 3: Productividad del capital público
Munnell (1990), Baltagi y Pinnoi (1995) consideraron la siguiente relación de función de producción
Cobb-Douglas que investiga la productividad del capital público en la producción privada:

En este ejemplo se analiza el producto bruto estatal (Y) de 48 estados contiguos de EE.UU. durante
1970-1986, considerando:
•
•
•
•

K1 (capital público, incluyendo carreteras, calles, instalaciones de agua y alcantarillado, y otros
edificios y estructuras públicas)
K2 (capital privado, basado en estimaciones nacionales del Bureau of Economic Analysis)
L (insumo laboral medido como empleo en nóminas no agrícolas)
Unemp (tasa de desempleo estatal para capturar los efectos del ciclo económico).

La Tabla 2.6 muestra estimaciones de un modelo de componente de error unidireccional, donde los
estimadores OLS y Between indican que el capital público es productivo y significativo, mientras que
los estimadores Within y GLS factibles lo consideran insignificante, como también encontró HoltzEakin (1994).

Las Tablas 2.7 y 2.8 reproducen las estimaciones Between y Within usando Stata, destacando una
prueba F significativa para los efectos estatales, indicando que las estimaciones OLS sufren de omisión
de variables. Se trata de la prueba F descrita en (2.12). Comprueba si todos los coeficientes de las

variables ficticias estatales son iguales y, en este caso, arroja un F(47.764) = 75,82, que es
estadísticamente significativo. Esto indica que las variables ficticias estatales son significativas
conjuntamente.

La Tabla 2.9 presenta el estimador de efectos aleatorios de Swamy y Arora (1972) y la Tabla 2.10
muestra el estimador de máxima verosimilitud, ambos obtenidos con Stata. Estos resultados resaltan la
importancia de considerar los efectos específicos del estado y elegir el método de estimación adecuado
en análisis de datos de panel.

Nota: Fíjense que uno utiliza el comando “re” y el otro el comando “mle”

THE TWO-WAY ERROR COMPONENT REGRESSION MODEL (BALTAGI CAP 3)
3.1 INTRODUCCIÓN
Wallace y Hussain (1969), Nerlove (1971b) y Amemiya (1971), entre otros, consideraron el modelo de
regresión dado por (2.1), pero con perturbaciones de componentes de error bidireccional.

donde 𝜇𝑖 representa el efecto individual no observable, 𝜆𝑡 el efecto temporal no observable, y 𝜈𝑖𝑡 el
término de perturbación residual. 𝜆𝑡 es invariante para el individuo y captura efectos específicos del
tiempo no incluidos en la regresión, por ejemplo, huelgas, embargos de petróleo o leyes contra el tabaco
que pueden afectar el comportamiento de consumo.
De forma vectorial, esto se representa como:

3.2 EL MODELO DE FIJACIÓN DE ERRORES:
El modelo de fijación de errores para datos de panel estima parámetros fijos 𝜇𝑖 y 𝜆𝑡 y maneja
perturbaciones estocásticas 𝜈𝑡 que son independientes e idénticamente distribuidas (IID). La inferencia
es específica para los individuos y periodos de tiempo observados.
Usar muchas variables dummy en regresiones grandes causa pérdida de grados de libertad y
multicolinealidad. En vez de invertir grandes matrices, se utiliza la transformación "Within" para obtener
estimaciones de efectos fijos, eliminando diferencias individuales.
El estimador Within no puede estimar variables invariantes en el tiempo e individuos, ya que la
transformación 𝑄 las elimina. Las estimaciones por OLS son sesgadas si ignoran los efectos fijos
bidireccionales.
La ecuación muestra la transformación 𝑄:

Esta transformación elimina las medias individuales y temporales, aislando las variaciones dentro de
cada unidad observacional y periodo.
3.2.1 Testeo para errores fijos
Para realizar el testeo de errores fijos y la significancia conjunta de las variables dummy, se pueden
probar dos hipótesis:
Hipótesis Nula: Todos los efectos individuales y de tiempo son cero

Hipótesis Alternativa: Solo los efectos individuales son cero, permitiendo efectos de tiempo:

Las sumas de cuadrados residuales restringidas (RSS) se obtienen usando el modelo de mínimos
cuadrados ordinarios agrupados (pooled OLS), mientras que las sumas de cuadrados residuales no
restringidas (URSS) provienen de la regresión Within.
El URSS es la suma de los residuos al cuadrado del estimador Within, mientras que el RSS se calcula
usando solo las dummies de series temporales. La regresión usada para esto es:

El RRSS se obtiene de la regresión en (2.10), mientras que el URSS se obtiene de la regresión completa.:

En este caso, el estadístico F resultante es 𝐹3~𝐻0 𝐹(𝑡 − 1), (𝑁 − 1)(𝑡 − 1) − 𝐾.
3.3 EL MODELO DE EFECTOS ALEATORIOS
El modelo de efectos aleatorios asume que 𝜇𝑖 , 𝜆𝑡 y 𝜈𝑖𝑡 son independientes entre sí e IID. Además, 𝑋𝑖𝑡
es independiente de 𝜇𝑖 , 𝜆𝑡 y 𝑣𝑖𝑡 para todo 𝑖 y 𝑡. La inferencia en este caso se refiere a la gran población
de la cual se extrajo esta muestra aleatoriamente. A partir de (3.2), se puede calcular la matriz de
varianza-covarianza.

Para obtener esta matriz, se reemplazan términos por sus correspondientes matrices y se reordenan,
llegando a:

Aquí, 𝜆𝑖 son las raíces características distintivas de Ω y los 𝑄𝑖 son las matrices de correspondencia de
eigenprojcetors.
Estimaciones GLS: Las ecuaciones para la transformación y el GLS (Generalized Least Squares) son:

Donde 𝜃 son coeficientes basados en 𝜎 y 𝜆. El GLS se puede obtener de 𝑦 ∗ y 𝑍 ∗ :

Estimadores Cuadráticos Insesgados(BQU): Los mejores estimadores cuadráticos insesgados (BQU)
de los componentes de varianza surgen naturalmente del hecho de que Qiu ~ (0, λiQi). Por lo tanto,

Es el mejor estimador cuadrático insesgado (BQU) de los componentes de varianza, bajo la normalidad
de las perturbaciones, son mínimos de varianza insesgados (MVU). Se pueden obtener estimaciones
factibles de los componentes de varianza reemplazando las perturbaciones verdaderas por residuos OLS
(Wallace y Hussain, 1969). Aunque OLS es insesgado y consistente bajo el modelo de efectos aleatorios,
es ineficiente y produce errores estándar y estadísticas t sesgadas. Alternativamente, se pueden usar los
residuos Within según el método de Amemiya (1971), que muestra que sus estimaciones de componentes
de varianza tienen la misma distribución asintótica que si se conocieran las perturbaciones verdaderas.
Sustituir los residuos OLS o Within en lugar de las perturbaciones verdaderas en (3.17) introduce sesgo
en las estimaciones de los componentes de varianza. Las correcciones de grados de libertad necesarias
dependen de las trazas de matrices que incluyen la matriz de regresores X.
Métodos Alternativos:
Swamy y Arora (1972) propusieron ejecutar tres regresiones de mínimos cuadrados y estimar los
componentes de varianza a partir de los errores cuadráticos medios de estas regresiones:
1. Regresión Within
2. Regresión entre individuos
3. Regresión entre periodos de tiempo
El estimador 𝛽𝐺𝐿𝑆 se calcula como la combinación ponderada de estos regresores
Baltagi (1981a) y otros estudios compararon estimadores OLS, Within y varios GLS factibles,
encontrando que:
•
•

OLS es sesgado e ineficiente.
Within es no sesgado y eficiente.

•

GLS es BLUE (mejor estimador lineal no sesgado) pero difícil de calcular en la práctica.

Swamy y Arora también hallaron que, para muestras pequeñas, SWAR es menos eficiente que OLS si
𝜎 2 es pequeño, y menos eficiente que Within si 𝜎 2 es grande.
En resumen, los modelos de efectos aleatorios permiten la estimación eficiente de parámetros cuando
se cumplen ciertas condiciones de independencia y normalidad, utilizando métodos como GLS y BQU
para manejar la varianza y corregir sesgos.
3.3.1 Experimento de Monte Carlo
Baltagi (1981a) consideró la siguiente ecuación de regresión simple

con
Dónde 𝜇𝑖 , 𝜆𝑡 y 𝜈𝑖𝑡 son independientes y distribuidos normalmente
Generación de datos y parámetros: En el experimento, se generaron datos para 𝑁 = 25, 𝑇 = 10, y
𝜔2 variando en el conjunto (0, 0.01, 0.2, 0.4, 0.6, 0.8) asegurando que (1 − 𝜌 − 𝜔) siempre sea positivo.
Se realizaron 100 replicaciones y los resultados fueron estos:
1. Modelo Bidireccional: Es difícil determinar si OLS es equivalente a GLS según el criterio de
MSE (error cuadrático medio), y la eliminación de una estimación negativa de varianza puede
afectar las estimaciones de los componentes de varianza.
2. Componentes de Varianza: Si estos componentes no son pequeños, hay una ganancia en MSE
al usar GLS factibles en lugar de mínimos cuadrados.
3. Métodos GLS de Dos Etapas: Todos funcionaron razonablemente bien según MSE, y ningún
método fue el mejor para todos los experimentos, con resultados similares en MSE.
4. Estimaciones de Componentes de Varianza: No necesariamente resultan en mejores
estimaciones de los coeficientes de regresión, y confirma resultados previos y extiende el
modelo de unidireccional a bidireccional.
Recomendación: La recomendación de Maddala y Mount (1973) sigue siendo válida. El GLS de dos
etapas corrige la heterocedasticidad y la correlación serial en los errores del modelo, mejorando la
eficiencia y precisión de las estimaciones en comparación con el método de mínimos cuadrados
ordinarios (MCO).
3.4 ESTIMACIÓN DE MÁXIMA VEROSIMILITUD
Para la estimación de máxima verosimilitud, se asume la normalidad en los errores estructurales. La
función de logverosimilitud es:

donde Ո y Ո−1 fueron dados en (3.13) y (3.14).
Los estimadores de MaxV de 𝑦, 𝜎𝑣2, 𝜎𝑚𝑢2, 𝑦 𝜎(λ)2 se obtienen haciendo lagrangiano a la ecuación
(3.29).
Aunque los u fueran observables, las ecuaciones serían altamente no lineales y difíciles de resolver
explícitamente.
3.5 PREDICCIÓN
¿Cómo se ve el mejor predictor lineal insesgado para el i-ésimo individuo, S períodos adelante para el
modelo de dos vías? De (3.1), para el período T+S

y

entonces, el elemento típico de

es

Después de algunos supuestos se llega a que, para el modelo de dos vías, si hay una constante en el
modelo, el BLUP (best linear unbiased prediction) para 𝑦𝑖,𝑇+𝑆 corrige la predicción GLS por una
fracción de la media de los residuos GLS correspondientes a ese i-ésimo individuo.

Esto se ve exactamente como el BLUP para el modelo de una vía, pero con un Ո diferente. Si no hay
una constante en el modelo, el último término en (3.44) debe ser reemplazado por (3.43).
3.6 EJEMPLOS
3.6.1 Ejemplo 1: Ecuación de Inversiones de Grunfeld
El ejemplo utiliza los datos del estudio de Grunfeld (1958) sobre la inversión corporativa, aplicando un
modelo de componentes de error de dos vías. Este tipo de modelo considera la variabilidad específica
tanto de las empresas (secciones transversales) como de los periodos de tiempo. La ecuación de
inversión se estima utilizando diferentes métodos de estimación, cada uno capturando de manera distinta
los componentes de error.
Tabla 3.1: Resultados de Componentes de Error de Dos Vías

Esta tabla presenta los resultados de varios métodos de estimación (OLS, Within, WALHUS,
AMEMIYA, SWAR e IMLE). Los puntos clave incluyen:
•
•
•

OLS: Coeficientes básicos (𝛽1 = 0.116, 𝛽2 = 0.231) pero con errores estándar sesgados.
Within: Utiliza efectos fijos, con coeficientes (𝛽1 = 0.118, 𝛽2 = 0.358)
Métodos de Efectos Aleatorios (WALHUS, AMEMIYA, SWAR, IMLE): Incluyen
componentes de varianza específicos para capturar la variabilidad entre empresas y periodos.
Por ejemplo, AMEMIYA muestra una varianza de efectos de tiempo (𝜎𝜆 = 15.78)

Tabla 3.2: Estimador de Efectos Fijos de Dos Vías

Esta tabla presenta los resultados de la estimación de efectos fijos utilizando mínimos cuadrados en
panel:
•
•
•

Coeficientes Significativos: 𝐹 y 𝐾 tienen coeficientes positivos y significativos, indicando un
impacto positivo en la inversión.
Constante Negativa: Refleja posibles costos fijos o iniciales altos.
Estadísticas de Ajuste: Alto 𝑅2 y 𝑅2 ajustado, indicando un buen ajuste del modelo a los datos.

Tabla 3.3: Estimador de Wallace y Hussain de Dos Vías

Esta tabla muestra los resultados del estimador de efectos aleatorios de Wallace y Hussain:
•
•
•

Coeficientes Positivos y Significativos: Para 𝐹 y 𝐾, similares a los efectos fijos.
Varianzas de Componentes de Error: 𝜎𝜇 (empresa), 𝜎𝜆 (tiempo) y 𝜎𝜈 (idiosincrática) capturan
diferentes fuentes de variabilidad.
Estadísticas de Ajuste: Menores que las del modelo de efectos fijos, pero aún razonables.

Tabla 3.4: Estimador de Amemiya/Wansbeek y Kapteyn de Dos Vías

Esta tabla presenta los resultados del estimador de efectos aleatorios de Amemiya, Wansbeek y Kapteyn:

•
•
•

Coeficientes Significativos: Para 𝐹 y 𝐾, con significancia similar a otros métodos.
Varianzas de Componentes de Error: Incluye un componente adicional de varianza,
mostrando cómo se distribuye la variabilidad entre las empresas, periodos y efectos
idiosincráticos.
Estadísticas de Ajuste: Buen desempeño, aunque ligeramente inferior al de los efectos fijos.

Conclusión: El análisis comparativo de estas tablas resalta la importancia de elegir el método adecuado
para manejar la variabilidad en los datos de panel. Los modelos de efectos fijos y aleatorios ofrecen
diferentes ventajas, con los efectos fijos proporcionando un mejor ajuste en este caso específico de la
ecuación de inversión de Grunfeld.
3.6.2 Ejemplo 2: Demanda de gasolina
La tabla 3.6 proporciona estimaciones OLS, Within, tres GLS factibles y MLE iterativas para los
coeficientes de pendiente. El estimador Within difiere drásticamente de OLS. Los métodos WALHUS y
SWAR arrojan estimaciones negativas de de σλ y se sustituye por cero. IMLE se obtiene utilizando TSP.

3.6.3 Ejemplo 3: Productividad del Capital Público
En este ejemplo, se utilizan los datos del estudio de Munnell (1990) sobre el capital público, analizados
por Baltagi y Pinnoi (1995), para estimar una función de producción Cobb-Douglas utilizando un
modelo de componentes de error de dos vías. Este modelo considera la variabilidad específica tanto de
las entidades (secciones transversales) como de los periodos de tiempo. Las estimaciones se realizan
utilizando diferentes métodos: OLS (Mínimos Cuadrados Ordinarios), Within (efectos fijos), y varios
estimadores de efectos aleatorios (GLS factibles y MLE iterativo).
Tabla 3.7: Resultados de Componentes de Error de Dos Vías para los Datos de Capital Público
La Tabla 3.7 muestra los resultados de varios métodos de estimación aplicados a los datos de
capital público. El método OLS, aunque proporciona coeficientes básicos, tiene errores estándar
sesgados. Los coeficientes para 𝛽1 (0.155) y 𝛽2 (0.309) son significativos, pero el coeficiente para el
capital público (𝛽4 = −0.007) es insignificante y muy pequeño. El método Within, que considera
efectos fijos por sección transversal y periodo, ajusta los coeficientes y muestra que el capital público
tiene un coeficiente insignificante (𝛽4 = −0.004). Los métodos de efectos aleatorios (WALHUS,
AMEMIYA, SWAR, IMLE) presentan componentes de varianza adicionales para capturar la
variabilidad entre entidades y periodos. Estos métodos muestran coeficientes de capital público (𝛽4 )
insignificantes y negativos en todos los casos, sugiriendo que el capital público no tiene un impacto
significativo en la producción en este conjunto de datos. Además, estos métodos no presentan
estimaciones negativas de los componentes de varianza, lo que indica una estimación robusta de los
componentes de error en el modelo.

TEST OF HYPOTHESES WITH PANEL DATA (BALTAGI CAP 4)
4.1. TESTS DE POOLABILIDAD DE LOS DATOS
En el análisis de datos de panel, surge la pregunta de si es apropiado o no agrupar (pool) los datos.
Agrupar los datos implica asumir que los parámetros de un modelo de comportamiento son los mismos
a lo largo del tiempo y entre diferentes regiones. Es importante determinar esto para obtener
estimaciones precisas y válidas en nuestro modelo.
Pruebas de Poolabilidad: Se deben realizar pruebas para determinar si es apropiado agrupar los datos.
Estas pruebas examinan si los parámetros de las ecuaciones de comportamiento varían entre regiones o
a lo largo del tiempo, o sea identifican si se pueden usar modelos más simples y agregados o si es
necesario considerar la heterogeneidad en los parámetros entre diferentes unidades o a lo largo del
tiempo
1. Modelo Restringido (Agrupado): Representa una ecuación de comportamiento con los mismos
parámetros a lo largo del tiempo y entre regiones: 𝑦 = 𝑍𝛿 + 𝑢, donde 𝑍 y 𝑢 son concatenaciones de las
matrices y vectores individuales de las regiones.
2. Modelo No Restringido: La misma ecuación de comportamiento, pero con parámetros diferentes a
lo largo del tiempo o entre regiones: 𝑦𝑖 = 𝑍𝑖 𝛿𝑖 + 𝑢𝑖 , dónde 𝑦𝑖 es el vector de observaciones, 𝑍𝑖 es una
matriz de variables explicativas, ∆𝑖 es un vector de parámetros específicos para cada región, y 𝑢𝑖 es el
vector de errores.
Hipótesis Nula de las pruebas (𝑯𝟎 ): Los parámetros son iguales para todas las regiones (𝛿𝑖 = 𝛿) para
todo 𝑖).
4.1.1 TEST DE POOLABILIDAD BAJO LA ASUNCIÓN 𝒖 ∼ 𝑵(𝟎, 𝝈𝟐 𝑰𝑵𝑻 )
Bajo esta asunción, el estimador insesgado de varianza mínima para 𝛿 en la ecuación (4.2) es el
estimador de mínimos cuadrados ordinarios (OLS) que coincide con el estimador de máxima
verosimilitud (MLE):
𝛿𝑂𝐿𝑆 = 𝛿𝑚𝑙𝑒 = (𝑍 ′ 𝑍)−1 𝑍′𝑦
El modelo restringido considera que los datos pueden ser agrupados: 𝑦 = 𝑍𝛿𝑂𝐿𝑆 + 𝑒, y el residuo se
define como 𝑒 = 𝑀𝑦, donde 𝑀 es una matriz de proyección.
El modelo no restringido considera que cada región 𝑖 tiene su propio modelo: 𝑦𝑖 = 𝑍𝑖 𝛿𝑖 + 𝑒𝑖 ,, y los
residuos de cada región se definirían como 𝑒𝑖 = 𝑀𝑖 𝑢𝑖
Test de Chow Extendido: Dividiendo las formas cuadráticas por sus respectivos grados de libertad y
tomando su relación, obtenemos el siguiente estadístico de prueba:

Bajo 𝐻0 , 𝐹𝑜𝑏𝑠 se distribuye como una 𝐹((𝑁 − 1)𝐾 ∗ , 𝑁(𝑇 − 𝐾 ∗ )). La región crítica para esta prueba es:

Cuestión 1: ¿Es el test de Chow el correcto si 𝒖~𝑵(𝟎, ∑)?
Respuesta: No, el test de Chow dado no es el correcto en este caso. Se necesita un test de Chow
generalizado.
Cuestión 2: ¿El estadístico de Chow sigue una distribución F bajo 𝒖~𝑵(𝟎, ∑)?
Respuesta: Toyoda (1974) mostró que el estadístico de Chow tiene una distribución F aproximada
cuando hay heterocedasticidad, pero la precisión depende de las verdaderas varianzas.

Conclusión: Si se asume 𝑢~𝑁(0, 𝜎 2 𝐼𝑁𝑇 ) y se desea agrupar los datos, se recomienda usar el test de
Chow. Sin embargo, en caso de heterocedasticidad o 𝑢~𝑁(0, ∑) es más adecuado usar un test de Chow
generalizado.
4.1.2 TEST DE POOLABILIDAD BAJO LA ASUNCIÓN GENERAL 𝒖 ∼ 𝑵(𝟎, Ω)
El test de poolabilidad bajo la asunción general de que 𝑢 sigue una distribución normal multivariada
𝑁(0, 𝛺) se utiliza para determinar si es válido agrupar los datos en un solo modelo o si deben ser tratados
por separado. Este test transforma el modelo de tal manera que las perturbaciones transformadas tienen
varianza homocedástica, permitiendo así la aplicación del test de Chow.
El modelo restringido se transforma para que las perturbaciones tengan varianza homocedástica:
1

Ω−2 𝑦 = 𝑦̃. El modelo restringido transformado es 𝑦̃ = 𝑍𝛿 + 𝑢̃,
1

Luego, en el modelo no restringido cada región 𝑖 tiene su propio modelo transformado Ω−2 𝑦𝑖 = 𝑦̃𝑖 . El
modelo no restringido transformado es 𝑦𝑖 = 𝑍𝑖 𝛿𝑖 + 𝑢̃𝑖
Estadístico de Prueba: Podemos probar 𝐻0 : 𝛿𝑖 = 𝛿 para cada 𝑖 = 1,2, … , 𝑁, usando el estadístico de
Chow en los modelos transformados:

Donde 𝑒 y 𝑒 ∗ son los residuos de los modelos transformados restringido y no restringido
respectivamente.
Aplicabilidad del Test: Este test es aplicable siempre y cuando Ω sea conocido. Si Ω es desconocido,
̂ , y se llama al estadístico resultante 𝐹̂𝑜𝑏𝑠
se reemplaza por un estimador consistente Ω
Motivación para la Poolabilidad: La poolabilidad de los datos permite aumentar la base de datos para
obtener estimaciones más precisas y confiables de los parámetros del modelo. El test de Chow ayuda a
determinar si es válido imponer la hipótesis 𝐻0 . Imponer restricciones verdaderas o falsas puede reducir
la varianza del estimador agrupado pero puede introducir sesgo si estas restricciones son falsas.
Evidencia de Monte Carlo: Baltagi (1981a) encontró que el test de Chow rechazaba frecuentemente la
hipótesis nula cuando era verdadera en un modelo de componentes de error. Esto se debía a que el test
de Chow es aplicable solo bajo la asunción de homocedasticidad, lo cual se viola bajo un modelo de
efectos aleatorios con grandes componentes de varianza. El test de Roy-Zellner, aplicable en caso 𝑢 ∼
𝑁(0, 𝛺), mostró menores frecuencias de error tipo I comparado con el test de Chow.
Alternativas y Comparaciones: Las alternativas al test de Chow incluyen los criterios de error
cuadrático medio (MSE) desarrollados por Toro-Vizcarrondo y Wallace (1968), que muestran
frecuencias de error tipo I más bajas y ofrecen una mayor precisión en la estimación de los parámetros.
El test de Roy-Zellner es otra alternativa robusta, especialmente útil en presencia de heterocedasticidad,
ya que presenta menores frecuencias de error tipo I comparado con el test de Chow. Además, las
extensiones de McElroy (1977) de los criterios MSE mejoran la precisión del test para perturbaciones
no esféricas, proporcionando resultados más confiables en situaciones con estructuras de varianza más
complejas.
4.1.3 EJEMPLOS
Ejemplo 1: Ecuación de Inversión de Grunfeld
1. Prueba de Chow para poolabilidad entre empresas: Se obtiene un estadístico F observado:
27.75, distribuido como 𝐹(27,170) bajo 𝐻0 : 𝛿𝑖 = 𝛿 para 𝑖 = 1, … , 𝑁. El resultado es que se
rechaza la poolabilidad entre empresas para todos los coeficientes.
2. Prueba de poolabilidad solo de las pendientes, permitiendo interceptos variables: Se usa un
modelo restringido basado en una regresión Within con dummies de empresa. Se obtiene un

estadístico F observado de 5.78 bajo la hipótesis nula, y se rechaza la poolabilidad de las
pendientes entre empresas.
3. Prueba de poolabilidad en el tiempo: Se obtiene un estadístico F observado: 1.12, y el resultado
es que no se rechaza la poolabilidad en el tiempo.
4. Prueba de Roy-Zellner para poolabilidad entre empresas, permitiendo un modelo de
componentes de error unidireccional: Se obtiene un estadístico F observado de 4.35, y el
resultado es que se rechaza la poolabilidad entre empresas.
5. Prueba de Roy-Zellner para poolabilidad en el tiempo, permitiendo un modelo de componentes
de error unidireccional: Se obtiene un estadístico F observado de 2.72, y el resultado es que se
rechaza la poolabilidad en el tiempo.
Ejemplo 2: Demanda de Gasolina
1. Prueba de Chow para poolabilidad entre países: Se obtiene un estadístico F observado de 129.38,
y se rechaza la poolabilidad entre países.
2. Prueba de estabilidad solo de los coeficientes de pendiente: Se obtiene un estadístico F
observado: 27.33, y el resultado es que se rechaza la estabilidad de los coeficientes de pendiente
entre países.
3. Prueba de Chow para poolabilidad en el tiempo: Se obtiene un estadístico F observado de 0.276,
y el resultado es que no se rechaza la poolabilidad en el tiempo.
4. Prueba de Roy-Zellner para poolabilidad entre países, permitiendo un modelo de componentes
de error unidireccional: Se obtiene un estadístico F observado de 21.64, y el resultado es que se
rechaza la poolabilidad entre países.
5. Prueba de Roy-Zellner para poolabilidad en el tiempo: Se obtiene un estadístico F observado de
1.66, y el resultado es que se rechaza la poolabilidad en el tiempo al 5%.
4.1.4 OTHER TESTS FOR POOLABILITY
Ziemer y Wetzstein (1983): Comparan estimadores agrupados (𝛿𝑂𝐿𝑆 ) con no agrupados (𝛿𝑖,𝑂𝐿𝑆 ) según
el rendimiento en el riesgo de pronóstico.Usan un modelo de demanda de recreación en áreas silvestres,
y encuentran que el estimador de regla de Stein tiene mejor rendimiento en el riesgo de pronóstico.
El estimador de Stein para 𝛿𝑖 es: 𝛿𝑖∗ = 𝛿𝑂𝐿𝑆 + (1 − 𝐹

𝑐

𝑜𝑏𝑠

) (𝛿𝑖,𝑂𝐿𝑆 − 𝛿𝑂𝐿𝑆 )

Maddala (1991): Argumenta que los estimadores de contracción son mejores que los estimadores
agrupados o individuales.
Brown, Durbin y Evans (1975): Desarrollan pruebas de suma acumulada y suma acumulada de
cuadrados para cambio estructural basadas en residuos recursivos en regresiones de series temporales.
Luego, Han y Park (1989) extienden estas pruebas al caso de datos de panel, y no encuentran evidencia
de cambio estructural en el comercio exterior de bienes manufacturados de EE.UU. durante 1958-76.
Baltagi, Hidalgo y Li (1996): Desarrollan una prueba no paramétrica para la agrupabilidad que es
robusta ante la especificación incorrecta de la forma funcional.
𝑦𝑖𝑡 = 𝑔𝑡 (𝑥𝑖𝑡 ) + 𝜖𝑖𝑡 , dónde 𝑔𝑡 (. ) puede variar con el tiempo. La prueba es consistente y asintóticamente
normal, y aplican la prueba a una ecuación de ingresos usando datos del PSID.
4.2. PRUEBAS DE EFECTOS INDIVIDUALES Y TEMPORALES
4.2.1. LA PRUEBA BREUSCH-PAGAN
El propósito de esta prueba es evaluar la hipótesis nula 𝐻0 : 𝜎𝜇2 = 𝜎𝜆2 = 0 (la presencia de efectos
individuales y temporales) en un modelo de error aleatorio de dos vías.
Modelo y Función de Verosimilitud: En un modelo de panel de efectos aleatorios de dos vías, estamos
interesados en saber si las variaciones individuales 𝜎𝜇2 y las variaciones temporales 𝜎𝜆2 son
significativamente diferentes de cero. Si ambos efectos son cero, entonces no hay variabilidad

significativa ni entre individuos ni a lo largo del tiempo, lo que simplificaría considerablemente el
modelo.
La función de verosimilitud bajo normalidad de las perturbaciones se expresa como:

Derivación del Test LM: Para construir el estadístico LM (Lagrange Multiplier), seguimos estos pasos:
1. Derivar el vector de puntaje (score): La derivada de la función de verosimilitud respecto a 𝜃,
evaluada en el estimador de máxima verosimilitud restringido bajo 𝐻0 , se usa para calcular el
vector de puntaje.
2. Calcular la matriz de información 𝑱(𝜽): Utilizamos esta matriz para evaluar la varianza del
estimador bajo 𝐻0
El estadístico LM se calcula como:

Donde 𝐷 es el vector de puntaje y 𝐽−1 es la inversa de la matriz de información. Bajo 𝐻0 , 𝐿𝑀 sigue una
distribución 𝜒22 .
Componentes del Test LM
𝑳𝑴𝟏 : Prueba la hipótesis nula 𝐻𝑎0 : 𝜎𝜇2 = 0 (efectos individuales son cero), aquí 𝐿𝑀1 seguiría una
distribución 𝜒12 , y este componente se calcula como:

𝑳𝑴𝟐 : Prueba la hipótesis nula 𝐻𝑏0 : 𝜎𝜆2 = 0 (efectos temporales son cero). Este componente seguiría una
distribución 𝜒12 y se calcula como:

Interpretación:
•
•
•

Si 𝑳𝑴 es significativo: Rechazamos 𝐻0 , indicando que al menos una de las varianzas (𝜎𝜇2 o 𝜎𝜆2 )
es significativamente diferente de cero. Esto sugiere que existen efectos individuales o efectos
temporales (o ambos) en el modelo.
Si 𝑳𝑴𝟏 es significativo: Rechazamos 𝐻𝑎0 , indicando que los efectos individuales 𝜎𝜇2 son
significativos.
Si 𝑳𝑴𝟐 es significativo: Rechazamos 𝐻𝑏0 , indicando que los efectos temporales 𝜎𝜆2 son
significativos.

Ventajas: Requiere solo residuos de MCO y es fácil de calcular, y es popular debido a su simplicidad y
efectividad en estudios de Monte Carlo.
Limitaciones: Puede haber problemas de rendimiento para valores pequeños de 𝜎𝜇2 o 𝜎𝜆2 cerca de cero,
donde es más probable obtener estimaciones negativas de los componentes de varianza.
4.2.2 PRUEBAS DE KING Y WU, HONDA Y MULTIPLICADOR DE LAGRANGE
ESTANDARIZADO

Esstas pruebas se usan para evaluar la presencia de efectos individuales y efectos temporales en un
modelo de componentes de error de dos vías. Por ello, la hipótesis nula es probar la ausencia de varianzas
significativas en: (1) Efectos individuales, (2) Efectos temporales, (3) Ambos:
𝐻𝑎0 : 𝜎𝜇2 = 0
𝐻𝑏0 : 𝜎𝜆2 = 0
𝐻𝑐0 : 𝜎𝜇2 = 𝜎𝜆2 = 0
Prueba de Honda: Honda (1985) sugiere una prueba de máxima potencia uniforme para 𝐻𝑎0 basada
en el estadístico:

Este estadístico es robusto a la no normalidad y es el cuadrado del estadístico LM1 de Breusch y Pagan.
Prueba de Multiplicador de Lagrange Estandarizado (SLM): Propuesta por Moulton y Randolph
(1989), esta prueba ajusta el estadístico LM para mejorar la aproximación asintótica en muestras
grandes. Se centra y escala el estadístico LM para obtener el SLM:

Pruebas de King y Wu: King y Wu (1997) proponen una prueba de potencia máxima media local
(LMMP) para 𝐻𝑎0 , coincidiendo con la prueba de Honda,

Combinación de Efectos Individuales y Temporales: Para 𝐻0𝑐 , Honda sugiere una prueba combinada:

King y Wu proponen una versión LMMP:

Interpretación: Estas pruebas ajustadas proporcionan una mejor aproximación crítica y son más
robustas en presencia de no normalidad y otros problemas en los datos.
•
•

Si 𝑯𝟎 , 𝑺𝑳𝑴, o 𝑲𝑾 son significativos: Rechazamos la hipótesis nula correspondiente,
indicando que hay efectos individuales, efectos temporales, o ambos en el modelo.
Si no son significativos: No podemos rechazar la hipótesis nula, sugiriendo que no hay
evidencia suficiente de efectos individuales o temporales significativos.

4.2.3. PRUEBAS DE GOURIEROUX, HOLLY AND MONFORT
Estas pruebas se usan para evaluar la presencia conjunta de efectos individuales y efectos temporales en
un modelo de componentes de error de dos vías, especialmente cuando uno o ambos componentes de
varianza son pequeños o cercanos a cero.
Hipótesis Nula: 𝐻𝑐0 : 𝜎𝜇2 = 𝜎𝜆2 = 0 , que prueba la ausencia de varianzas significativas en los efectos
individuales y temporales.
2 combinando dos estadísticos, 𝐴 y 𝐵, que están basados
Prueba GHM: Se construye el estadístico 𝜒𝑚
en la varianza de los efectos individuales y temporales, respectivamente:

•
•
•

2 se calcula como la suma de los cuadrados de 𝐴 y 𝐵.
Si ambos 𝐴 y 𝐵 son positivos, 𝜒𝑚
2 se calcula como el cuadrado del valor positivo.
Si solo uno de ellos es positivo, 𝜒𝑚
2 se establece en cero.
Si ambos son no positivos, 𝜒𝑚

Distribución del Estadístico 𝝌𝟐𝒎 : Bajo la hipótesis nula de que no hay varianza en los efectos
2 sigue una distribución mixta compuesta por
individuales ni en los efectos temporales (𝐻𝑐0 ), 𝜒𝑚
distribuciones ponderadas, dónde 1/4 del tiempo es 𝜒 2 (0), 1/2 del tiempo es 𝜒 2 (1) y 1/4 es 𝜒 2 (2)
Esta mezcla de distribuciones permite que la prueba GHM maneje casos en los que las varianzas de los
efectos son muy pequeñas o cercanas a cero, proporcionando una interpretación más robusta y confiable
en estos escenarios.
Ventajas de la Prueba GHM: A diferencia de las pruebas de Honda y KW, la prueba GHM es inmune
a valores negativos de 𝐴 y 𝐵, y es mucho más flexible, pues maneja situaciones donde uno o ambos
componentes de varianza son pequeños o cercanos a cero, ofreciendo una mejor interpretación y
confiabilidad en estos casos.
Interpretación:
•
•

Si 𝝌𝟐𝒎 es significativo: Rechazamos 𝐻𝑐0 , indicando que hay efectos individuales y/o temporales
significativos en el modelo.
Si 𝝌𝟐𝒎 no es significativo: No podemos rechazar 𝐻𝑐0 , sugiriendo que no hay evidencia
suficiente de efectos individuales o temporales significativos.

4.2.4. PRUEBAS LM CONDICIONALES
Hay un problema a la hora de usar la prueba 𝐻0 para 𝐻𝑎0 : 𝜎𝜇2 = 0, pues se asume que los efectos
específicos de tiempo no existen, lo cual puede llevar a decisiones incorrectas si la varianza de los
efectos temporales es grande. Por ello, se crea la prueba para efectos individuales condicionales
Prueba para Efectos Individuales Condicionales (𝑯𝒅𝟎 : 𝝈𝟐𝝁 = 𝟎)
El objetivo de esta prueba es testear efectos individuales permitiendo la existencia de efectos específicos
de tiempo (𝝈𝟐𝝀 > 𝟎). El estadístico de prueba es:

Bajo 𝐻𝑑0 , 𝐿𝑀𝜇 sigue una distribución 𝑁(0,1), y si 𝜎̂𝜆2 → 0 entonces 𝜎̂22 → 𝜎̂𝜈 y 𝐿𝑀𝜇 se aproximaría a
la prueba unilateral de Honda.
Prueba para Efectos Temporales Condicionales 𝑯𝒆𝟎 : 𝝈𝟐𝝀 = 𝟎
Esta prueba se creó para testear efectos temporales permitiendo la existencia de efectos específicos de
individuos 𝜎𝜇2 > 0. El estadístico de prueba sigue una distribución 𝑁(0,1) bajo la hipótesis nula, y es:

Ambas pruebas permiten testar efectos individuales o temporales considerando la existencia del otro
tipo de efecto, lo cual evita decisiones incorrectas debido a la omisión de uno de los componentes de
varianza, y se usan en contextos donde se necesita verificar la significancia de efectos individuales y
temporales, proporcionando un enfoque más robusto comparado con pruebas unilaterales que pueden
subestimar la complejidad del modelo.
4.2.5. ANOVA F Y PRUEBAS DE RAZÓN DE VEROSIMILITUD

ANOVA F-Test: Su objetivo es evaluar la significancia de los efectos fijos en el modelo de componentes
de error de una vía. Esta prueba determina si las variables explicativas tienen un impacto significativo
sobre la variable dependiente.
Su estadístico de prueba sigue una distribución 𝐹 central con 𝑝 − 𝑟 y 𝑁𝑇 − (𝑘 + 𝑝 − 𝑟) grados de
libertad, es:

Dónde 𝐷 es la matriz de proyección, 𝑀 y las matrices de proyección ajustadas, 𝑝 es el número de
parámetros en el modelo, 𝑟 es el rango de la matriz de diseño ajustada, y 𝑁𝑇 es el número total de
observaciones Los componentes esenciales del estadístico (𝐷, 𝑀, 𝐺) aseguran que la prueba sea válida
bajo la estructura del modelo especificado.
Pruebas de Razón de Verosimilitud (LR): Se usan para comparar valores de verosimilitud máxima
restringidos y no restringidos para evaluar hipótesis específicas sobre los componentes de varianza del
modelo. Su estadístico de prueba es:

Dónde 𝑙(𝑟𝑒𝑠) es el valor de verosimilitud máxima restringido (bajo la hipótesis nula), y 𝑙(𝑢𝑛𝑟𝑒𝑠) es el
valor de verosimilitud máxima no restringido.
4.2.6. RESULTADOS DE MONTE CARLO PARA PRUEBAS DE EFECTOS INDIVIDUALES Y
TEMPORALES
Baltagi et al. (1992b) llevaron a cabo experimentos Monte Carlo para comparar el rendimiento de varias
pruebas en un modelo de componentes de error de dos vías. Los resultados clave son los siguientes:
•

•

•

•
•

Pruebas para 𝑯𝒂𝟎 : 𝝈𝟐𝝁 = 𝟎: Todas las pruebas usuales para 𝐻0𝑎 funcionan mal cuando 𝜎𝜆2 es
grande, especialmente la prueba BP, que rechaza en exceso la hipótesis nula. Pruebas como HO,
SLM, LR y F subestiman el tamaño nominal. Sin embargo, cuando 𝜎𝜇2 es grande, todas las
pruebas rechazan bien la hipótesis nula. La potencia de todas las pruebas disminuye a medida
que aumenta 𝜎𝜆2
Pruebas para 𝑯𝒅𝟎 : 𝝈𝟐𝝁 = 𝟎 (permitiendo 𝝈𝟐𝝀 > 𝟎): Las pruebas LMµ, LR y F tienen un buen
rendimiento, con tamaños estimados cercanos al tamaño nominal y alta potencia para grandes
𝜎𝜇2 . La sobreespecificación del modelo (asumiendo 𝜎𝜆2 > 0 cuando en realidad 𝜎𝜆2 = 0) no
afecta negativamente la potencia de estas pruebas.
Prueba conjunta 𝑯𝒄𝟎 : 𝝈𝟐𝝁 = 𝝈𝟐𝝀 = 𝟎: Las pruebas BP, HO, KW y LR subestiman
significativamente el tamaño nominal, mientras que las pruebas GHM y F tienen tamaños
estimados cercanos al tamaño nominal. Las pruebas GHM son inmunes a valores negativos de
A y B y funcionan bien en los experimentos de Monte Carlo.
Recomendaciones: Las pruebas F de ANOVA funcionan razonablemente bien en comparación
con las pruebas LR y LM, tanto para modelos de una vía como de dos vías, y son recomendadas.
Estimadores de Pretest: Baltagi, Bresson y Pirotte (2003b) evaluaron el rendimiento de los
estimadores de pretest en modelos de componentes de error de dos vías. El estimador de pretest,
basado en la aplicación de la prueba GHM seguida de las pruebas LM condicionales, mostró un
buen rendimiento en términos de MSE relativo. Si el pretest no rechaza la nula, se reduce a
OLS; si la rechaza, se convierte en un estimador FGLS de una o dos vías.

4.2.7. EJEMPLO ILUSTRATIVO
Los resultados de los experimentos de Monte Carlo muestran que los estadísticos de prueba A y/o B
toman valores negativos grandes con bastante frecuencia en algunos diseños. Surge la pregunta de si es
posible que A y/o B tomen valores negativos grandes para datos reales. Para responder a esto, se aplican
las pruebas consideradas a la ecuación de inversión de Grunfeld (1958).

Resultados de las Pruebas: La tabla 4.1 son los resultados de la prueba para el ejemplo de Grunfield,
y la 4.2 es la prueba de Multiplicador de Lagrange de Breusch y Pagan para estos datos

Explicación de los Resultados
1. Pruebas para 𝑯𝟎𝒂 : 𝝈𝟐𝝁 = 𝟎: Todas las pruebas consideradas (BP, HO, KW, SLM, F, LR)
rechazan la hipótesis nula, lo que sugiere fuertemente la existencia de efectos específicos de los
individuos en los datos.
2. Pruebas para 𝑯𝟎𝒃 : 𝝈𝟐𝝀 = 𝟎: La prueba BP rechaza la hipótesis nula, pero otras pruebas (HO,
KW, SLM, F, LR) no la rechazan. Esto se debe a un valor negativo grande de B (-2.540),
indicando que puede no haber efectos específicos de tiempo significativos.
3. Pruebas para 𝑯𝟎𝒄 : 𝝈𝟐𝝁 = 𝝈𝟐𝝀 = 𝟎: Todas las pruebas, excepto GHM, subestiman el tamaño
nominal. La prueba GHM y la prueba F no son significativamente diferentes del tamaño
nominal.
4. Pruebas para 𝑯𝟎𝒅 : 𝝈𝟐𝝁 = 𝟎/𝝈𝟐𝝀 > 𝟎: Las pruebas 𝐿𝑀µ, LR y F rechazan la hipótesis nula,
sugiriendo que puede haber efectos específicos de los individuos cuando se permiten efectos
específicos de tiempo.
5. Pruebas para 𝑯𝟎𝒆 : 𝝈𝟐𝝀 = 𝟎𝝈𝟐𝝁 > 𝟎: La prueba 𝐿𝑀𝜆 y la prueba F no rechazan la hipótesis nula,
sugiriendo que no hay efectos específicos de tiempo cuando se permiten efectos específicos de
los individuos.
En resumen, los datos de Grunfeld respaldan el uso de pruebas unilaterales en aplicaciones empíricas y
demuestran la importancia de considerar efectos específicos de los individuos y del tiempo al realizar
análisis con modelos de componentes de error.
4.3. PRUEBA DE ESPECIFICIDAD DE HAUSMAN
La prueba de especificación de Hausman se utiliza para determinar si se deben usar modelos de efectos
fijos o aleatorios. Específicamente, evalúa si los efectos individuales no observados 𝜇𝑖 están
correlacionados con los regresores 𝑋𝑖𝑡 . Si existe tal correlación, el modelo de efectos aleatorios no es
apropiado.
Modelos Involucrados:
1. Modelo de Efectos Fijos (Within Estimator): Se elimina la influencia de los efectos
individuales no observados 𝜇𝑖 mediante la transformación "within", que se basa en restar la
media de cada individuo a lo largo del tiempo: 𝑦̃𝑖𝑡 = 𝑦𝑖𝑡 − 𝑦̅𝑖 , 𝑋̃𝑖𝑡 = 𝑋𝑖𝑡 − 𝑋𝑖
El estimador Within 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 es obtenido al aplicar MCO al modelo Transformado

dónde Q es la matriz de transformación que elimina las medias individuales.
2. Modelo de Efectos Aleatorios (GLS Estimator): Los efectos individuales 𝜇𝑖 se modelan como
parte del término de error y se asume que no están correlacionados con los regresores.
̂ 𝑮𝑳𝑺 es obtenido al aplicar el método de Máxima Verosimilitud Generalizada:
El estimador GLS 𝜷

dónde Ω es la matriz de covarianza de los errores.
Hipótesis Nula de la Prueba de Hausman: 𝑯𝟎 : 𝑬(𝒖𝒊𝒕 /𝑿𝒊𝒕 ) = 𝟎. Esta hipótesis significa que no hay
correlación entre los efectos individuales no observados 𝜇𝑖 y los regresores 𝑋𝑖𝑡 Bajo esta hipótesis, tanto
el estimador de efectos aleatorios 𝛽̂𝐺𝐿𝑆 como el de efectos fijos 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 son consistentes, pero el
estimador de efectos aleatorios es más eficiente.
Procedimiento
1. Comparación de Estimadores: Se comparan 𝛽̂𝐺𝐿𝑆 y 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 .
2. Diferencia de Estimadores: Se calcula 𝑞̂1 = 𝛽̂𝐺𝐿𝑆 − 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛
3. Varianza de la Diferencia: Se estima la varianza de la diferencia entre los estimadores.
𝑣𝑎𝑟(𝑞̂1 ) = 𝑣𝑎𝑟(𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 ) − 𝑣𝑎𝑟(𝛽̂𝐺𝐿𝑆 )
4. Estadístico de Prueba: Se construye el estadístico de prueba 𝑚1 basado en 𝑞̂1 y su varianza:

5. Distribución del Estadístico: Bajo la hipótesis nula, el estadístico de prueba sigue una
distribución 𝜒 2 con 𝐾 grados de libertad, donde 𝐾 es la dimensión del vector de pendientes 𝛽.
Alternativas y Generalizaciones
Regresión Aumentada: La prueba de Hausman puede ser derivada de una regresión aumentada, que
sería equivalente a probar si 𝛾 = 0

Prueba de Arellano (1993): Arellano (1993) proporcionó una alternativa robusta a la prueba de
Hausman, que es resistente a la autocorrelación y heterocedasticidad de forma arbitraria. Sugiere
construir la siguiente regresión, dónde 𝑦𝑖+ y 𝑋𝑖+ son transformaciones ortogonales hacia adelante y 𝑦̅𝑖 ,
𝑋̅𝑖 son las medias sobre el tiempo.

La metodología MCO en este modelo da 𝛽̂ = 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 y 𝛾̂ = 𝛽̂𝐵𝑒𝑡𝑤𝑒𝑒𝑛 − 𝛽̂𝑊𝑖𝑡ℎ𝑖𝑛 .. La prueba de
Hausman puede obtenerse de esta regresión artificial probando si 𝛾 = 0 usando errores estándar robustos
Prueba Generalizada: Ahn y Low (1996) sugieren que la prueba de Hausman puede ser generalizada
para probar que cada 𝑋𝑖𝑡 es no correlacionado con 𝜇𝑖 , calculando 𝑁𝑇 × 𝑅2 de la regresión de los residuos
′
∗
′
′
GLS ( 𝑦𝑖𝑡
− 𝑋𝑖𝑡∗ 𝛽̂𝐺𝐿𝑆 ) en 𝑋̃𝑖𝑡 y [𝑋𝑖1
, … , 𝑋𝑖𝑇
]
Conclusión: La prueba de Hausman es esencial para determinar la idoneidad de usar modelos de efectos
fijos o aleatorios en panel data. Garantiza que el modelo seleccionado proporciona estimaciones
consistentes y eficientes, verificando la no correlación entre efectos no observados y los regresores.
4.3.1. EJEMPLO 1: ECUACIÓN DE INVERSIÓN DE GRUNFELD
El propósito es evaluar si los efectos individuales no observados están correlacionados con las variables
explicativas 𝑋𝑖𝑡
Método:
•
•

Estimaciones Within: Estima los coeficientes usando la transformación que elimina los efectos
individuales fijos.
Estimaciones Between: Estima los coeficientes usando la media de las variables a lo largo del
tiempo para cada individuo.

•

Prueba de Hausman: Compara las diferencias entre las estimaciones de efectos aleatorios (RE)
y fijos (FE) para detectar posibles inconsistencias.

Para los datos de Grunfeld, las estimaciones Within son (𝛽1, 𝛽2) = (0.1101238,0.310065) y las
Between son (0.1346461,0.03203147). El estadístico de prueba de Hausman 𝑚3 es 𝜒2 = 2.131, no
significativo al 5%, lo que indica que no hay correlación entre los efectos individuales y 𝑋𝑖𝑡 .
En Stata, se debe usar el contraste entre los estimadores
RE y FE para calcular correctamente el estadístico de
Hausman 𝑚1, que es 2.33 (χ2), no rechazando la
hipótesis nula, similar a m3. La Tabla 4.3 muestra este
resultado.
También se puede calcular m2 (contraste entre SWAR
y Between). La Tabla 4.4 presenta este resultado.
La regresión aumentada con estimaciones GLS
factibles de SWAR da 𝛽 = (0.135,0.032) y 𝛾 =
(−0.025,0.278), con 𝐹 = 1.066 (𝐹(2,195)), no
significativo, confirmando la no correlación de los
efectos individuales con 𝑋𝑖𝑡 .
4.3.2. EJEMPLO 2: DEMANDA DE GASOLINA
El propósito es el mismo del ejemplo 1, y utilizan la misma metodología. Para los datos sobre la gasolina
de Baltagi y Griffin (1983), las estimaciones de los Withines son (𝛽1, 𝛽2, 𝛽3) =
(0,66128, −0,32130, −0,64015). Las estimaciones Between vienen dadas por (0,96737,-0,96329,0,79513).
El estadístico de prueba de Hausman resultante 𝜒 2 es 𝑚3 = 26.507, lo cual es significativo. Por lo
tanto, se rechaza la hipótesis nula de que no hay correlación entre los efectos individuales y 𝑋𝑖𝑡 .
De manera similar, se puede calcular 𝑚2 = 27.45, basado en el contraste entre el estimador GLS factible
SWAR y el estimador Between, y 𝑚1 = 302.8, basado en el contraste entre el estimador GLS factible
SWAR y el estimador de efectos fijos, todos obtenidos usando Stata. Aunque m1 da un valor del
estadístico de Hausman drásticamente diferente de m2 o m3, los tres estadísticos llevan a la misma
decisión: se rechaza la hipótesis nula y el estimador RE no es consistente.
La regresión aumentada, basada en la estimación iterativa de máxima verosimilitud (MLE) de θ, produce
las siguientes estimaciones: 𝛽𝐵𝑒𝑡𝑤𝑒𝑒𝑛 = (0.967, −0.963, −0.795 y 𝛾 = 𝛽𝑊𝑖𝑡ℎ𝑖𝑛 − 𝛽𝐵𝑒𝑡𝑤𝑒𝑒𝑛 =
(−0.306,0.642,0.155), con un valor 𝐹 observado para 𝐻0: 𝛾 = 0 igual a 4.821, lo que lleva a rechazar
H0.
4.3.3. EJEMPLO 3: ACTIVIDAD DE HUELGA
Owusu-Gyapong (1986) analizó datos de panel sobre la actividad de huelga en 60 industrias
manufactureras canadienses entre 1967 y 1979 utilizando un modelo de componente de error de un solo
sentido. Se obtuvieron estimaciones mediante MCO, Within y GLS.
1. Prueba F para efectos específicos de la industria: Con un valor F de 5.56, se rechazó la
hipótesis nula de cero efectos específicos de la industria, prefiriéndose el estimador Within al
MCO.
2. Prueba LM de Breusch y Pagan: Con un valor χ2 de 21.4, se rechazó la hipótesis nula de cero
efectos aleatorios, prefiriéndose el estimador GLS al MCO.
3. Prueba Hausman: Con un valor χ2 de 3.84, no se encontró significancia en la correlación entre
los efectos aleatorios y los regresores, por lo que el estimador GLS fue preferido al estimador
Within.

En conclusión, Owusu-Gyapong (1986) eligió GLS como el estimador preferido basándose en estas
pruebas.
4.3.4. EJEMPLO 4: COMPORTAMIENTO DE LA PRODUCCIÓN DE LAS SERRERÍAS
Cardellichio (1990) estudió la producción de 1147 aserraderos en Washington entre 1972 y 1984
utilizando un modelo de componente de error de un solo sentido. Se obtuvieron estimaciones mediante
MCO, Within y GLS.
1. Prueba F para la estabilidad de pendientes: No significativa al 5%, indicando estabilidad de
los parámetros de pendiente a lo largo del tiempo.
2. Prueba F para efectos de aserraderos: Rechazó la hipótesis nula al 1%, sugiriendo la inclusión
de efectos específicos de aserraderos.
3. Prueba de Hausman: Rechazó la hipótesis nula al 1%, indicando que la suposición de
ortogonalidad entre los regresores y los efectos de los aserraderos no es válida.
Cardellichio (1990) concluyó que las pendientes de regresión son estables, se deben incluir las variables
dummy de los aserraderos y el estimador Within es preferible al MCO y al GLS.
4.3.5. EJEMPLO 5: PRIMA SALARIAL POR MATRIMONIO
Cornwell y Rupert (1997) estudiaron la prima salarial atribuida al matrimonio utilizando datos del NLSY
de 1971, 1976, 1978 y 1980.
1. Estimaciones Within vs. GLS: Las estimaciones Within de la prima por matrimonio son
menores que las obtenidas mediante GLS factible.
2. Prueba de Hausman: Rechaza la hipótesis nula, indicando que hay características individuales
omitidas que están correlacionadas con el matrimonio y la tasa salarial.
En conclusión, la prima por matrimonio es solo un desplazamiento del intercepto, no excede del 5% al
7%, y cuestionan que el matrimonio mejore la productividad a través de la especialización.
4.3.6. EJEMPLO 6: UNIÓN MONETARIA Y COMERCIO
Glick y Rose (2002) investigaron si salir de una unión monetaria reduce el comercio internacional,
utilizando datos de comercio bilateral entre 217 países desde 1948 hasta 1997. Estimaron un modelo de
gravedad ampliado y aplicaron métodos de MCO, efectos fijos (FE) y efectos aleatorios (RE),
prefiriendo el modelo FE según la prueba de Hausman. Encontraron que los países que se unieron a una
unión monetaria duplicaron su comercio bilateral, mientras que aquellos que la abandonaron vieron su
comercio reducido a la mitad.
4.3.7. PRUEBA DE HAUSMAN PARA EL MODELO BIDIRECCIONAL
Para el modelo de componente de error bidireccional, la prueba de Hausman (1978) puede basarse en la
diferencia entre el estimador de efectos fijos y el estimador GLS de efectos aleatorios bidireccionales.
Sin embargo, las pruebas equivalentes para el modelo unidireccional no se extienden completamente al
bidireccional.
Kang (1985) mostró que no hay equivalencia similar para la prueba de Hausman en el modelo
bidireccional debido a la existencia de dos estimadores Between: uno entre períodos de tiempo (βT) y
otro entre secciones transversales (βC). Además, 𝛽𝐺𝐿𝑆 es una combinación ponderada de 𝛽𝑇 , 𝛽𝐶 y el
estimador Within (𝛽𝑊 ). Kang clasifica cinco hipótesis comprobables basadas en estas diferencias:
1.
2.
3.
4.
5.

Probar 𝑬(𝝀𝒕 /𝑿𝒊𝒕 ) = 𝟎 asumiendo 𝜇𝑖 fijos con 𝛽𝑊 − 𝛽𝑇 .
Probar 𝑬(𝝀𝒕 /𝑿𝒊𝒕 ) = 𝟎 asumiendo 𝜇𝑖 aleatorios con 𝛽𝑇 − 𝛽𝐺𝐿𝑆 .
Probar 𝑬(𝝁𝒊 /𝑿𝒊𝒕 ) = 𝟎 asumiendo 𝜆𝑡 fijos con 𝛽𝑊 − 𝛽𝐶 .
Probar 𝑬(𝝁𝒊 /𝑿𝒊𝒕 ) = 𝟎 asumiendo 𝜆𝑡 aleatorios con 𝛽𝐶 − 𝛽𝐺𝐿𝑆 .
Comparar estimadores que asumen 𝜇𝑖 y 𝜆 𝑇 fijos versus aleatorios, tal que 𝐸(𝜆𝑡 /𝑋𝑖𝑡 ) = 𝐸(𝜇𝑖
/𝑋𝑖𝑡 ) = 0. Esta prueba se basa en 𝛽𝐺𝐿𝑆 − 𝛽𝑊 .

DYNAMIC PANEL DATA MODELS (BALTAGI CAP 8)
8.1. INTRODUCCIÓN
Los modelos de datos de panel dinámicos son cruciales para entender las relaciones económicas a lo
largo del tiempo. Estos modelos permiten estudiar la dinámica de ajuste mediante la inclusión de una
variable dependiente rezagada en los regresores. La fórmula básica del modelo es:
′
𝑦𝑖𝑡 = 𝛿𝑦𝑖,𝑡−1 + 𝑥𝑖𝑡
𝛽 + 𝑢𝑖𝑡

donde 𝛿 es un escalar, 𝑥′𝑖𝑡 es un vector de regresores, y 𝑢𝑖𝑡 sigue un modelo de componente de error:
𝑢𝑖𝑡 = 𝜇𝑖 + 𝜈𝑖𝑡
Aquí, 𝜇𝑖 son efectos individuales no observados y 𝜈𝑖𝑡 son errores aleatorios. La presencia de la variable
dependiente rezagada introduce autocorrelación y correlación con los efectos individuales, haciendo que
el estimador de MCO sea sesgado e inconsistente.
Problemas en la estimación: La inclusión de una variable dependiente rezagada introduce problemas
de autocorrelación y correlación con los efectos individuales. Esto hace que el estimador de mínimos
cuadrados ordinarios (MCO) sea sesgado e inconsistente. El estimador de efectos fijos (FE) elimina los
efectos individuales mediante la transformación "within", pero la variable dependiente rezagada sigue
estando correlacionada con el término de error transformado, lo que resulta en un sesgo de orden
𝑂(1/𝑇). Por su parte, el estimador de efectos aleatorios (GLS) también es sesgado debido a la
correlación entre los efectos individuales y la variable dependiente rezagada.
Soluciones propuestas: Se sugiere las primeras diferencias como solución, ya que puede manejar mejor
la correlación con las variables predeterminadas. Anderson y Hsiao (1981) sugieren diferenciar el
modelo para eliminar 𝜇𝑖 y usar 𝛥𝑦𝑖𝑡−2 o 𝑦𝑖𝑡−2 como instrumentos para 𝛥𝑦𝑖𝑡−1 , lo que lleva a
estimaciones consistentes, aunque no necesariamente eficientes.
El método de momentos generalizados (GMM) propuesto por Arellano y Bond (1991) proporciona un
procedimiento más eficiente para estimar modelos de panel dinámicos utilizando condiciones de
ortogonalidad entre los valores rezagados de 𝑦𝑖𝑡 y las perturbaciones 𝜈𝑖𝑡 .
Avances y métodos alternativos: Los trabajos de Arellano y Bover (1995) y Blundell y Bond (1998)
extienden el método GMM para ofrecer soluciones más robustas.
8.2. EL ESTIMADOR DE ARELLANO Y BOND
Arellano y Bond (1991) propusieron un método eficiente para estimar modelos de datos de panel
dinámicos utilizando condiciones de ortogonalidad entre los valores rezagados de 𝑦𝑖𝑡 y las
perturbaciones 𝜈𝑖𝑡 . Considere el siguiente modelo autorregresivo simple sin regresores adicionales:
𝑦𝑖𝑡 = 𝛿𝑦𝑖,𝑡−1 + 𝑢𝑖𝑡

𝑝𝑎𝑟𝑎 𝑖 = 1, … , 𝑁; 𝑡 = 1, … , 𝑇

donde 𝑢𝑖𝑡 = 𝜇𝑖 + 𝜈𝑖 con 𝜇𝑖 ∼ 𝐼𝐼𝐷(0, 𝜎𝜇2 ) y 𝜈𝑖𝑡 ∼ 𝐼𝐼𝐷(0, 𝜎𝜈2 ), independientes entre sí y entre ellos. Para
obtener una estimación consistente de 𝛿 cuando 𝑁 → ∞ y 𝑇 es fijo, diferenciamos la ecuación para
eliminar los efectos individuales:

Esta diferenciación resulta en un término de error que es un proceso MA (1) con una raíz unitaria.
Utilizando instrumentos válidos como 𝑦1𝑖 para 𝑡 = 3, 𝑦𝑖2 y 𝑦𝑖1 para 𝑡 = 4, y así sucesivamente, se
pueden obtener estimaciones consistentes de 𝛿.
Construcción del Estimador Para el periodo 𝑡 = 3: 𝑦𝑖3 − 𝑦𝑖2 = 𝛿(𝑦𝑖2 − 𝑦𝑖1 ) + (𝜈𝑖3 − 𝜈𝑖2 ), dónde 𝑦𝑖1
es un instrumento válido porque no está correlacionado con (𝜈𝑖3 − 𝜈𝑖2 ). De manera similar, para 𝑡 = 4:
𝑦𝑖4 − 𝑦𝑖3 = 𝛿(𝑦𝑖3 − 𝑦𝑖2 ) + (𝜈𝑖4 − 𝜈𝑖3 ), dónde los instrumentos válidos son 𝑦𝑖2 y 𝑦𝑖1 .

En base a este proceso, para el periodo 𝑇, el conjunto de instrumentos válidos se convierte en
(𝑦𝑖1 , 𝑦𝑖2 , … , 𝑦𝑖,𝑇−2 )
Método de Momentos Generalizados (GMM): Para utilizar estos instrumentos de manera eficiente,
se construye una matriz de instrumentos 𝑊𝑖 que agrupa todos los valores rezagados de 𝑦. Utilizando las
condiciones de momento 𝐸(𝑊𝑖𝑇 𝛥𝜈𝑖 ) = 0, se obtienen estimaciones eficientes mediante métodos de
mínimos cuadrados generalizados (GLS). Premultiplicando la ecuación diferenciada por la matriz de
instrumentos y resolviendo, se obtiene un estimador preliminar consistente.
Este estimador se mejora utilizando un enfoque de GMM en dos pasos, que es más eficiente y aprovecha
todas las condiciones de momento disponibles. El estimador final de Arellano y Bond resulta en una
estimación óptima para 𝛿, que es consistente y eficiente en grandes muestras.
En resumen, el estimador de Arellano y Bond mejora la eficiencia del método de Anderson y Hsiao
utilizando más condiciones de momento y proporcionando un procedimiento GMM más robusto. Es
ideal para paneles dinámicos donde 𝑁 es grande y 𝑇 es fijo, eliminando los efectos individuales y
manejando adecuadamente la correlación en los errores diferenciados.
8.2.1. PRUEBA PARA EFECTOS INDIVIDUALES EN MODELOS AUTORREGRESIVOS
Holtz-Eakin (1988) propone una prueba simple para detectar la presencia de efectos individuales en
modelos de datos de panel dinámicos. Utiliza un modelo autorregresivo simple sin regresores
adicionales:
𝑦𝑖𝑡 = 𝛿𝑦𝑖,𝑡−1 + 𝑢𝑖𝑡
donde 𝑢𝑖𝑡 = 𝜇𝑖 + 𝜈𝑖 con 𝜇𝑖 ∼ 𝐼𝐼𝐷(0, 𝜎𝜇2 ) y 𝜈𝑖𝑡 ∼ 𝐼𝐼𝐷(0, 𝜎𝜈2 ), Supongamos que solo hay tres períodos
𝑇 = 3, entonces este modelo se puede estimar usando los dos últimos períodos. Bajo la hipótesis nula
de no efectos individuales, se mantienen ciertas siguientes condiciones de ortogonalidad reformuladas:

1. La primera restricción puede usarse para identificar 𝛿 incluso si hay efectos individuales, ya que
𝑦𝑖1 no está correlacionado con la diferencia de los errores en los períodos 3 y 2.
2. La segunda y tercera restricciones se usan para probar la hipótesis nula de no efectos
individuales. Si se cumplen, no hay correlación entre 𝑦𝑖,𝑡 y los errores 𝑢𝑖,𝑡 en diferentes
períodos, indicando que no hay efectos individuales.
Entonces, se usa la primera restricción para identificar 𝛿, y luego se evalúan las restricciones (2) y (3)
para evaluar si hay efectos individuales.
Para aplicar este modelo en la práctica, se forma un sistema de ecuaciones apiladas, que luego se puede
escribir como 𝒚∗ = 𝒀∗ 𝜹 + 𝒖∗ , dónde 𝑦 ∗ es el vector de diferencias de las variables dependientes, 𝑌 ∗ es
el vector de diferencias de las variables independientes, y 𝑢∗ es el vector de diferencias de los términos
de error.
Holtz-Eakin estima este sistema de ecuaciones simultáneas utilizando variables instrumentales
diferentes para cada ecuación debido a la naturaleza dinámica del modelo, y luego realiza mínimos
cuadrados generalizados (GLS) para obtener una estimación preliminar de 𝛿 y evaluar las restricciones
de ortogonalidad adicionales para detectar la presencia de efectos individuales.
Prueba de efectos individuales: Para probar la presencia de efectos individuales, se sigue estos pasos:
1. Restricciones de ortogonalidad: Evaluar si las condiciones de ortogonalidad adicionales (2 y
3) son válidas, dado que la primera restricción se utiliza para identificar 𝛿.
2. Cálculo de SSQ: Calcular la suma ponderada de los cuadrados de los residuales transformados

3. Cálculo de L: Comparar SSQ bajo la hipótesis nula (SSQR) y la suma de cuadrados de los
residuales (SSW) que impone solo las restricciones necesarias para la versión diferenciada.
4. Prueba 𝝌𝟐 : Verificar si la diferencia 𝐿 = 𝑆𝑆𝑄𝑅 − 𝑆𝑆𝑊 sigue una distribución 𝜒 2 con grados
de libertad iguales al número de restricciones sobreidentificadas.
En conclusión, el test de Holtz-Eakin es una herramienta poderosa para detectar efectos individuales en
modelos dinámicos de panel, permitiendo identificar y controlar la heterogeneidad individual en la
estimación de estos modelos.
8.2.2. MODELOS CON VARIABLES EXÓGENAS
En los modelos dinámicos de datos de panel, se pueden incluir variables exógenas 𝑥𝑖𝑡 que, aunque se
correlacionen con los efectos individuales 𝜇𝑖 , no se correlacionan con los errores 𝜈𝑖𝑡 . Esta situación es
común en econometría y se representa en la siguiente ecuación:
′
𝑦𝑖𝑡 = 𝛿𝑦(𝑖,𝑡−1) + 𝑥𝑖𝑡
𝛽 + 𝑢𝑖𝑡

donde 𝑢𝑖𝑡 = 𝜇𝑖 + 𝜈𝑖𝑡 . El desafío radica en manejar las correlaciones entre las variables exógenas y los
efectos individuales al mismo tiempo que se mantienen las propiedades deseadas de los errores.
Transformaciones y Restricciones de Ortogonalidad: Para eliminar los efectos individuales y obtener
estimaciones consistentes, se utiliza una transformación de primeras diferencias (FD). Esta
transformación elimina los efectos individuales al restar las observaciones consecutivas de la variable
dependiente y los regresores:
′
𝛥𝑦𝑖𝑡 = 𝛿𝛥𝑦𝑖,𝑡−1 + 𝛥𝑥𝑖𝑡
𝛽 + 𝛥𝜈𝑖𝑡

En este contexto, las primeras diferencias de las variables 𝑥𝑖𝑡 son instrumentos válidos para la ecuación
diferenciada, siempre y cuando sean estrictamente exógenas. La exogeneidad estricta significa que las
𝑥𝑖𝑡 no están correlacionadas con los errores en ningún periodo.
Proceso de Instrumentación:
Para manejar correctamente las primeras diferencias y las correlaciones de los errores, se construye una
matriz de instrumentos (𝑊). Esta matriz contiene todas las variables 𝑥𝑖𝑡 adecuadas que no se
correlacionan con los errores diferenciados. Si 𝑥𝑖𝑡 son estrictamente exógenas, todas las observaciones
de 𝑥𝑖𝑡 se pueden usar como instrumentos. En los casos donde 𝑥𝑖𝑡 son variables predeterminadas (que
pueden estar correlacionadas con errores pasados, pero no con errores futuros), solo las observaciones
previas de 𝑥𝑖𝑡 son válidas como instrumentos en cada período.
Estimación de Parámetros: Con la matriz de instrumentos adecuada, se puede proceder a estimar los
parámetros del modelo utilizando el método de momentos generalizados (GMM). Este método se basa
en las propiedades de ortogonalidad de los instrumentos y los errores diferenciados para obtener
estimaciones consistentes y eficientes de los coeficientes del modelo.
En términos prácticos, esto implica:
1. Definir las primeras diferencias: Transformar los datos restando observaciones consecutivas.
2. Construir la matriz de instrumentos: Incluir todas las observaciones relevantes de las
variables exógenas que no se correlacionan con los errores.
3. Aplicar GMM: Utilizar las propiedades de ortogonalidad para estimar los parámetros del
modelo dinámico.
Validación y Pruebas: Para validar la consistencia del estimador GMM, Arellano y Bond (1991)
proponen una prueba para la ausencia de correlación serial de segundo orden en los errores
diferenciados. Además, el test de Sargan verifica la validez de las restricciones de sobre identificación,
asegurando que los instrumentos son adecuados y suficientes para el modelo.
En conclusión, los modelos con variables exógenas en datos de panel dinámicos requieren una cuidadosa
consideración de las correlaciones entre los regresores y los efectos individuales. El uso de la

transformación de primeras diferencias y la correcta instrumentación mediante GMM permiten obtener
estimaciones consistentes y eficientes, asegurando que los modelos reflejen adecuadamente las
relaciones económicas dinámica
8.3. EL ESTIMADOR DE ARELLANO Y BOVER
Arellano y Bover (1995) desarrollan un marco unificado para estimadores de variables instrumentales
(IV) eficientes en modelos de datos de panel dinámicos. Su enfoque se basa en el modelo de Hausman
y Taylor (1981) que incluye tanto variables que varían con el tiempo como variables invariables en el
tiempo. El modelo general es:

donde 𝑦𝑖𝑡 es la variable dependiente, 𝑥𝑖𝑡 son las variables que varían en el tiempo, mientras que 𝑍𝑖 son
las invariables en el tiempo, 𝜇𝑖 son los efectos individuales no observados y 𝜈𝑖𝑡 los errores aleatorios
Transformación y Matriz de Instrumentos: Para manejar los efectos individuales y la heterogeneidad,
Arellano y Bover transforman el sistema de ecuaciones utilizando una matriz no singular 𝐻. Esta matriz
está diseñada para que los primeros 𝑇 − 1 errores transformados no contengan los efectos individuales
𝜇𝑖 , permitiendo que todas las variables exógenas sean instrumentos válidos.
La clave está en la construcción de la matriz de instrumentos 𝑊. En el caso del modelo de Hausman y
Taylor, las variables exógenas 𝑋1 y 𝑍1 se utilizan como instrumentos. La matriz de instrumentos 𝑀𝑖 se
configura para garantizar la ortogonalidad entre los instrumentos y los errores transformados.
Estimador GMM: El estimador GMM (Método de Momentos Generalizados) se obtiene al aplicar la
transformación y los instrumentos en las ecuaciones del modelo. La fórmula del estimador GMM se
expresa como:

donde 𝛺 es la matriz de covarianza de los errores transformados, y 𝜂̂ es el estimador de los coeficientes
del modelo.
Caso Dinámico: Cuando se introduce una variable dependiente rezagada, el modelo se ajusta para
incluir 𝑦𝑖𝑡−1 en la ecuación. Se redefine la matriz de instrumentos para incluir las observaciones
rezagadas adicionales que son válidas como instrumentos. Si los errores 𝜈𝑖𝑡 no están correlacionados
serialmente, se pueden incorporar restricciones de ortogonalidad adicionales, lo que permite usar valores
pasados de 𝑦 como instrumentos adicionales.
Robustez y Pruebas: Arellano y Bover también proponen pruebas para verificar la ausencia de
correlación serial en los errores, así como la validez de los instrumentos utilizados mediante la prueba
de Sargan. Estas pruebas aseguran que los instrumentos sean adecuados y que el estimador GMM sea
consistente y eficiente.
Asimismo, el estimador de Arellano y Bover se aplica en varios estudios empíricos, como en los modelos
de empleo y decisiones de inversión, determinando la importancia de variables como la Q de Tobin,
considerando su posible endogeneidad. Este enfoque unificado y flexible permite abordar problemas
complejos en econometría con modelos dinámicos de panel, proporcionando estimaciones robustas y
fiables.
8.4. CONDICIONES DE MOMENTO DE AHN Y SCHMIDT
Ahn y Schmidt (1995) identifican condiciones de momento adicionales que no son aprovechadas por
estimadores de variables instrumentales (IV) anteriores en modelos de datos de panel dinámicos. Estas
condiciones surgen a partir de las suposiciones estándar del modelo dinámico y se pueden utilizar en un
marco de GMM (Método de Momentos Generalizados).
Para ilustrarlo, consideremos un modelo dinámico simple sin regresores: 𝑦𝑖𝑡 = 𝛿𝑦𝑖,𝑡−1 + 𝑢𝑖𝑡

Donde 𝑢𝑖𝑡 = 𝜇𝑖 + 𝜈𝑖𝑡 , y las suposiciones estándar son: (1) 𝜈𝑖𝑡 es no correlacionado con 𝑦𝑖0 ; (2) 𝜈𝑖𝑡 es
no correlacionado con 𝜇𝑖 ; (3) 𝜈𝑖𝑡 no están correlacionados entre sí
Condiciones de Momento Tradicionales: Bajo estas suposiciones, se obtienen las siguientes
condiciones de momento tradicionales: 𝑬(𝒚𝒊𝒔 ∆𝒖𝒊𝒕 ) = 𝟎 𝒑𝒂𝒓𝒂 𝒕 = 𝟐, … 𝑻, 𝒔 = 𝟎, … , 𝒕 − 𝟐, que son las
restricciones utilizadas por Arellano y Bond (1991). Sin embargo, Ahn y Schmidt encuentran
condiciones de momento adicionales que no están implícitas en las restricciones tradicionales.
Condiciones de Momento Adicionales: Además de las restricciones tradicionales, Ahn y Schmidt
identifican condiciones adicionales: 𝑬(𝒖𝒊𝑻 ∆𝒖𝒊𝒕 ) = 𝟎 𝒑𝒂𝒓𝒂 𝒕 = 𝟐, … , 𝑻 − 𝟏, estas nuevas condiciones
implican que 𝑢𝑖𝑇 no está correlacionado con los errores diferenciados anteriores. Combinando estas
condiciones con las tradicionales, obtenemos un conjunto completo de condiciones de momento.
Aplicación en el Estimador GMM: Para utilizar estas condiciones en la estimación GMM,
consideramos el modelo dinámico en forma diferenciada junto con la última observación en niveles:
∆𝑦𝑖𝑡 = 𝛿∆𝑦𝑖,𝑡−1 + ∆𝑢𝑖𝑡 𝑝𝑎𝑟𝑎 𝑡 = 2,3, . . , 𝑇
𝑦𝑖𝑇 = 𝛿𝑦𝑖,𝑇1 + 𝑢𝑖𝑇
Aunque no hay instrumentos observables legítimos para la ecuación en niveles, la ecuación sigue siendo
útil debido a las restricciones de covarianza adicionales. Las condiciones de momento combinadas
permiten una estimación más eficiente usando GMM.
Robustez y Aplicaciones: Las condiciones de momento de Ahn y Schmidt son robustas bajo
suposiciones más débiles que las tradicionales. Por ejemplo, permiten que el estimador GMM basado
en estas condiciones sea asintóticamente equivalente al estimador de mínima distancia óptimo de
Chamberlain, alcanzando el límite de eficiencia semiparamétrica. Además, Ahn y Schmidt exploran
restricciones adicionales bajo suposiciones de homocedasticidad y estacionariedad.
Modelos con Variables Exógenas: En el modelo dinámico de Hausman y Taylor (1981), Ahn y Schmidt
muestran cómo usar eficientemente variables exógenas como instrumentos, aprovechando más
condiciones de ortogonalidad que son relevantes en la versión dinámica del modelo.
Conclusión: El trabajo de Ahn y Schmidt (1995) proporciona un marco robusto para mejorar la
eficiencia de los estimadores en modelos de datos de panel dinámicos al identificar y utilizar condiciones
de momento adicionales. Estas condiciones permiten una estimación más precisa y eficiente mediante
GMM, incluso bajo suposiciones más débiles que las tradicionales. La metodología desarrollada es
particularmente útil para modelos dinámicos con variables exógenas, proporcionando un enfoque
generalizado y eficiente para la estimación de parámetros en estos contextos.
8.5. EL ESTIMADOR DE SISTEMA GMM DE BLUNDELL Y BOND
Blundell y Bond (1998) revisitan la importancia de explotar la condición inicial para generar estimadores
eficientes en el modelo de datos de panel dinámico cuando 𝑇 es pequeño. Consideran un modelo de
panel autorregresivo simple sin regresores exógenos:
𝑦𝑖𝑡 = 𝛿𝑦𝑖,𝑡−1 + 𝜇𝑖 + 𝜈𝑖𝑡
con 𝐸(𝜇𝑖 ) = 0, 𝐸(𝜈𝑖𝑡 ) = 0, y 𝐸(𝜇𝑖 𝜈𝑖𝑡 ) = 0. Blundell y Bond se enfocan en el caso donde 𝑇 = 3, lo que
genera una única condición de ortogonalidad dada por 𝐸(𝑦𝑖1 𝛥𝜈𝑖3 ) = 0. En este escenario, 𝛿 está
justificada por una identificación exacta.
Problemas con los Instrumentos Débiles: Para obtener el estimador IV de primera etapa, se corre una
regresión de ∆𝑦𝑖2 sobre 𝑦𝑖1 . Esta regresión puede derivarse de (8.38) evaluada en 𝑡 = 2, restando 𝑦𝑖1 de
ambos lados:

Dado que se espera 𝐸(𝑦𝑖1 𝜇𝑖 ) > 0, entonces (𝛿 − 1) estará sesgado hacia arriba. Blundell y Bond
encuentran que el sesgo y la precisión pobre del estimador GMM en diferencias primeras se deben a
problemas de instrumentos débiles, caracterizados por el parámetro de concentración 𝜏.
Mejorando la Eficiencia con el Estimador de Sistema GMM: Al imponer una restricción adicional
de estacionariedad suave en el proceso de condiciones iniciales, es posible utilizar un estimador GMM
de sistema. Este estimador utiliza diferencias rezagadas de 𝑦𝑖𝑡 como instrumentos para las ecuaciones
en niveles y niveles rezagados de 𝑦𝑖𝑡 como instrumentos para las ecuaciones en diferencias primeras.
Esta combinación de ecuaciones en niveles y diferencias mejora significativamente la eficiencia del
estimador, especialmente cuando 𝛿 se aproxima a 1 y 𝜎𝜇2 /𝜎𝑢2 aumenta.
Resultados Empíricos y Simulaciones: Blundell y Bond revisan los coeficientes de capital y trabajo
en una función de producción Cobb-Douglas usando datos de empresas manufactureras de EE. UU.
Durante 8 años. El estimador GMM estándar muestra una baja estimación del coeficiente de capital y
baja precisión en todos los coeficientes estimados. Sin embargo, el estimador GMM de sistema
proporciona estimaciones razonables y más precisas.
Importancia de la Estacionariedad en las Condiciones Iniciales: Hahn (1999) examinó el papel de
la condición inicial impuesta por el estimador de Blundell y Bond, encontrando que la ganancia en
eficiencia puede ser sustancial cuando se incorpora la estacionariedad. Bond y Windmeijer (2002)
proponen un estimador lineal simple para el caso donde las condiciones iniciales satisfacen la
estacionariedad de covarianza, mostrando que el estimador de Blundell y Bond es eficiente bajo esta
suposición.
Conclusión: El estimador GMM de sistema de Blundell y Bond proporciona una mejora significativa
en la precisión y reducción del sesgo en comparación con el estimador GMM en diferencias primeras,
especialmente en casos de series persistentes y T pequeños. Este enfoque utiliza condiciones de
momento adicionales basadas en restricciones de estacionariedad, ofreciendo una solución robusta para
superar las limitaciones de instrumentos débiles en modelos de datos de panel dinámicos. La
metodología de Blundell y Bond ha demostrado ser valiosa en estudios empíricos y simulaciones,
respaldando su utilidad en aplicaciones prácticas de econometría.
8.6. EL ESTIMADOR DE KEANE Y RUNKLE
El modelo de datos de panel con una variable dependiente rezagada y efectos aleatorios o cualquier otro
tipo de correlación serial que es invariante entre individuos se presenta como:
𝑦 = 𝑋𝛽 + 𝑢
donde 𝑋 contiene una variable dependiente rezagada. Consideramos el caso donde 𝐸(𝑢𝑖𝑡 /𝑋𝑖𝑡 ) ≠ 0, y
existe un conjunto de instrumentos predeterminados 𝑊 tal que 𝐸(𝑢𝑖𝑡 /𝑊𝑖𝑠 ) = 0 para 𝑠 ≤ 𝑡, pero 𝐸(𝑢𝑖𝑡
/𝑊𝑖𝑠 ) ≠ 0 para 𝑠 > 𝑡. En otras palabras, 𝑊 puede contener valores rezagados de 𝑦𝑖𝑡 . Para este modelo,
el estimador de 2SLS proporcionará un estimador consistente para 𝛽.
Problemas con la Eficiencia en 2SLS: En el modelo de efectos aleatorios o cualquier otro tipo de
correlación serial, el estimador 2SLS no será eficiente. Por ello, Keane y Runkle (1992) sugieren un
algoritmo más eficiente que toma en cuenta esta estructura de varianza-covarianza más general para los
disturbios, basado en la idea de filtrado hacia adelante de la literatura de series de tiempo.
Procedimiento de Estimación: Primero, se obtiene una estimación consistente de ∑−1
𝑇𝑆 ⬚ y su
̂
correspondiente descomposición de Cholesky 𝑃𝑇𝑆 Luego, se premultiplica el modelo por 𝑄̂𝑇𝑆 = (𝐼𝑁
⊗ 𝑃̂𝑇𝑆 ) y se estima el modelo por 2SLS usando los instrumentos originales. El estimador es:

Uso de Diferencias Primeras: La diferenciación primera también se usa en modelos de datos de panel
dinámicos para eliminar efectos específicos individuales. Los errores diferenciados resultantes están
correlacionados serialmente de tipo MA (1) con raíz unitaria si los errores originales 𝜈𝑖𝑡 son clásicos.
En este caso, habrá una ganancia en eficiencia al realizar el procedimiento KR en el modelo en
diferencias primeras.
Hipótesis Importantes y Pruebas: Existen dos hipótesis importantes que son verificables:
1. Hipótesis 𝑯𝑨 : Dice que los instrumentos 𝑊 son estrictamente exógenos. Para probar 𝐻𝐴 , KR
proponen una prueba basada en la diferencia entre 2SLS de efectos fijos (𝐹𝐸 − 2𝑆𝐿𝑆) y
2𝑆𝐿𝑆 en diferencias primeras (𝐹𝐷 − 2𝑆𝐿𝑆). La segunda hipótesis es 𝐻𝐵 : 𝐸(𝜇𝑖 /𝑊𝑖𝑡 ) = 0
2. Prueba de 𝑯𝑩 : Si 𝐻𝐴 es rechazado, los instrumentos son predeterminados y la prueba de
Hausman-Taylor es inapropiada. Por ello, la prueba para 𝐻𝐵 se basa en la diferencia entre 𝐹𝐷 −
2𝑆𝐿𝑆 y 2𝑆𝐿𝑆. Si 𝐻𝐵 es verdadero, ambos estimadores son consistentes, pero si no, solo 𝐹𝐷 −
2𝑆𝐿𝑆 permanece consistente.
Aplicación Empírica: Keane y Runkle aplican sus procedimientos de estimación y prueba a un modelo
de consumo de ciclo de vida con expectativas racionales. Usando datos de 627 hogares encuestados
entre 1972 y 1982 por el Panel de Estudio sobre Dinámica de Ingresos de Michigan (PSID), rechazan la
exogeneidad fuerte de los instrumentos, lo que significa que el estimador Within es inconsistente y la
prueba estándar de Hausman no es adecuada. Concluyen que las estimaciones de KR-2SLS no rechazan
el modelo de ciclo de vida simple, pero usando las estimaciones Within inconsistentes para la inferencia
se obtendrían conclusiones incorrectas contra el modelo de ciclo de vida.
Conclusión: El estimador de Keane y Runkle proporciona una solución eficiente para tratar con la
correlación serial general en datos de panel dinámicos, utilizando la idea de filtrado hacia adelante y
preservando el uso de instrumentos predeterminados. Las pruebas desarrolladas por KR son útiles para
verificar la exogeneidad de los instrumentos y la correlación entre los efectos individuales y los
instrumentos. En aplicaciones empíricas, este método ha demostrado ser más preciso y consistente,
evitando conclusiones incorrectas que podrían derivarse de estimaciones inconsistentes.
8.7. DESARROLLO FUTURO EN MODELOS DINÁMICOS DE DATA PANEL
La literatura sobre modelos dinámicos de datos de panel ha crecido significativamente, reflejando la
naturaleza dinámica de muchos modelos económicos. Esta sección resume algunos de los desarrollos
recientes más destacados.
•

•

•

•

Condiciones de Momentos Adicionales: Ahn y Schmidt (1995) identificaron condiciones de
momentos adicionales para modelos dinámicos de datos de panel que no habían sido
aprovechadas por estimadores IV anteriores. En 1997, propusieron un estimador GMM
linealizado que es tan eficiente como el GMM no lineal, proporcionando además pruebas de
validez para estas restricciones no lineales.
Estimación Asintótica Eficiente: Hahn (1997) demostró que el estimador GMM basado en un
conjunto creciente de instrumentos, a medida que aumenta el tamaño de la muestra, alcanza la
eficiencia semiparamétrica del modelo. También exploró el uso de series de Fourier o
polinomios como instrumentos eficientes.
IV Óptimo y Comparación con ML: Wansbeek y Bekker (1996) encontraron que el estimador
de variables instrumentales (IV) óptimo es menos eficiente que el estimador de máxima
verosimilitud (ML) en regiones probables del parámetro autorregresivo δ. Sugieren que agregar
restricciones de momentos no lineales, como las propuestas por Ahn y Schmidt, puede reducir
esta pérdida de eficiencia.
Sesgo y Eficiencia en GMM: Ziliak (1997) investigó el equilibrio entre sesgo y eficiencia en
estimadores GMM para modelos dinámicos de datos de panel. Sus experimentos de Monte
Carlo indicaron que usar demasiadas condiciones de momento puede introducir sesgo severo,
recomendando el uso de instrumentos subóptimos para mantener un equilibrio adecuado entre
sesgo y eficiencia.

•

•

•

Condiciones Redundantes: Ahn y Schmidt (1999) abordaron la identificación de condiciones
de momentos redundantes en modelos con variables estrictamente exógenas. Encontraron que
ciertos momentos no mejoran la eficiencia y, por lo tanto, pueden ser ignorados sin pérdida de
eficiencia asintótica.
Estimadores LIML y GMM: Alonso-Borrego y Arellano (1999) y Alvarez y Arellano (2003)
exploraron las propiedades asintóticas de los estimadores LIML y GMM. Encontraron que
LIML es preferible a GMM cuando los instrumentos son débiles, mostrando menor sesgo.
Además, cuando 𝑇 < 𝑁, el sesgo asintótico de LIML es menor que el de GMM y FE.
Modelos con Coeficientes Heterogéneos: Wansbeek y Knapp (1999) propusieron métodos
para tratar con modelos dinámicos de panel que tienen coeficientes heterogéneos en la variable
dependiente rezagada y la tendencia temporal. Mostraron que la doble diferenciación puede
eliminar efectos específicos de países y coeficientes heterogéneos, sugiriendo LIML como una
alternativa viable a GMM.

Conclusión: Los desarrollos recientes en modelos dinámicos de datos de panel incluyen la
identificación y uso de condiciones de momentos adicionales, la adaptación de metodologías GMM y
LIML, y el desarrollo de criterios robustos para la selección de modelos y momentos. Estos avances han
mejorado significativamente la precisión de las estimaciones en estudios empíricos y proporcionado
nuevas herramientas para abordar los desafíos inherentes a los datos de panel dinámicos
8.8. EJEMPLO EMPÍRICO: DEMANDA DINÁMICA DE CIGARRILLOS
Baltagi y Levin (1992) estimaron un modelo de demanda dinámica para cigarrillos utilizando datos de
panel de 46 estados de EE.UU. actualizados desde 1963 hasta 1992. La ecuación estimada es:

donde 𝐶𝑖𝑡 es la venta per cápita real de cigarrillos, 𝑃𝑖𝑡 es el precio minorista promedio de un paquete de
cigarrillos en términos reales, 𝑌𝑖𝑡 es el ingreso disponible real per cápita, y 𝑃𝑛𝑖𝑡 es el precio mínimo real
de los cigarrillos en cualquier estado vecino, representando un efecto de contrabando casual. El término
de perturbación 𝑢𝑖𝑡 sigue un modelo de componentes de error bidimensional:
𝑢𝑖𝑡 = 𝜇𝑖 + 𝜆𝑡 + 𝜈𝑖𝑡
donde 𝜇𝑖 son efectos específicos del estado y 𝜆𝑡 son efectos específicos del año, incluyendo
intervenciones políticas y advertencias de salud.
Tabla 8.1: La Tabla 8.1 muestra los resultados de diversas estimaciones agrupadas de la ecuación de
demanda de cigarrillos para el periodo 1963-1992.

•

•

•

OLS: Ignora los efectos de estado y
tiempo, arrojando una elasticidad de
precio a corto plazo baja (-0.09) y
una elasticidad de precio a largo
plazo alta (-2.98).
Within: Incluye efectos fijos de
estado y tiempo, proporcionando
una elasticidad de precio a corto
plazo más alta (-0.30) y una
elasticidad de precio a largo plazo
más baja (-1.79).
2SLS:
Utiliza
variables
instrumentales para abordar la
endogeneidad, con una elasticidad de precio a corto plazo de -0.205.

•
•
•
•

2SLS-KR: Introduce un método más eficiente basado en la filtración hacia adelante, con una
elasticidad de precio a corto plazo de -0.311.
Within-2SLS: Combina efectos fijos con 2SLS, obteniendo una elasticidad de precio a corto
plazo de -0.496.
FD-2SLS: Aplica diferenciación primera con 2SLS, resultando en una elasticidad de precio a
corto plazo de -0.348, luego FD-2SLS-KR es similar a FD-2SLS pero con filtración hacia
adelante, obteniendo una elasticidad de precio a corto plazo de -0.348.
GMM-one-step: Estimador GMM de un paso con una elasticidad de precio a corto plazo de 0.377, luego GMM-two-step es el estimador GMM de dos pasos con una elasticidad de precio
a corto plazo de -0.379.

Tabla 8.2: La Tabla 8.2 muestra los resultados del estimador GMM en dos pasos según Arellano y Bond
para la demanda de cigarrillos.
•
•

•

•

El modelo dinámico de panel de datos de
Arellano-Bond utiliza 1288 observaciones de
46 estados a lo largo de 28 años.
Los coeficientes estimados para el consumo
rezagado (ln 𝐶𝑖,𝑡−1 ) y el precio propio
(ln 𝑃𝑖𝑡 ) son significativos y coherentes con
las expectativas teóricas.
Las pruebas de sobreidentificación de Sargan
no rechazan la nulidad, lo que puede indicar
una falta de poder en la prueba dada la
estructura de los datos (𝑁 = 46, 𝑇 = 28).
La prueba de correlación serial de primer
orden rechaza la nulidad, pero no se rechaza
para la correlación de segundo orden, que es
el resultado esperado en un modelo de
diferencias primeras con perturbaciones no
transformadas
originalmente
no
correlacionadas.

Conclusión: El análisis de la demanda dinámica de cigarrillos en 46 estados de EE. UU. de 1963 a 1992
revela que los métodos de estimación que consideran los efectos específicos del estado y del tiempo, así
como la endogeneidad del consumo rezagado, producen resultados más precisos y coherentes. Los
modelos que utilizan métodos como Within, 2SLS y GMM ajustados muestran una mayor elasticidadprecio a corto plazo y menores coeficientes de consumo rezagado en comparación con OLS. Las pruebas
de sobreidentificación y de correlación serial validan la robustez de estos modelos. En particular, el
estimador FD-2SLS-KR y los métodos GMM destacan por proporcionar estimaciones más confiables,
sugiriendo la relevancia de utilizar técnicas avanzadas para manejar la dinámica y la heterogeneidad en
los datos de panel.
8.9. OTRAS LECTURAS
La exploración de modelos dinámicos de panel ha llevado al desarrollo de métodos avanzados para
manejar la autocorrelación, la heterogeneidad individual y los instrumentos débiles. Los trabajos
mencionados proporcionan herramientas cruciales para la estimación eficiente y robusta de parámetros,
destacando la importancia de la correcta especificación de los modelos y la validación de hipótesis en
estudios empíricos.

NONSTATIONARY PANELS (BALTAGI CAP 12)
12.1 INTRODUCCIÓN
El creciente uso de datos de distintos países en econometría de datos de panel ha permitido estudiar
importantes fenómenos económicos, como r la paridad del poder adquisitivo, la convergencia del
crecimiento y los efectos de derrame internacional de I+D. Esta sección destaca el enfoque en las
propiedades asintóticas de macro paneles con un gran número de países (N) y una larga serie temporal
(T). Se mencionan dos corrientes principales:
1. Regresiones Heterogéneas: Este enfoque rechaza la homogeneidad de los parámetros de
regresión, favoreciendo regresiones específicas para cada país. Se critica el uso de estimadores
agrupados estándar debido al sesgo potencial que pueden introducir.
2. Procedimientos de Series Temporales en Paneles: Este enfoque se centra en la no
estacionariedad, regresiones espurias y cointegración. Argumenta que la no estacionariedad es
más relevante en macro paneles. Se mencionan técnicas de estimación modificadas que abordan
la endogeneidad y la heterocedasticidad.
Propiedades Asintóticas: En paneles no estacionarios, muchos estadísticos de prueba y estimadores
tienen distribuciones límite normales, a diferencia de las series temporales individuales. Esto facilita la
realización de pruebas de raíz unitaria y cointegración, evitando problemas de regresión espuria y
proporcionando estimaciones consistentes cuando 𝑁 y 𝑇 tienden al infinito
Métodos recientes: En los últimos años, se han aplicado numerosos métodos de series temporales a
datos de panel, destacando las pruebas de raíz unitaria, pruebas de cointegración y la estimación de
relaciones a largo plazo. Estos métodos han mejorado la capacidad de análisis y precisión en la
estimación de relaciones económicas de largo plazo, permitiendo una mejor comprensión de los
fenómenos económicos complejos a través de un enfoque de datos de panel
Principales críticas a los usos de los métodos de datos de panel:
•
•
•
•

Maddala et al. (2000): Argumentan que las pruebas de raíz unitaria en datos de panel no
resuelven adecuadamente la PPP, ya que los resultados son mixtos y dependen del grupo de
países, el período de estudio y el tipo de prueba utilizada.
Maddala (1999): Las pruebas de raíz unitaria en panel no abordan eficazmente la convergencia
del crecimiento entre países, aunque han impulsado investigaciones en modelos dinámicos de
datos de panel.
Quah (1996): Señala que los paneles tradicionales no pueden responder si los países pobres
alcanzan a los ricos, sugiriendo modelos de dinámica de ingresos en su lugar.
Smith (2000): Advierte contra la aplicación mecánica de pruebas de raíz unitaria o
cointegración en paneles, enfatizando que las hipótesis deben ser relevantes en el contexto
específico.

Asimismo, las pruebas existentes dentro de los modelos asumen independencia transversal, lo cual es
restrictivo, ya que las series temporales macroeconómicas muestran una correlación significativa entre
países. Por ello, se han propuesto modelos de factores dinámicos y pruebas alternativas para abordar la
dependencia transversal.
12.2 PRUEBAS DE RAÍCES UNITARIAS EN PANEL ASUMIENDO INDEPENDENCIA
TRANSVERSAL
Las pruebas de raíces unitarias en series temporales y paneles se han vuelto comunes en la investigación
econométrica. Las pruebas de raíces unitarias en paneles, aunque recientes, han sido desarrolladas por
varios autores como Levin, Lin y Chu (2002), Im et al. (2003), y otros.
Las aproximaciones para los límites asintóticos en estudios de panel se pueden abordar de varias
maneras, y cada una tiene sus propias características y aplicaciones. Se pueden clasificar en tres
enfoques principales:

1. Límites Secuenciales: En esta aproximación, primero se fija un índice, por ejemplo, el número
de unidades transversales 𝑁. Luego, se permite que el otro índice, la longitud de la serie
temporal 𝑇, aumente hasta el infinito. Después, se deja que 𝑁 tienda al infinito.
Ventajas: Son fáciles de derivar y útiles para obtener resultados asintóticos rápidamente.
Desventajas: Pueden ser engañosos en algunos casos, ya que los resultados obtenidos por esta
vía no siempre representan adecuadamente la realidad asintótica.
2. Trayectorias Diagonales: Aquí, ambos índices 𝑁 y 𝑇 tienden al infinito simultáneamente,
siguiendo una trayectoria específica en la matriz bidimensional. La trayectoria está determinada
por una relación funcional creciente 𝑇 = 𝑇(𝑁)
Ventajas: Este enfoque considera el crecimiento simultáneo de 𝑁 y 𝑇, proporcionando una
perspectiva más equilibrada.
Desventajas: Los resultados dependen de la relación funcional específica y la trayectoria
asumida puede no ser adecuada para todas las situaciones.
3. Límites Conjuntos: En este enfoque, tanto 𝑁 como 𝑇 tienden al infinito al mismo tiempo sin
imponer una trayectoria específica.
Ventajas: Es más robusto y generalmente proporciona resultados más realistas.
Desventajas: Es más difícil de derivar y requiere condiciones estrictas, como la existencia de
momentos superiores para la uniformidad en los argumentos de convergencia.
Phillips y Moon (1999, 2000) aplican la teoría asintótica de múltiples índices a límites conjuntos donde
𝑇 y 𝑁 tienden al infinito y 𝑇/𝑁 → ∞ Esto es útil cuando la muestra de la serie temporal es grande en
comparación con la muestra de corte transversal. No obstante, este enfoque también puede aplicarse a
situaciones donde 𝑇/𝑁 → 0, aunque los resultados límite obtenidos serán diferentes.
12.2.1 PRUEBA DE LEVIN, LIN Y CHU (LLC)
La prueba de LLC busca mejorar el poder de detección de raíces unitarias en series temporales en
paneles, especialmente en muestras pequeñas. Argumentan que las pruebas de raíces unitarias
individuales tienen un poder limitado frente a hipótesis alternativas con desviaciones del equilibrio
persistentes. Asimismo, esta prueba es más poderosa que realizar pruebas individuales para cada sección
transversal.
Hipótesis:
•
•

Hipótesis nula (𝐻0 ): Cada serie temporal individual contiene una raíz unitaria.
Hipótesis alternativa (𝐻1 ): Cada serie temporal es estacionaria.

Procedimiento de la Prueba
1. Regresiones ADF Individuales: Se realizan regresiones Augmented Dickey-Fuller (ADF) para
cada sección transversal. Aquí, el orden de rezago 𝑝𝑖 varía entre individuos y se debe seleccionar
𝑝𝑚𝑎𝑥 y ajustar según los t-estadísticos.
2. Estimación de la Varianza a Largo Plazo: Se estima la varianza a largo plazo bajo la hipótesis
nula de raíz unitaria, calculando la ratio de la desviación estándar a largo plazo a la desviación
estándar de la innovación.
3. Estadísticos de la Prueba del Panel: Se ejecuta una regresión agrupada y se calcula el
∗
∗
estadístico t convencional para 𝐻0 : 𝜌 = 0, ajustando el t-estadístico con los valores 𝜇𝑚𝑇
y 𝜎𝑚𝑇
proporcionados por LLC.
La prueba LLC es ideal para paneles de tamaño moderado (N entre 10 y 250 y T entre 25 y 250).
Limitaciones: La prueba depende crucialmente de la suposición de independencia entre secciones
transversales, no es aplicable si hay correlación transversal, y la suposición de que todas las secciones
transversales tienen o no una raíz unitaria es restrictiva.
Importancia: Los simulacros de Monte Carlo muestran que la distribución normal es una buena
aproximación para la distribución empírica del estadístico de prueba, incluso en muestras relativamente

pequeñas. Asimismo, la prueba de raíces unitarias en panel de LLC proporciona mejoras significativas
en el poder sobre las pruebas de raíces unitarias individuales.
Conclusión: La prueba de LLC es una herramienta poderosa y ampliamente utilizada para detectar
raíces unitarias en paneles de datos, especialmente en muestras de tamaño moderado, aunque tiene
ciertas limitaciones que deben considerarse al aplicarla.
12.2.2 PRUEBA DE IM, PESARAN Y SHIN
La prueba de Im, Pesaran y Shin (IPS) fue propuesta como una alternativa a la prueba de LLC debido a
que esta última es restrictiva al requerir que el coeficiente de raíz unitaria (𝜌) sea homogéneo entre todas
las unidades transversales. Esto puede ser problemático en aplicaciones donde se espera heterogeneidad,
como la convergencia del crecimiento entre países. La prueba IPS permite coeficientes heterogéneos y
se basa en promediar los estadísticos de pruebas de raíz unitaria individuales, y sus hipótesis son:
•
•

Hipótesis nula (𝑯𝟎 ): Cada serie en el panel contiene una raíz unitaria, es decir, 𝜌𝑖 = 0 para
todos los 𝑖.
Hipótesis alternativa (𝑯𝟏 ): Algunas, pero no todas, las series individuales son estacionarias,
es decir,

1. Realización de Pruebas DFA Individuales: Se realizan pruebas DFA para cada sección
transversal, donde el orden de rezago (𝜌𝑖 ) puede variar entre individuos. El estadístico t
individual para cada sección se obtiene al probar 𝐻0 : 𝜌𝑖 = 0
2. Cálculo del Estadístico t-bar: Se calcula el promedio de los estadísticos t individuales de las
1
pruebas ADF: 𝑁 ∑𝑁
𝑖=1 𝑡𝜌𝑖
3. Estandarización del Estadístico t-bar: Se estandariza el estadístico t-bar utilizando valores
esperados y varianzas simuladas bajo la hipótesis nula para diferentes combinaciones de 𝑇 y 𝜌𝑖

El estadístico estandarizado 𝑍𝑡−𝑏𝑎𝑟
asintóticamente.

sigue una distribución normal estándar 𝑁(0,1)

Aplicaciones y Limitaciones: La prueba IPS es ideal cuando se espera heterogeneidad en los
coeficientes de las series temporales individuales. Permite que algunas series sean estacionarias mientras
que otras no, proporcionando una mayor flexibilidad y realismo en las aplicaciones prácticas. Sin
embargo, requiere simulaciones para determinar valores críticos y varianzas específicas, y su
rendimiento depende de la correcta selección del orden de rezago en las regresiones ADF subyacentes.
Importancia y Validación: Los experimentos de Monte Carlo realizados por Im, Pesaran y Shin
muestran que, si se selecciona un orden de rezago adecuado, el rendimiento en muestras pequeñas de la
prueba t-bar es razonablemente satisfactorio y generalmente mejor que la prueba LLC. La prueba IPS
aborda mejor la heterogeneidad y permite una evaluación más realista de la estacionariedad en paneles
de datos complejos.
Conclusión: La prueba IPS es una herramienta potente para detectar raíces unitarias en paneles de datos,
ofreciendo una alternativa más flexible y adecuada en situaciones donde la homogeneidad no es una
suposición válida. Esta metodología mejora la precisión y el realismo de las evaluaciones de
estacionariedad en estudios empíricos con datos de panel
12.2.3 PRUEBA DE BREITUNG
Las pruebas de LLC y IPS asumen que el número de secciones transversales (𝑁) debe ser pequeño en
relación con el número de períodos (𝑇). Sin embargo, Breitung (2000) aborda la pérdida de poder
significativa en estas pruebas cuando se incluyen tendencias específicas de cada individuo debido a la

corrección de sesgo. Por ello, propone una prueba que no emplea dicha corrección, mejorando así el
poder de la prueba.
Procedimiento de la prueba:
1. Regresiones DFA Individuales: Igual que en la prueba LLC, se realizan regresiones DFA
individuales para cada sección transversal utilizando solo 𝛥𝑦𝑖𝑡−𝐿 para obtener los residuos 𝑒𝑖𝑡
y 𝜈𝑖𝑡−1
2. Transformación de los Residuos: Los residuos 𝑒𝑖𝑡 se transforman usando la ortogonalización
hacia adelante de Arellano y Bover (1995).
3. Regresión Agrupada: Se ejecuta una regresión agrupada para obtener el estadístico t para
𝐻0 : 𝜌 = 0, que sigue una distribución normal estándar 𝑁(0,1) en el límite sin necesidad de
cálculos de kernel.
Aplicaciones y Limitaciones: La prueba de Breitung ofrece una alternativa poderosa a las pruebas de
LLC e IPS, especialmente útil en situaciones donde la inclusión de tendencias individuales puede causar
pérdidas de poder en otras pruebas. La transformación de los residuos y la eliminación de la corrección
de sesgo hacen que esta prueba sea más robusta y eficaz en diversas aplicaciones. McCoskey y Selden
(1998) aplicaron la prueba IPS para testar la raíz unitaria en gastos de salud per cápita y PIB para un
panel de 20 países de la OCDE, mientras que Wu (2000) aplicó la prueba IPS a los balances de cuenta
corriente de 10 países de la OCDE.
Conclusión: La prueba de Breitung proporciona una metodología más robusta y eficaz para detectar
raíces unitarias en paneles de datos, mejorando el poder de las pruebas en comparación con LLC e IPS.
Su enfoque, que evita la corrección de sesgo, es particularmente útil en estudios empíricos donde las
tendencias individuales son relevantes
12.2.4 PRUEBAS DE COMBINACIÓN DE P-VALUES
Las pruebas de combinación de p-values, propuestas por Maddala y Wu (1999) y Choi (2001), buscan
combinar los p-values de pruebas de raíces unitarias individuales para cada sección transversal en
paneles de datos. Esta metodología ofrece una alternativa a las pruebas LLC e IPS, permitiendo más
flexibilidad y poder de prueba.
Procedimiento de la Prueba
1. Estadístico de la Prueba Fisher: El estadístico de la prueba Fisher se define como:

Aquí 𝑝𝑖 es el p-value asintótico de la prueba de raíz unitaria para la sección transversal 𝑖. Este
estadístico PPP sigue una distribución 𝜒 2 con 2𝑁 grados de libertad.
2. Pruebas Adicionales Propuestas por Choi:
Prueba Inversa Normal (Z): La prueba inversa normal transforma los p-values individuales
en puntajes z utilizando la función inversa de la distribución normal estándar y luego promedia
estos puntajes. Si el resultado promedio indica que muchos de los puntajes z son significativos,
se puede rechazar la hipótesis nula de que todas las series tienen raíces unitarias. Esta
transformación permite evaluar la significancia global de las pruebas individuales en una única
medida.
Prueba Logit: La prueba logit convierte los p-values individuales en log-odds (la razón
logarítmica de las probabilidades) y luego promedia estos valores. Esta transformación maneja
mejor los p-values extremos y ofrece una manera robusta de combinar la información de
múltiples pruebas. Un promedio significativo de log-odds indicaría que muchas de las series no
tienen raíces unitarias.
Prueba Modificada de Fisher (Pm): La prueba modificada de Fisher combina los p-values
ajustando el estadístico de prueba para grandes paneles de datos, asegurando que el resultado
tenga una distribución normal estándar en grandes muestras. Esta combinación de p-values

permite una evaluación robusta y precisa de la hipótesis global de raíces unitarias, mejorando la
interpretación y aplicación en estudios empíricos.
Aplicaciones y Limitaciones: Las pruebas combinadas de p-values son ideales para paneles grandes y
permiten la heterogeneidad entre las series temporales individuales, permitiendo que algunas sean
estacionarias y otras no. Además, no requieren un panel balanceado y permiten el uso de diferentes
longitudes de rezagos en las regresiones ADF individuales. Sin embargo, los p-values deben derivarse
mediante simulaciones de Monte Carlo, lo cual puede ser computacionalmente intensivo.
Importancia: Las pruebas combinadas de p-values tienen un rendimiento satisfactorio en muestras
pequeñas y ofrecen un poder ajustado al tamaño superior en comparación con la prueba IPS. En
particular, la prueba Z muestra un desempeño sobresaliente en términos de poder y precisión. Estas
pruebas han sido aplicadas a datos de tasas de cambio reales mensuales de EE.UU., encontrando
evidencia a favor de la hipótesis de paridad del poder adquisitivo (PPP), en contraste con la prueba IPS
Conclusión: Las pruebas de combinación de p-values ofrecen una metodología robusta y flexible para
la detección de raíces unitarias en paneles de datos, especialmente cuando se espera heterogeneidad
entre las series temporales individuales. Esta metodología mejora significativamente la precisión y el
realismo en la evaluación de la estacionariedad en estudios empíricos con datos de panel
12.2.5 PRUEBA LM BASADA EN RESIDUOS
Hadri (2000) desarrolló una prueba de Lagrange Multipliers (LM) basada en residuos para paneles de
datos, generalizando la prueba KPSS de series temporales a datos de panel. La hipótesis nula es que no
hay raíz unitaria en ninguna serie del panel, mientras que la alternativa es que existe una raíz unitaria en
al menos una serie del panel.
Procedimiento de la Prueba
1. Modelos Considerados:
Modelo con constante:
Modelo con constante y tendencia:
Aquí, 𝑟𝑖𝑡 es un paseo aleatorio y 𝜖𝑖𝑡 ∼ 𝐼𝐼𝑁(0, 𝜎𝜖2 ).
2. Estadístico LM: El estadístico LM se basa en los residuos obtenidos por MCO (Mínimos Cuadrados
Ordinarios) de las regresiones anteriores, y se utiliza la suma parcial de los residuos para calcularlo
3. Cálculo del Estadístico Z: Se transforma el estadístico LM en un estadístico Z para obtener una
distribución estándar normal. Esto se hace ajustando los valores obtenidos para modelos con constante
y modelos con tendencia.
Aplicaciones y limitaciones: La prueba LM basada en residuos es útil para detectar la ausencia de raíces
unitarias de manera eficiente en paneles grandes, manteniendo el tamaño empírico cerca del nivel
nominal del 5% para N y T grandes. Sin embargo, la prueba puede ser sensible a la especificación de
términos determinísticos y su potencia disminuye significativamente cuando se incluyen tendencias
lineales en el modelo.
Importancia: Hadri (2000) mostró mediante experimentos de Monte Carlo que la prueba LM tiene un
tamaño empírico cercano al nivel nominal del 5% para N y T suficientemente grandes. La prueba es
particularmente útil para validar la estacionariedad en paneles de datos, complementando otras pruebas
de raíces unitarias.
Conclusión: La prueba LM basada en residuos de Hadri es una herramienta potente para detectar la
ausencia de raíces unitarias en paneles de datos, especialmente útil en estudios con paneles grandes y
complejos. Su robustez y eficiencia la convierten en una opción valiosa para análisis de estacionariedad
en econometría

12.3 PRUEBAS DE RAÍCES UNITARIAS EN PANEL PERMITIENDO DEPENDENCIA
TRANSVERSAL
La dependencia transversal es una característica común en paneles de datos macroeconómicos, donde
las series temporales de diferentes países o regiones pueden estar correlacionadas entre sí. Pesaran
(2004) sugiere una prueba para detectar la dependencia transversal de los errores en modelos de panel,
lo cual es aplicable a paneles heterogéneos y dinámicos con 𝑇 corto y 𝑁 grande
Prueba CD de Pesaran: Se basa en el promedio de los coeficientes de correlación empírica de los
residuos obtenidos de las regresiones individuales en el panel. Esta prueba permite detectar si existe
dependencia transversal significativa entre las series temporales de diferentes unidades del panel.
Para su procedimiento, se calcula el coeficiente de correlación entre los residuos de cada par de
secciones transversales y se promedia. Si el promedio de estos coeficientes es significativamente
diferente de cero, se rechaza la hipótesis nula de independencia transversal.
Modelos de factor dinámico: Además de la prueba CD, varios modelos de factor dinámico han sido
propuestos para abordar la dependencia transversal:
1. Moon y Perron (2004): Proponen un modelo de factor común donde los errores tienen una estructura
de factor. Sugieren defactorizar los datos para construir una prueba de raíz unitaria utilizando
estimaciones de componentes principales para identificar los factores.
2. Phillips y Sul (2003): Proponen un modelo de factor de tiempo común con parámetros idiosincráticos
que miden el impacto de los efectos comunes. Utilizan un procedimiento de ortogonalización basado en
el método de momentos iterados y recomiendan una combinación de p-values de pruebas individuales
de raíz unitaria.
3. Bai y Ng (2004): Proponen un modelo de factor dinámico para separar la estacionariedad de los
factores y el componente idiosincrático, utilizando componentes principales estimados en diferencias
primeras de los datos.
4. Choi (2002): Usa un modelo de componentes de error donde las secciones transversales responden
homogéneamente a un único factor común. Sugiere descentrar los datos mediante GLS y combinar pvalues de pruebas ADF individuales.
Prueba CADF (Cross-Sectionally Augmented Dickey-Fuller) de Pesaran: Pesaran propone añadir
la media transversal rezagada y su primera diferencia en la regresión ADF para eliminar la dependencia
transversal. La prueba CADF promedia los t-estadísticos CADF de cada unidad del panel y proporciona
valores críticos específicos para diferentes combinaciones de 𝑁 y 𝑇.
Conclusión: Las pruebas de raíz unitaria que permiten la dependencia transversal, como la prueba CD
y los modelos de factor dinámico, abordan de manera efectiva la correlación entre series temporales de
diferentes unidades en un panel de datos. Estas metodologías ofrecen soluciones más precisas y robustas
para evaluar la estacionariedad en paneles grandes y complejos, mejorando la validez de los análisis
econométricos en estudios macroeconómicos.
12.4 REGRESIÓN ESPURIA EN DATOS DE PANEL
La regresión espuria ocurre cuando las variables no estacionarias parecen estar relacionadas cuando en
realidad no lo están, un problema identificado inicialmente en series temporales por Granger y Newbold
(1974). Entorf (1997) extendió este estudio a datos de panel con efectos fijos, mientras que Kao (1999)
y Phillips y Moon (1999) derivaron las distribuciones asintóticas de los estimadores de mínimos
cuadrados ordinarios (OLS) y otros estadísticos convencionales en la regresión espuria en datos de
panel.
Modelos y cointegración:

1. Definición de 𝜷: En series temporales no estacionarias con matriz de varianza a LP 𝜎, el
coeficiente 𝛽 puede interpretarse como un coeficiente de regresión a largo plazo. Si la matriz
de varianza a largo plazo 𝜎 tiene rango deficiente, 𝛽 es un coeficiente de cointegración porque
𝑦𝑡 − 𝛽𝑋𝑡 sería estacionario.
2. Extensión a Paneles:: Phillips y Moon (1999) extienden este concepto a regresiones de panel
con datos no estacionarios, permitiendo heterogeneidad entre individuos con matrices de
covarianza heterogéneas.
Casos de Modelos de Panel: Phillips y Moon (1999) consideraron cuatro casos, donde el estimador
agrupado será consistente y tendrá una distribución normal en el límite.
1. Regresión espuria de panel: No hay cointegración en series temporales.
2. Cointegración heterogénea de panel: Cada individuo tiene su propia relación de
cointegración.
3. Cointegración homogénea de panel: Los individuos tienen la misma relación de
cointegración.
4. Cointegración casi homogénea de panel: Los individuos tienen relaciones de cointegración
ligeramente diferentes.
Resultados y Estimaciones: En regresiones espurias de panel, el estimador de mínimos cuadrados es
√𝑁-consistente y tiene una distribución normal en el límite, y Phillips y Moon (1999) demostraron que
la regresión de sección cruzada con datos promediados en el tiempo también es √𝑁-consistente para 𝛽.
Detrending y Estimaciones: Phillips y Moon (2000) mostraron que el detrending mediante OLS es más
eficiente asintóticamente que el detrending mediante GLS para estimar el coeficiente 𝛽, luego ellos
también investigaron las propiedades asintóticas del estimador de máxima verosimilitud gaussiano
(MLE) en modelos de regresión dinámicos con tendencias determinísticas y estocásticas, encontrando
consistencia en el modelo homogéneo de tendencia.
Conclusión: La regresión espuria en paneles de datos puede ser consistente y tener una distribución
normal en el límite, diferente de las regresiones espurias de series temporales puras. Diversos modelos
y técnicas, como OLS, GLS y estimaciones de componentes principales, se utilizan para abordar la
heterogeneidad y la dependencia transversal en los datos de panel. Las investigaciones de Phillips y
Moon proporcionan un marco fundamental para el estudio de teorías de límite secuencial y conjunto en
datos de panel no estacionarios.
12.5 PRUEBAS DE COINTEGRACIÓN PANEL
Las pruebas de cointegración en paneles buscan ofrecer mayor potencia que las pruebas individuales de
series temporales, que tienen bajo poder con series cortas. En estudios como la paridad del poder
adquisitivo y la convergencia en el crecimiento, agrupar datos de países similares (G7, OCDE,
Eurozona) añade variación transversal, aumentando así el poder de estas pruebas.
12.5.1 Pruebas DF y ADF basadas en residuales (Pruebas de Kao)
Kao (1999) propuso pruebas de raíz unitaria tipo Dickey-Fuller (DF) y Augmented Dickey-Fuller (ADF)
para los residuales en un modelo de regresión de panel. El objetivo de estas pruebas es verificar la
hipótesis nula de no cointegración en datos de panel, mejorando la potencia y precisión respecto a las
pruebas individuales de series temporales.
Procedimiento de la Prueba: Su metodología se basa en utilizar residuos de efectos fijos para calcular
pruebas de tipo DF y ADF.
1. Modelo de Regresión: Se considera un modelo de regresión de panel con efectos fijos.

′
2. Estimación de Residuos: Se calculan los residuos de efectos fijos: 𝑒𝑖𝑡 = 𝑦𝑖𝑡 − 𝑥𝑖𝑡
𝛽, y el
modelo de prueba se define como 𝑒𝑖𝑡 = 𝜌𝑒𝑖𝑡−1 + 𝜈𝑖𝑡 , donde la hipótesis nula será: 𝐻0 : 𝜌 = 1
3. Cálculo del Estimador OLS de 𝝆 y el Estadístico t: Se estima 𝜌 mediante MCO y se calcula
el estadístico t correspondiente.
4. Pruebas tipo DF: Se realizan dos tipos de prueba DF:
a. DFρ y DFt: Consideran la exogeneidad fuerte de regresores y errores.
b. DFρ y DFt**: Consideran la cointegración con relación endógena entre regresores y
errores.
5. Prueba ADF: Se extiende el modelo de prueba incluyendo rezagos de las diferencias de los
𝑝
residuos: 𝑒𝑖𝑡 = 𝜌𝑒𝑖𝑡−1 + ∑𝑗=1 𝜗𝑗 𝛥𝑒𝑖𝑡−𝑗 + 𝜈𝑖𝑡. Luego, el estadístico DFA se ajusta para probar
la no cointegración.

Resultados y Distribución Asintótica: Los estadísticos 𝐷𝐹𝜌, 𝐷𝐹𝑡, 𝐷𝐹𝜌∗ , 𝐷𝐹𝑡 ∗ y DFA convergen a
una distribución normal estándar 𝑁(0,1) mediante teoría de límite secuencial. Esto permite realizar
inferencias robustas sobre la cointegración en paneles de datos.
Importancia y Validación: Las pruebas de Kao son esenciales para verificar la presencia de relaciones
de cointegración en paneles de datos. Proporcionan una mejora significativa en términos de poder y
precisión en comparación con las pruebas individuales de series temporales, siendo especialmente útiles
en estudios econométricos con datos de panel.
Conclusión: Las pruebas DF y ADF basadas en residuales de Kao son herramientas poderosas para
evaluar la hipótesis de no cointegración en datos de panel. Estas pruebas ofrecen una metodología
robusta y eficiente, mejorando la validez y fiabilidad de los análisis econométricos en paneles de datos
12.5.2 PRUEBA LM BASADA EN RESIDUOS
McCoskey y Kao (1998) desarrollaron una prueba basada en residuales para evaluar la hipótesis nula de
cointegración en paneles de datos, en lugar de la de no cointegración en paneles de datos, por lo que
extienden la prueba LM y la prueba LBI de la literatura de series temporales. Esta prueba es útil para
validar la cointegración en datos de panel.
Procedimiento de la Prueba:
1. Modelo de Regresión: La hipótesis nula de cointegración se define como 𝜃 = 0, donde 𝜃 es el
coeficiente de la variable de error.
2. Estimación Eficiente: Se utilizan estimadores como FM-OLS (Fully Modified Ordinary Least
Squares) y DOLS (Dynamic Ordinary Least Squares) para obtener estimaciones eficientes de
las variables cointegradas.
3. Cálculo del estadístico LM: El estadístico LM se basa en los residuos obtenidos de las
regresiones anteriores. Se define como el proceso de suma parcial de los residuales 𝑆𝑖𝑡 =
∑𝑡𝑗=1 𝑒𝑖𝑗 y la varianza de los residuales 𝜎𝑒2
4. Distribución asintótica: El resultado asintótico del estadístico LM es una distribución normal
estándar 𝑁(0,1). Los momentos 𝜇𝜈 y 𝜎𝜈2 se obtienen mediante simulaciones de Monte Carlo
para asegurar la robustez del estadístico ante heterocedasticidad.
Importancia y Validación: La prueba LM basada en residuales es robusta y eficaz para detectar la
cointegración en paneles de datos. Permite la validación de la cointegración al mantener un tamaño
empírico cercano al nivel nominal del 5% para paneles grandes. Los resultados muestran que esta prueba
es eficiente y puede complementar otras pruebas de cointegración en estudios econométricos con datos
de panel.
Conclusión: La prueba LM basada en residuales de McCoskey y Kao es una herramienta potente para
validar la cointegración en paneles de datos, proporcionando una metodología robusta y eficiente. Esta
prueba mejora la precisión y confiabilidad en la evaluación de la cointegración en estudios empíricos.

12.5.3 PRUEBA PEDRONI
La prueba de Pedroni (1999) está diseñada para probar la hipótesis nula de no cointegración en datos de
panel, permitiendo una considerable heterogeneidad entre las secciones transversales. Pedroni clasifica
las pruebas en dos categorías principales: pruebas basadas en el promediado de estadísticos de
cointegración de series temporales y pruebas de ratio de varianza de panel.
Clasificación de las Pruebas:
1. Primera Categoría: Promedio de estadísticos de cointegración de series temporales: Esta
categoría promedia los estadísticos de cointegración de las series temporales a través de las
secciones transversales. La prueba se basa en los métodos desarrollados por Phillips y Ouliaris
(1990). Aquí, los residuos estimados de cada sección transversal se utilizan para construir los
estadísticos de prueba, considerando las varianzas a largo plazo y contemporáneas de los
residuos.
Ejemplo de estadístico: Uno de los estadísticos se define como la media de las razones entre las
varianzas contemporáneas y las varianzas a largo plazo de los residuos, proporcionando una
medida robusta de cointegración en presencia de heterogeneidad.
2. Segunda Categoría: Estadísticas de Ratio de Varianza de Panel: Pedroni define cuatro
estadísticas diferentes dentro de esta categoría, que comparan las varianzas de los residuos
estimados. Estas pruebas evalúan la cointegración en el contexto de la heterogeneidad permitida
entre las secciones transversales, asegurando que las relaciones cointegradas sean robustas a las
diferencias individuales.
Ejemplo de estadístico: Una de las estadísticas es la ratio de la varianza de los residuos a corto
plazo frente a la varianza de los residuos a largo plazo, evaluando si las series se mueven
conjuntamente a largo plazo pese a la variabilidad a corto plazo.
Distribución Asintótica: Las pruebas de Pedroni tienen una distribución asintótica normal estándar
bajo la hipótesis nula de no cointegración. Esto permite la aplicación de estas pruebas en grandes paneles
de datos con múltiples secciones transversales, mejorando la capacidad de inferencia sobre la
cointegración en estudios empíricos.
Intuición de la prueba: La convergencia en la distribución se basa en la convergencia individual de los
términos del numerador y denominador en las estadísticas de prueba. Si suficientes secciones
transversales muestran desviaciones significativas de los valores esperados bajo la hipótesis nula, la
prueba rechazará la hipótesis nula, indicando la presencia de cointegración.
Importancia y Validación: Las pruebas de Pedroni permiten una considerable heterogeneidad en las
relaciones de cointegración, proporcionando una metodología robusta para analizar la cointegración en
datos de panel. Estas pruebas son esenciales para estudios macroeconómicos y financieros donde las
relaciones a largo plazo entre variables pueden variar significativamente entre diferentes países o
regiones.
Conclusión: Las pruebas de Pedroni ofrecen una metodología robusta y flexible para detectar la
cointegración en paneles de datos, permitiendo la heterogeneidad entre las secciones transversales. Esta
capacidad de manejar diferencias individuales mejora significativamente la validez y precisión de los
análisis econométricos en estudios empíricos con datos de panel
12.5.4 PRUEBA DE COINTEGRACIÓN BASADA EN LA VEROSIMILITUD
Su objetivo es probar el rango de cointegración en modelos de panel heterogéneos. Para ello, su método
se basa en usar el promedio de los estadísticos de traza individuales de Johansen, y sus resultados son
que se requiere una gran dimensión de series temporales para evitar distorsiones de tamaño.
12.5.5 PROPIEDADES EN MUESTRAS FINITAS:
El análisis de propiedades en muestras finitas se centra en evaluar el rendimiento y la validez de
diferentes pruebas de cointegración cuando se aplican a conjuntos de datos de tamaño limitado. Este

estudio es crucial para entender cómo los resultados de estas pruebas pueden variar y qué tan confiables
son bajo condiciones de muestra finita.
Comparaciones de Tamaño y Poder de Pruebas de Cointegración
1. Pruebas ADF Promedio y Pedroni: Estas pruebas han demostrado ser efectivas en términos
de poder, lo que significa que tienen una buena capacidad para detectar cointegración cuando
realmente está presente en los datos.
Son utilizadas para verificar la presencia de relaciones a largo plazo entre variables en paneles
de datos, siendo robustas a la heterogeneidad entre las secciones transversales.
3. Pruebas Basadas en la Hipótesis Nula de Cointegración (LM de McCoskey y Kao): Estas
pruebas son eficaces para determinar si las series de datos subyacentes están cointegradas bajo
la hipótesis nula. Son particularmente útiles cuando la hipótesis de cointegración es más lógica
que la de no cointegración.
Aunque tienen un buen desempeño en ciertos contextos, su principal fortaleza radica en evaluar
la cointegración como hipótesis nula.
Relevancia:
•
•

Resolución de Problemas de Bajo Poder en Pruebas Individuales: Las pruebas de
cointegración en panel, como las mencionadas anteriormente, abordan eficazmente el problema
del bajo poder que se observa en las pruebas de cointegración aplicadas a series individuales.
Utilidad de las Pruebas LM: Estas pruebas son particularmente útiles cuando se trabaja con
datos donde la hipótesis de cointegración es más probable. Ayudan a validar la presencia de
relaciones a largo plazo entre las variables estudiadas.

Conclusión: El estudio de las propiedades en muestras finitas resalta la importancia de seleccionar la
prueba adecuada según el contexto del estudio y la naturaleza de los datos. Las pruebas de ADF
promedio y las pruebas de Pedroni son recomendables por su buen desempeño en poder, mientras que
las pruebas LM son valiosas para validar la hipótesis de cointegración.
12.6 ESTIMACIÓN E INFERENCIA EN MODELOS DE COINTEGRACIÓN DE PANEL
Los modelos de cointegración en panel se utilizan para estudiar relaciones económicas a largo plazo,
típicas en datos macroeconómicos y financieros. Las propiedades asintóticas de los estimadores de
coeficientes de regresión en estos modelos difieren de los modelos de cointegración de series
temporales.
Principales Métodos de Estimación
1. Estimador OLS: El estimador OLS presenta un sesgo considerable en muestras finitas y no es
consistente en paneles, en contraste con su uso en series temporales individuales.
2. Estimador FM (Fully Modified OLS): Propuesto por Phillips y Moon (1999) y extendido por
Pedroni (2000), este método generaliza el trabajo de Phillips y Hansen (1990). Aunque es
asintóticamente normal, no siempre mejora los resultados obtenidos con OLS.
3. Estimador DOLS (Dynamic OLS): Propuesto por Kao y Chiang (2000), este estimador es más
prometedor que OLS o FM para regresiones cointegradas en paneles. Utiliza datos ajustados
para corregir sesgos y obtener una distribución normal en el límite.
Comparación de métodos: Kao y Chiang (2000) sugieren que el DOLS es superior al OLS y FM en
regresiones cointegradas de panel, mientras que Chen, McCoskey y Kao (1999): Analizaron las
propiedades de muestra finita del OLS y los estimadores corregidos por sesgo, concluyendo que FM y
DOLS son alternativas más prometedoras.
Conclusiones: Es fundamental corregir los sesgos en estimaciones OLS para mejorar la precisión, y el
DOLS es recomendado sobre OLS y FM en muchos casos. Asimismo, la independencia transversal es
una suposición clave, y las técnicas deben considerar la posibilidad de raíces unitarias cercanas en lugar
de exactas.

Recomendaciones: Considerar el sesgo en estimaciones OLS y corregirlo con métodos como FM y
DOLS, y tener cuidado al interpretar resultados empíricos de métodos recientes de cointegración en
panel que asumen raíces unitarias exactas, ya que las raíces unitarias cercanas también son plausibles.
12.7 EMPIRICAL EXAMPLE: PARIDAD DEL PODER ADQUISITIVO
Banerjee et al. (2005) revisan la literatura empírica sobre la validez de la paridad de poder adquisitivo
(PPP). La versión fuerte de la PPP prueba si el tipo de cambio real es estacionario. Generalmente, se
encuentra que la PPP se mantiene cuando se prueba en datos de panel, pero no cuando se prueba
individualmente por país. Esta discrepancia puede deberse al sobreajuste presente cuando las relaciones
de cointegración vinculan a los países del panel.
Hallazgos de Banerjee et al. (2005):
Se encuentra que los tests de raíz unitaria de panel son más poderosos que los tests univariados, y la
existencia de relaciones de cointegración entre los países del panel puede causar un tamaño empírico
mayor que el nominal, lo que lleva al rechazo de la hipótesis nula de una raíz unitaria con demasiada
frecuencia.
Asimismo, usando datos trimestrales de tipos de cambio reales para el período 1975:1–2002:4 para 18
países de la OCDE, encontraron que la hipótesis nula de una raíz unitaria no se rechazó para la mayoría
de los países cuando se probó individualmente, excepto para Francia y Corea cuando Alemania es el
numerario, y ajustando los valores críticos para la presencia de cointegración entre países, las rechazos
disminuyen significativamente.
Tabla 12.1: Prueba de Raíz Unitaria en Panel para Tipos de Cambio Reales
Resumen de la Prueba:
•
•
•
•
•

Muestra: 1975Q1-2002Q4
Series: Tipos de cambio reales de
18 países de la OCDE con
Alemania como numerario
Variables exógenas: Efectos
individuales
Selección
automática
de
retrasos: Basado en SIC, de 0 a 8
Selección de ancho de banda de
Newey-West usando kernel de
Bartlett

Interpretación de Resultados
•
•

Los resultados confirman que todas las pruebas de raíz unitaria de panel, incluyendo efectos
individuales, rechazan la hipótesis nula de una raíz unitaria común.
La prueba de Hadri, que invierte la hipótesis nula, rechaza la hipótesis de que no hay raíz unitaria
en favor de una raíz unitaria común en el panel.

Conclusiones
•
•
•

Es necesario tener precaución al usar métodos de panel para probar raíces unitarias en series
temporales macroeconómicas.
Las pruebas deben considerar la posibilidad de cointegración entre las unidades del panel.
Banerjee et al. (2004) sugieren extraer las tendencias comunes de cada unidad usando el método
de máxima verosimilitud de Johansen y probar la cointegración entre estas tendencias antes de
aplicar métodos de cointegración en panel.

BASIC LINEAR UNOBSERVED EFFECTS PANEL DATA MODELS (WOOLRIDGE CAP 10)
10.1 MOTIVATION: OMITTED VARIABLES PROBLEM
Se pueden usar los datos de panel, al menos bajo ciertos supuestos, para obtener estimadores consistentes
en presencia de variables omitidas. Las variables omitidas son aquellas que no se incluyen en el modelo
pero que afectan la variable dependiente. Estas variables pueden causar sesgo en las estimaciones si
están correlacionadas con las variables explicativas observadas.
Las variables omitidas pueden influir en los resultados de nuestro modelo. En un modelo lineal, si 𝑦 es
la variable dependiente, 𝑥 son las variables explicativas observables y 𝑐 representa las variables
omitidas, tenemos la siguiente función de regresión poblacional:
𝐸(𝑦|𝑥, 𝑐) = 𝛽0 + 𝑥𝛽 + 𝑐
Aquí, 𝛽 son los coeficientes a estimar. Si no podemos controlar 𝑐 (la variable omitida), las estimaciones
de 𝛽 serán sesgadas. Esto significa que las conclusiones sobre la relación entre 𝑥 e 𝑦 pueden ser
incorrectas.
Para obtener estimaciones consistentes y no sesgadas de 𝛽, es crucial mantener 𝑐 constante. Sin
información adicional que permita controlar 𝑐, no podemos estar seguros de que nuestras estimaciones
sean correctas. Los datos de panel permiten controlar estos efectos no observados mediante la inclusión
de efectos fijos o aleatorios, lo que mejora la precisión y validez de las estimaciones.
Bajo supuestos adicionales, hay formas de abordar el problema 𝐶𝑜𝑣(𝑥, 𝑐) ≠ 0. Hemos cubierto al menos
tres posibilidades en el contexto del análisis de sección transversal.
1. Podríamos ser capaces de encontrar una variable proxy adecuada para 𝑐, en cuyo caso podemos
estimar una ecuación por OLS donde el proxy está conectado para 𝑐.
2. Podemos encontrar instrumentos para los elementos de 𝑥 que están correlacionados con 𝑐 y usar
un método de variables instrumentales, como 2SLS.
3. Podemos ser capaces de encontrar indicadores de 𝐶 que luego se pueden utilizar en el
procedimiento de variables instrumentales de indicadores múltiples.
Si podemos observar las mismas unidades de sección transversal en diferentes puntos en el tiempo, es
decir, si podemos recopilar un conjunto de datos de panel, entonces surgen otras posibilidades.
Una variable no observada y constante en el tiempo se denomina efecto no observado en el análisis de
datos de panel.
Si asumiéramos 𝐸(𝑥𝑡′ 𝑐) = 0, podríamos aplicar OLS agrupados. Si 𝑐 se correlaciona con cualquier
elemento de 𝑥𝑡 , entonces el OLS agrupado está sesgado y es inconsistente.
Con dos años de datos podemos hacer una ecuación de diferencia a través de los dos períodos de tiempo
para eliminar la constante de tiempo inobservable, 𝑐.
¿Bajo qué supuestos será consistente el estimador OLS de la siguiente ecuación?
∆𝑦 = ∆𝑥𝛽 + ∆𝑢
La condición clave para que OLS estime consistentemente 𝛽 son las condiciones de ortogonalidad:
𝐸(∆𝑥∆𝑢) = 0, y la condición de rango 𝐸(∆𝑥 ′ ∆𝑥) = 𝐾
Si una variable en 𝑥𝑡 es constante a lo largo del tiempo para cada miembro de la población, entonces ∆𝑥
será cero, lo que viola la condición de rango necesaria para estimar consistentemente 𝛽𝑗 . Esto se debe a
que no podemos distinguir el efecto de una variable constante del efecto de 𝑐. Por lo tanto, solo podemos
estimar consistentemente 𝛽𝑗 si hay variación en 𝑥𝑡𝑗 a lo largo del tiempo.

Un panel equilibrado es aquel en el que todas las unidades de sección transversal se observan en los
mismos períodos de tiempo. En cambio, un panel desequilibrado, donde los períodos de observación
varían entre las unidades, requiere un tratamiento cuidadoso para evitar problemas de selección de
muestras.
En estudios de grandes regiones geográficas, la suposición de muestreo aleatorio en la dimensión
transversal puede ser inapropiada. Sin embargo, si el tamaño de la población (\(N\)) es grande en
comparación con el tiempo 𝑇, y se puede asumir independencia entre las secciones transversales, el
análisis asintótico es adecuado. Cuando 𝑇 es del mismo orden que 𝑁, se necesita un análisis que haga
suposiciones explícitas sobre la dependencia temporal. Si 𝑇 es mucho mayor que 𝑁, el enfoque se
convierte en un análisis de series temporales múltiples, manteniendo 𝑁 fijo mientras 𝑇 tiende a infinito.
En resumen, para estimar consistentemente los coeficientes en datos de panel, es crucial tener variación
en las variables explicativas a lo largo del tiempo y manejar adecuadamente la estructura del panel, ya
sea equilibrado o desequilibrado.
10.2 SUPUESTOS SOBRE LOS EFECTOS NO OBSERVADOS Y LAS VARIABLES
EXPLICATIVAS
Antes de analizar los métodos de estimación de datos de panel, es útil discutir la naturaleza de los efectos
no observados y ciertas características de las variables explicativas observadas.
10.2.1 ¿EFECTOS ALEATORIOS O FIJOS?
El modelo básico de efectos no observados (UEM) puede incluir variables que cambian a lo largo del
tiempo pero no entre unidades, que cambian entre unidades pero no a lo largo del tiempo, y que cambian
tanto a lo largo del tiempo como entre unidades. El efecto no observado 𝑐𝑖 puede ser llamado
componente no observado, variable latente o heterogeneidad no observada. Si 𝑖 representa individuos,
𝑐𝑖 se denomina efecto o heterogeneidad individual; términos similares se aplican a familias, empresas,
ciudades y otras unidades. Los 𝑢𝑖𝑡 son los errores idiosincráticos que varían tanto a lo largo del tiempo
como entre unidades.
En trabajos metodológicos y aplicaciones, se discute si 𝑐𝑖 debe tratarse como un efecto aleatorio o fijo.
Originalmente, esto se basaba en si 𝑐𝑖 es una variable aleatoria o un parámetro a estimar. En los modelos
tradicionales de datos de panel, 𝑐𝑖 es un "efecto aleatorio" si es una variable aleatoria y un "efecto fijo"
si es un parámetro para cada observación 𝑖. Sin embargo, para aplicaciones de datos de panel
microeconómicos, es más adecuado tratar 𝑐𝑖 como selecciones aleatorias de la población, junto con 𝑣𝑖𝑡
y 𝑥𝑖𝑡 . El punto clave es si 𝑐𝑖 está correlacionado con las variables explicativas observadas 𝑥𝑖𝑡 .
Mundlak (1978) argumentó que "efecto aleatorio" es sinónimo de correlación cero entre las variables
explicativas observadas y el efecto no observado (𝐶𝑜𝑣(𝑥𝑖𝑡 , 𝑐𝑖 ) = 0, 𝑡 = 1,2, … , 𝑇. En trabajos aplicados,
cuando se refiere a 𝑐𝑖 como un "efecto aleatorio individual", se asume que no está correlacionado con
𝑥𝑖𝑡 .
En aplicaciones microeconómicas, el término "efecto fijo" no implica necesariamente que 𝑐𝑖 sea tratado
como no aleatorio; más bien, indica que se permite una correlación arbitraria entre el efecto no
observado 𝑐𝑖 y las variables explicativas observadas 𝑥𝑖𝑡 . Por lo tanto, si 𝑐𝑖 se denomina "efecto fijo
individual" o "efecto fijo de la empresa", en términos prácticos significa que se permite que 𝑐𝑖 esté
correlacionado con 𝑥𝑖𝑡 .
En resumen, la diferencia entre efectos aleatorios y fijos radica en la correlación entre 𝑐𝑖 y 𝑥𝑖𝑡 . Los
efectos aleatorios asumen que no hay correlación, mientras que los efectos fijos permiten esta
correlación, proporcionando una estimación más robusta cuando la correlación es probable.
10.2.2 SUPUESTOS
EXPLICATIVAS

DE

EXOGENEIDAD

ESTRICTA

SOBRE

LAS

VARIABLES

En los modelos tradicionales de datos de panel, se asume que las variables explicativas (\(x_{it}\)) son
fijas y no aleatorias para evitar la correlación con los errores. Este supuesto ayuda a evitar el feedback
entre 𝑦𝑖𝑡 y 𝑥𝑖𝑡 para 𝑠 > 𝑡.
Para realizar inferencias y asegurar la eficiencia de las estimaciones, se necesita expresar el supuesto de
exogeneidad estricta en términos de expectativas condicionales. Este supuesto indica que, dado el
conjunto completo de variables explicativas y el efecto no observado (\(c_i\)), las expectativas de la
variable dependiente \(y_{it}\) se expresan como:
𝐸(𝑦𝑖𝑡 |𝑥𝑖1 , 𝑥𝑖2 , … , 𝑥𝑖𝑇 , 𝑐𝑖 ) = 𝑥𝑖𝑡 𝛽 + 𝑐𝑖
Esto implica que, una vez controladas las variables explicativas y 𝑐𝑖 , las variables explicativas no tienen
ningún efecto parcial sobre 𝑦𝑖𝑡 para 𝑠 ≠ 𝑡.
Si se cumple el supuesto de exogeneidad estricta, las variables explicativas (𝑥𝑖𝑡 ) son estrictamente
exógenas condicionales al efecto no observado (𝑐𝑖 ). Este supuesto es crucial porque asegura que no hay
correlación entre (𝑥𝑖𝑡 ) y los errores idiosincráticos (𝑢𝑖𝑡 )
Un ejemplo puede ayudar a entender esto: supongamos que 𝑦𝑖𝑡 es la producción de soja en una granja,
y 𝑥𝑖𝑡 incluye capital, trabajo, materiales y otros insumos observables. El efecto no observado (𝑐𝑖 ) podría
ser la capacidad de gestión de la familia dueña de la granja. Si los insumos utilizados en otros años no
afectan la producción actual, el supuesto de exogeneidad estricta es razonable.
El supuesto de exogeneidad estricta implica que: 𝐸(𝑢𝑖𝑡 |𝑥𝑖1 , 𝑥𝑖2 , … , 𝑥𝑖𝑇 ) = 0
Esto asegura que las variables explicativas no estén correlacionadas con los errores en ningún período,
lo cual es más fuerte que asumir una correlación contemporánea cero: 𝐸(𝑥𝑖𝑡 𝑢𝑖𝑡 ) = 0, ∀𝑡
Este supuesto permite que 𝑐𝑖 y 𝑥𝑖𝑡 estén correlacionados, pero no que 𝑥𝑖𝑡 y 𝑢𝑖𝑡 lo estén. Para la
consistencia de los estimadores de datos de panel, generalmente basta con el supuesto de correlación
cero.
En resumen, el supuesto de exogeneidad estricta es fundamental para asegurar que las estimaciones de
los modelos de datos de panel sean consistentes y eficientes, controlando adecuadamente la correlación
entre las variables explicativas y los errores.
10.2.3 Algunos Ejemplos de Modelos de Datos de Panel con Efectos No Observados
Ejemplo 10.1: Evaluación de Programas: Un modelo estándar para estimar los efectos de la formación
laboral en los salarios posteriores se representa como:
𝑙𝑜𝑔(𝑤𝑎𝑔𝑒𝑖𝑡 ) = 𝜃𝑡 + 𝑧𝑖𝑡 𝛾 + 𝛿1 𝑝𝑟𝑜𝑔𝑖𝑡 + 𝑐𝑖 + 𝑢𝑖𝑡
Este modelo se utiliza en estudios microeconómicos para evaluar el impacto de programas sobre salarios
usando datos de panel. Aquí, 𝑦𝑖𝑡 representa un intercepto variable en el tiempo, mientras que 𝑧𝑖𝑡 son
características observables que pueden estar correlacionadas con la participación en el programa. Los
datos suelen recopilarse antes y después del programa, permitiendo evaluar el efecto del mismo. El
efecto individual 𝑐𝑖 captura habilidades no observadas y aborda el problema de auto-selección. La
exogeneidad estricta de las variables explicativas, como 𝑝𝑟𝑜𝑔𝑖𝑡 , es crucial para evitar la correlación
entre los errores y la participación futura en el programa.
Ejemplo 10.2: Modelo de Retardo Distribuido: Hausman, Hall y Griliches (1984) desarrollaron
modelos no lineales de retardo distribuido para estudiar la relación entre las patentes y los gastos de
I+D. Un modelo lineal es:
𝑝𝑎𝑡𝑒𝑛𝑡𝑠𝑖𝑡 = 𝜃𝑡 + 𝑧𝑖𝑡 𝛾 + 𝛿0 𝑅𝐷𝑖𝑡 + 𝛿1 𝑅𝐷𝑖,𝑡−1 + ⋯ + 𝛿5 𝑅𝐷𝑖,𝑡−5 + 𝑐𝑖 + 𝑢𝑖𝑡
Aquí, 𝑅𝐷𝑖𝑡 es el gasto en I+D y 𝑧𝑖𝑡 incluye variables como el tamaño de la empresa. El término 𝑐𝑖
captura la heterogeneidad de la empresa, que puede correlacionarse con los gastos actuales y futuros en
I+D. Este modelo permite analizar el efecto persistente de I+D en las patentes. Si los gastos en I+D están

correlacionados con 𝑐𝑖 o si los choques en patentes influyen en el gasto futuro en I+D, la exogeneidad
estricta puede fallar, afectando la validez del modelo.
Ejemplo 10.3: Variable Dependiente con Retardo: Un modelo dinámico para la determinación de
salarios con heterogeneidad no observada es:
𝑙𝑜𝑔(𝑤𝑎𝑔𝑒𝑖𝑡 ) = 𝛽1 𝑙𝑜𝑔(𝑤𝑎𝑔𝑒𝑖,𝑡−1 ) + 𝑐𝑖 + 𝑢𝑖𝑡
Aquí, el interés se centra en la persistencia de los salarios, controlando la heterogeneidad no observada
(𝑐𝑖 ). Supongamos que 𝑦𝑖𝑡 es la productividad individual, la cual puede influir en los salarios futuros.
Bajo la suposición de que 𝑢𝑖𝑡 no está correlacionado con 𝑥𝑖𝑡 y sus rezagos, este modelo busca capturar
la dinámica de los salarios. Sin embargo, si 𝑢𝑖𝑡 está correlacionado con 𝑥𝑖,𝑡−1 o futuras variables
explicativas, la exogeneidad estricta falla, haciendo inaplicable el supuesto para MCO agrupado.
10.3: ESTIMACIÓN DE MODELOS DE EFECTOS NO OBSERVADOS MEDIANTE
POOLED OLS
Modelo y Suposiciones: La estimación por Pooled OLS puede proporcionar un estimador consistente
de 𝛽 en el modelo 𝑦𝑖𝑡 = 𝑥𝑖𝑡 𝛽 + 𝑣𝑖𝑡 , donde 𝑣𝑖𝑡 = 𝛼𝑖 + 𝑢𝑖𝑡 . Aquí, 𝜐𝑖𝑡 es la suma de los efectos no
′
observados y un error idiosincrático. La consistencia del estimador Pooled OLS requiere que 𝐸(𝑥𝑖𝑡
𝑣𝑖𝑡
′
) = 0, es decir, no debe haber correlación entre 𝑥𝑖𝑡 y 𝜈𝑖𝑡 . Esto implica dos condiciones: 𝐸(𝑥𝑖𝑡 𝑢𝑖𝑡 ) = 0 y
′
𝐸(𝑥𝑖𝑡
𝛼𝑖 ) = 0
′
Suposición Restrictiva: La condición 𝐸(𝑥𝑖𝑡
𝛼𝑖 ) = 0 es más restrictiva y se debe cumplir si se ha
modelado correctamente 𝐸(𝑦𝑖𝑡 ∣ 𝑥𝑖𝑡 , 𝛼𝑖 ). En modelos estáticos y con retardos distribuidos finitos, a
veces se acepta esta suposición, aunque los modelos con variables dependientes rezagadas en 𝑥𝑖𝑡 tienden
a violarla porque 𝑦𝑖,𝑡−1 y 𝑎𝑖 están correlacionados.
′
Errores Compuestos y Correlación Serial:: Incluso si se cumple 𝐸(𝑥𝑖𝑡
𝛼𝑖 ) = 0, los errores
compuestos 𝜈𝑖𝑡 estarán correlacionados serialmente debido a la presencia de 𝑎𝑖 en cada período. Por lo
tanto, la inferencia usando Pooled OLS requiere el uso de una matriz de varianza robusta y estadísticas
de prueba robustas. La correlación entre 𝜈𝑖𝑡 y 𝜈𝑖𝑠 no disminuye generalmente a medida que aumenta la
distancia entre 𝑡 y 𝑠, lo que significa que los 𝜈𝑖𝑡 no son débilmente dependientes a lo largo del tiempo.

Importancia de Asintóticos de Gran N y T Fijo: Es crucial poder aplicar asintóticos de gran 𝑁 y 𝑇
fijo cuando se utiliza Pooled OLS para asegurar inferencias válidas.
Organización de los Datos: Cada conjunto (𝑦𝑖 , 𝑋𝑖 ) debe tener 𝑇 filas y ordenarse cronológicamente.
El orden de las observaciones de la sección transversal es irrelevante, lo importante es mantener el orden
temporal dentro de cada unidad para un análisis adecuado.
Conclusión: El uso de Pooled OLS en modelos con efectos no observados requiere supuestos
específicos sobre la falta de correlación entre los regresores y los efectos no observados. Aunque estas
suposiciones se cumplan, es esencial tener en cuenta la correlación serial de los errores compuestos y
ajustar las inferencias en consecuencia, enfatizando la importancia de técnicas robustas y un enfoque
adecuado para análisis de series temporales y datos de panel.
10.4: MÉTODOS DE EFECTOS ALEATORIOS
10.4.1 Estimación e Inferencia bajo las Suposiciones Básicas de Efectos Aleatorios:
Suposiciones Básicas: El análisis de efectos aleatorios en datos de panel incluye 𝑐𝑖 en el término de
error, y requiere más suposiciones que el método de OLS agrupados. La clave es la exogeneidad estricta
y la ortogonalidad entre 𝑐𝑖 y 𝑥𝑖𝑡 . Las suposiciones RE.1 establecen que:
𝐸(𝑢𝑖𝑡 |𝑥𝑖 , 𝑐𝑖 ) = 0
𝐸(𝑐𝑖 |𝑥𝑖 ) = 𝐸(𝑐𝑖 )

Modelo y Estimación: La suposición RE.1 es más restrictiva que la necesaria para un análisis OLS
agrupado, pero es esencial para el enfoque de efectos aleatorios. Esta suposición implica que \(c_i\) no
está correlacionado con las variables explicativas. Usamos la matriz de varianza incondicional de los
errores compuestos para ajustar la correlación serial en los errores.
Condición de Rango para GLS: Para la consistencia del método de Mínimos Cuadrados Generalizados
(GLS), se necesita una condición de rango:
′

𝑟𝑎𝑛𝑘 𝐸(𝑋𝑖′ 𝛺−1 𝑋𝑖 ) = 𝐾
Matriz de Varianza: La matriz de varianza incondicional de los errores compuestos se define como:
𝛺 = 𝜎𝑢2 𝐼𝑇 + 𝜎𝑐2 𝐽𝑇
Esto permite que 𝛺 dependa de dos parámetros principales, 𝜎𝑢2 y 𝜎𝑐2 , que capturan la varianza de los
errores idiosincráticos y la varianza de los efectos no observados, respectivamente.
Implementación de FGLS: Para implementar el método de GLS factible (FGLS), asumimos que:
𝐸(𝑢𝑖𝑡 𝑢𝑖𝑠 |𝑥𝑖 , 𝑐𝑖 ) = 𝜎𝑢2 𝐼𝑇
𝐸(𝑐𝑖2 |𝑥𝑖 ) = 𝜎𝑐2
Esto nos permite definir una matriz de varianza para los errores compuestos y utilizarla para estimar los
coeficientes mediante FGLS:

Estimación de Parámetros: Para implementar el enfoque de efectos aleatorios, necesitamos estimar
los parámetros 𝜎𝑢2 y \𝜎𝑐2 . Utilizamos los residuos obtenidos del estimador OLS agrupado para estimar
𝜎𝑢2 y 𝜎𝑐2 :

Este procedimiento asegura que las estimaciones sean consistentes bajo las suposiciones RE.1-RE.3.
Conclusión: El enfoque de efectos aleatorios requiere suposiciones estrictas sobre la exogeneidad y la
estructura de la varianza para obtener estimaciones válidas. Aunque implica restricciones adicionales en
comparación con OLS agrupado, proporciona estimaciones más eficientes y consistentes al considerar
la correlación serial en los errores compuestos. La implementación cuidadosa de FGLS y la adecuada
consideración de la estructura de los datos de panel son esenciales para lograr resultados fiables.
10.4.2 Estimador Robusto de la Matriz de Varianza
La suposición RE.3, aunque estricta, no causa inconsistencia en el estimador RE. Sin embargo, es útil
poder realizar inferencias estadísticas sin depender de esta suposición. La suposición RE.3 puede fallar
por varias razones, como la falta de estructura de efectos aleatorios en la matriz de varianza de los errores
compuestos.
Uso de Errores Estándar Robustos: Para abordar esta posible falla, se utilizan errores estándar
robustos que no dependen de la suposición RE.3. Esto se logra utilizando la matriz de varianza robusta
y calculando estadísticos de Wald robustos. La fórmula para los estadísticos de Wald robustos es:
𝑊 = (𝑅𝑏 − 𝑟)′ (𝑅′ 𝑉𝑅′ )−1 (𝑅𝑏 − 𝑟)
donde 𝑉̂ es el estimador de la matriz de varianza robusta. Si la suposición RE.3 está violada, esta forma
robusta asegura que los resultados de los estadísticos F y otros análisis sean válidos.
Beneficios de la Matriz de Varianza Robusta: La idea detrás del uso de una matriz de varianza robusta
es aprovechar una técnica bien establecida bajo las suposiciones RE.1-RE.3. Al hacer el análisis robusto,

se mejora la confiabilidad de las inferencias estadísticas, incluso si \(T\) es fijo y \(N\) es grande. Esto
asegura que las inferencias sean válidas y que los errores estándar y estadísticos robustos se mantengan
consistentes.
Implementación Práctica: En la práctica, el uso de la matriz de varianza robusta facilita obtener errores
estándar robustos y estadísticos como \(t\) y \(F\). Esto es particularmente útil en aplicaciones donde la
suposición RE.3 puede no ser completamente válida. En la Sección 10.7.2, se describe cómo obtener el
estimador RE a partir de una regresión OLS agrupada, que proporciona una manera práctica de aplicar
estos métodos robustos.
Conclusión: El uso de estimadores robustos de la matriz de varianza es esencial para asegurar
inferencias estadísticamente válidas en modelos de efectos aleatorios. Aunque la suposición RE.3 puede
ser restrictiva, los errores estándar robustos y los estadísticos de Wald permiten realizar análisis
confiables sin depender completamente de esta suposición. Esta práctica es crucial para mejorar la
precisión y confiabilidad de los resultados en el análisis de datos de panel.
10.4.3 Un Análisis General de Mínimos Cuadrados Generalizados Factibles (FGLS)
Introducción: El análisis de Mínimos Cuadrados Generalizados Factibles (FGLS) es útil cuando los
errores idiosincráticos 𝑢𝑖𝑡 son heterocedásticos y están correlacionados en serie. En estas situaciones,
se puede utilizar un estimador más general de la matriz de varianza Ω en FGLS:
𝑁

𝛺 = 𝑁 −1 ∑ 𝑣̂𝑖 𝑣̂𝑖 ′
𝑖=1
donde 𝑣̂𝑖 son los residuos OLS agrupados. Este estimador es consistente bajo las suposiciones RE.1 y
RE.2 y es asintóticamente eficiente, tomando la forma usual de la matriz de varianza.
Comparación con RE: Usar la ecuación (10.38) proporciona una estimación de Ω que es más general
que el análisis de efectos aleatorios tradicional (RE). Para muestras grandes, el estimador FGLS es tan
eficiente como el RE bajo RE.1-RE.3 y más eficiente si Ω no tiene la estructura de efectos aleatorios.
Históricamente, se prefieren los métodos RE sobre FGLS general debido a que los errores compuestos
𝑣𝑖𝑡 estaban asociados con efectos no observados.
Ventajas de FGLS: Si 𝑁 es mucho mayor que 𝑇, un análisis FGLS no restringido puede tener mejores
propiedades porque la estimación Ω tiene 𝑇(𝑇 + 1)/2 elementos, proporcionando mayor flexibilidad.
Los efectos aleatorios solo necesitan dos parámetros de varianza para cualquier 𝑇. Con un 𝑁 muy
grande, la estimación general de Ω es una alternativa atractiva, especialmente si la estimación muestra
un patrón diferente al de los efectos aleatorios.
Implementación y Aplicaciones: Como punto intermedio entre un análisis de efectos aleatorios
tradicional y un análisis FGLS completo, podríamos especificar una estructura particular para la matriz
de varianza del error idiosincrático 𝐸(𝑢𝑢′ ). Esto permite capturar adecuadamente la heterocedasticidad
y la correlación serial en los errores, mejorando la eficiencia de las estimaciones y proporcionando
inferencias más robustas.
Conclusión: El análisis FGLS proporciona una herramienta poderosa para manejar heterocedasticidad
y correlación serial en modelos de datos de panel. Aunque el método de efectos aleatorios es
tradicionalmente preferido, FGLS ofrece una mayor flexibilidad y eficiencia en muestras grandes. Esta
metodología es especialmente útil cuando la estructura de varianza de los errores no sigue el patrón de
efectos aleatorios, permitiendo obtener estimaciones más precisas y confiables.
10.4.4 Prueba para la Presencia de un Efecto No Observado
Introducción: Para determinar si un modelo de efectos aleatorios es apropiado, debemos probar la
presencia de efectos no observados 𝜎𝑐2 . Si las suposiciones RE.1 a RE.3 se cumplen, pero el modelo no
contiene un efecto no observado, el estimador OLS agrupado es eficiente y sus estadísticas son
asintóticamente válidas. La ausencia de un efecto no observado equivale a la hipótesis nula 𝐻0 : 𝜎𝑐2 = 0.

Prueba de Correlación Serial: Para probar 𝐻0 : 𝜎𝑐2 = 0, podemos usar la prueba de correlación serial
AR (1). Esta prueba es válida porque los errores 𝑣𝑖𝑡 no están correlacionados bajo 𝐻0 y asumimos que
𝑥𝑖𝑡 es estrictamente exógeno.
Estimador de 𝝈𝟐𝒄 : Una mejor prueba se basa en el estimador de 𝜎𝑐2 en la ecuación (10.37):

Desde esta ecuación, basamos una prueba de 𝐻0 : 𝜎𝑐2 = 0 en la distribución asintótica nula del estimador
de 𝜎𝑐2 , escalado por √𝑁.
Estadístico de Prueba: El estadístico de prueba se construye como:

Interpretación del Estadístico: Bajo la hipótesis nula de que \(v_{it}\) no están correlacionados en
serie, este estadístico tiene una distribución normal estándar asintótica. A diferencia del estadístico de
Breusch-Pagan, podemos rechazar \(H_0\) por estimaciones negativas de \(\sigma_c^2\), aunque son
raras. El estadístico puede detectar varios tipos de correlación en serie en \(v_{it}\), por lo que el rechazo
de la nula no implica necesariamente que la estructura de error de efectos aleatorios sea verdadera.
Conclusión: La prueba para la presencia de un efecto no observado es crucial para determinar si un
modelo de efectos aleatorios es adecuado. Usar pruebas de correlación serial y basar los estadísticos en
estimadores robustos permite realizar inferencias confiables sobre la existencia de efectos no
observados. Este enfoque asegura que las conclusiones derivadas del modelo sean válidas y robustas.
10.5 MÉTODOS DE EFECTOS FIJOS
10.5.1 Consistencia del estimador de efectos fijos (FE)
En el análisis de datos de panel, el objetivo de utilizar modelos de efectos fijos es permitir que 𝑐𝑖 se
correlacione arbitrariamente con 𝑥𝑖𝑡 . Un análisis de efectos fijos logra este propósito explícitamente,
asegurando que los coeficientes estimados no estén sesgados por variables omitidas constantes en el
tiempo. El modelo se presenta como:
𝑦𝑖𝑡 = 𝑥𝑖𝑡 𝛽 + 𝑐𝑖 + 𝑢𝑖𝑡 ,

𝑡 = 1, … , 𝑇

Suposición de Exogeneidad: La suposición clave para los modelos de efectos fijos (EF) es la
exogeneidad estricta de las variables explicativas condicionales a 𝑐𝑖 :
𝐸(𝑢𝑖𝑡 |𝑥𝑖 , 𝑐𝑖 ) = 0,

𝑡 = 1,2, … , 𝑇

Esto permite que 𝐸(𝑐𝑖 |𝑥𝑖 ) sea cualquier función de 𝑥𝑖 . Al relajar esta suposición, podemos estimar de
manera consistente los efectos parciales de las variables omitidas constantes en el tiempo.
Transformación de Ecuaciones: Para eliminar 𝑐𝑖 , transformamos las ecuaciones restando la media de
cada unidad en el tiempo: 𝑦̃𝑖𝑡 = 𝑥̃𝑖𝑡 𝛽 + 𝑢̃𝑖𝑡 , 𝑡 = 1,2, … , 𝑇
Donde 𝑦̃𝑖𝑡 = 𝑦𝑖𝑡 − 𝑦̅𝑖 , 𝑥̃𝑖𝑡 = 𝑥𝑖𝑡 − 𝑥̅𝑖 , y 𝑢𝑖𝑡 = 𝑢𝑖𝑡 − 𝑢̅𝑖 . Esta transformación elimina 𝑐𝑖 , permitiendo
que la estimación de 𝛽 no esté sesgada por factores no observados constantes en el tiempo.
Estimación de Efectos Fijos: El estimador de efectos fijos (FE), denotado por 𝛽𝐹𝐸 , se obtiene aplicando
MCO agrupado a la ecuación transformada:
𝑦̃𝑖𝑡 = 𝑥̃𝑖𝑡 𝛽 + 𝑢̃𝑖𝑡 ,

𝑡 = 1,2, … , 𝑇

La consistencia del estimador FE depende de que 𝑥𝑖𝑡 varíe en el tiempo. Si 𝑥𝑖𝑡 contiene una variable
que no varía en el tiempo, la transformación resultará en ceros, lo que implica que esa variable no puede
ser estimada.

Especificación General: El modelo general incluye variables constantes en el tiempo (𝑧𝑖 ) y variables
que varían en el tiempo (𝑤𝑖𝑡 ):

Donde 𝑑 son variables dummy para cada período. Esta especificación permite evaluar el efecto de
variables que varían en el tiempo y controlar por las constantes en el tiempo.
Conclusión: El modelo de efectos fijos es robusto frente a la correlación entre 𝑐𝑖 y 𝑥𝑖𝑡 , permitiendo
estimar los efectos parciales de variables omitidas constantes en el tiempo. La transformación de las
ecuaciones elimina 𝑐𝑖 , asegurando estimaciones consistentes de 𝛽. Este enfoque es crucial en análisis
donde las variables no observadas pueden influir en los resultados, proporcionando un marco robusto
para inferencia en datos de panel.
10.5.2 Inferencia asintótica con efectos fijos
Para asegurar que el estimador de efectos fijos (EF) sea eficiente, se necesitan suposiciones adicionales
más allá del supuesto EF.1. El siguiente supuesto, FE.3, garantiza la eficiencia del estimador de EF.
Supuesto FE.3: El supuesto FE.3 establece que:

Esto implica que la matriz de varianza incondicional del error compuesto 𝑣𝑖 = 𝑐𝑖 𝐽𝑇 + 𝑢𝑖 tiene la forma
de efectos aleatorios.
Condiciones para la Eficiencia: Para que los MCO agrupados sean relativamente eficientes,
requerimos que 𝑢𝑖𝑡 sea homocedástico en 𝑡 y no correlacionado serialmente:

El primer requisito se cumple, pero el segundo no, ya que los errores están correlacionados
negativamente. Sin embargo, esto no causa mayores complicaciones.
Varianza Asintótica del Estimador FE: La varianza asintótica de 𝛽𝐹𝐸 se halla como:

Estimación de 𝝈𝟐𝒖 : Para estimar 𝜎𝑢2 , definimos los residuos de los efectos fijos:
Luego, la estimación de 𝜎𝑢2 es:

Bajo los supuestos FE.1 a FE.3, 𝜎̂𝑢2 es un estimador imparcial de 𝜎𝑢2 condicional a 𝑋 (y, por lo tanto,
también incondicional).
Conclusión: Para asegurar la eficiencia del estimador de efectos fijos (FE), es necesario considerar
suposiciones adicionales como FE.3. Aunque los errores estén correlacionados negativamente, esto no
impide la estimación correcta de 𝜎𝑢2 . Las herramientas econométricas modernas facilitan la
implementación de estos estimadores, proporcionando resultados confiables y eficientes en el análisis
de datos de panel.
10.5.3 La regresión de variable dummy
Los enfoques tradicionales para la estimación de efectos fijos consideran los 𝑐𝑖 como parámetros que
deben estimarse junto con 𝛽. Una posibilidad es definir 𝑁 variables ficticias (dummies), una para cada
observación de la sección transversal.

Implementación de Variables Dummy: Para realizar esto, corremos la regresión OLS agrupada:

Aquí, cada 𝑑1𝑖 estima 𝑐𝑖 . Se puede demostrar que el estimador de 𝛽 obtenido de esta regresión es, de
hecho, el estimador de efectos fijos. Por esta razón, a veces se hace referencia a 𝛽𝐹𝐸 como el estimador
de variable ficticia.
Diferencia entre 𝒄̂𝒊 y 𝜷𝑭𝑬 : Hay una diferencia importante entre 𝑐̂𝑖 y 𝛽𝐹𝐸 . Sabemos que 𝛽𝐹𝐸 es
consistente con 𝑇 fijo como 𝑁 → ∞. Esto no es el caso con 𝑐̂𝑖 . Cada vez que se agrega una nueva
observación de sección transversal, se agrega otro 𝑐𝑖 y la información no se acumula en el 𝑐𝑖 .
El software econométrico que emplea efectos fijos generalmente suprime las "estimaciones" de los 𝑐𝑖 .
Estimación de 𝒄𝒊 : Se puede demostrar que cada 𝑐̂𝑖 se estima como:

Conclusión: La regresión de variables dummy es una técnica efectiva para estimar efectos fijos en datos
de panel. Aunque el enfoque de variables dummy puede ser intuitivo y fácil de implementar, la diferencia
clave entre 𝑐̂𝑖 y 𝛽𝐹𝐸 radica en su consistencia y acumulación de información. Este método asegura que
los efectos fijos sean capturados adecuadamente, proporcionando estimaciones robustas y precisas en el
análisis de datos de panel.
10.5.4 Correlación serial y estimador de matriz de varianza robusta
Aunque la heterocedasticidad en 𝑢𝑖𝑡 es un problema potencial, la correlación serial puede ser más
importante en ciertas aplicaciones. Probar los errores idiosincráticos (𝑢̃𝑖𝑡 ) para la correlación serial es
complicado porque no podemos estimar 𝑢𝑖𝑡 directamente debido a la desvalorización temporal utilizada
en los efectos fijos (EF). Solo podemos estimar los errores desvalorizados en el tiempo (𝑢̃𝑖𝑡 ).
Determinación de la Correlación Serial: Cuando 𝑇 ≥ 3, podemos utilizar la ecuación (10.52) para
determinar si existe correlación serial en los errores. Ignorar el error de estimación en 𝛽 permite obtener
la distribución asintótica de cualquier estadística de prueba basada en covarianzas y varianzas de
muestra.
Métodos para Probar la Correlación Serial: La prueba se complica porque los 𝑢̃𝑖𝑡 están
correlacionados serialmente bajo la hipótesis nula. Dos métodos pueden ser utilizados:
1. Usar dos períodos de tiempo cualesquiera (por ejemplo, los dos últimos) para probar la ecuación
(10.52) usando una regresión simple.
2. Ejecutar la regresión OLS agrupada y usar el error estándar totalmente robusto para OLS
agrupado.
Si se encuentra correlación serial, es esencial ajustar el estimador de la matriz de varianza asintótica y
las estadísticas de prueba.
Estimador de Matriz de Varianza Robusta: El estimador de matriz de varianza robusta de 𝛽̂𝐹𝐸 es:

Este estimador es válido en presencia de cualquier heterocedasticidad o correlación serial en \(u_{it}\).
Conclusión: En el análisis de datos de panel, la correlación serial y la heterocedasticidad son problemas
importantes que deben ser abordados para obtener inferencias válidas. Utilizar un estimador de matriz
de varianza robusta permite ajustar estos problemas, proporcionando estimaciones consistentes y
eficientes. Este enfoque es crucial para asegurar la precisión y confiabilidad de los resultados en modelos
de efectos fijos.
10.5.5 GLS de efectos fijos

En lugar de calcular una matriz de varianza robusta para el estimador de efectos fijos (EF), podemos
relajar el supuesto EF3 para permitir una matriz de covarianza condicional sin restricciones, aunque
constante.
Supuesto FGLS.3: El supuesto FGLS.3 establece que:

Bajo este supuesto, se tiene:

La cual tiene rango |Λ|. El rango deficiente en la expresión anterior causa problemas para el enfoque
habitual de GLS porque la matriz de varianza no se puede invertir. Una forma de proceder es utilizar
una inversa generalizada.
Implementación Práctica: Para ser más concretos, supongamos que eliminamos el período de tiempo
𝑇, dejando las ecuaciones:

Estimador GLS de Efectos Fijos: El estimador GLS de efectos fijos (FEGLS) se obtiene como:

Bajo los supuestos FE.1 y FEGLS.2, el estimador FEGLS es consistente:

Cuando añadimos el supuesto FGLS.3, la varianza asintótica es fácil de estimar:

Aplicaciones y Eficiencia: La suma de las estadísticas de residuos cuadrados de FGLS se puede utilizar
para probar múltiples restricciones. El estimador FEGLS fue propuesto por Kiefer (1980) cuando los 𝑐𝑖
se tratan como parámetros. El estimador FEGLS no es asintóticamente menos eficiente que el estimador
FE bajo el supuesto FGLS.3.
En lugar de permitir que Ω sea una matriz sin restricciones, podemos imponer restricciones a Λ que
impliquen que Ω tiene una forma restringida.
Conclusión: El uso de GLS de efectos fijos proporciona una alternativa eficiente al estimador de efectos
fijos tradicional, especialmente cuando se relajan ciertas suposiciones. Al permitir una matriz de
covarianza condicional sin restricciones, se puede mejorar la eficiencia de las estimaciones. Este
enfoque es útil para manejar problemas de heterocedasticidad y correlación serial en datos de panel,
asegurando resultados robustos y precisos.
10.5.6 Uso de la estimación de efectos fijos para el análisis de políticas
La estimación de efectos fijos es especialmente útil para el análisis de políticas y la evaluación de
programas. Para ilustrar esto, consideremos el siguiente modelo:

Aquí, 𝜐𝑖𝑡 puede o no contener un efecto no observado. 𝑤𝑖𝑡 es la variable de política de interés, que podría

ser continua o discreta. El vector 𝑧𝑖𝑡 incluye otros controles que podrían estar correlacionados con 𝑤𝑖𝑡 ,
incluidas variables ficticias de período de tiempo.
Consistencia de los Efectos Fijos: Para que los efectos fijos sean consistentes, se requiere que 𝑤𝑖𝑡 no
esté correlacionado con las desviaciones de 𝜐𝑖𝑡 respecto del promedio a lo largo del período de tiempo.
Esto significa que una variable de política, como la participación en un programa, no debe relacionarse
sistemáticamente con el componente persistente en el error 𝜐𝑖𝑡 medido por 𝜐̅𝑖 .
Ventajas de los Efectos Fijos: Los efectos fijos son a menudo preferibles a los modelos de MCO
agrupados o efectos aleatorios para aplicaciones de políticas. Esto se debe a que los efectos fijos
permiten controlar por factores no observados que son constantes en el tiempo, eliminando así el sesgo
que podría surgir de variables omitidas que afectan tanto a la política de interés como al resultado.
Conclusión: La estimación de efectos fijos es una herramienta poderosa en el análisis de políticas, ya
que permite controlar por factores no observados constantes en el tiempo y proporcionar estimaciones
consistentes y no sesgadas de los efectos de las políticas. Esto es crucial para evaluar adecuadamente la
efectividad de programas y políticas en diversas aplicaciones.
10.6 MÉTODOS DE PRIMERA DIFERENCIACIÓN
10.6.1 Inferencia
En la sección 10.1 se usó la diferenciación para eliminar el efecto inobservado 𝑐𝑖 con 𝑇 = 2. Ahora
aplicamos esta técnica al modelo general (10.41).
Suposición FD.1: La suposición FD.1 es la misma que la suposición FE.1, que asegura exogeneidad
estricta de las variables explicativas.
Transformación de Primera Diferencia: Para eliminar 𝑐𝑖 , tomamos el rezago de la ecuación (10.41):
u
un período y restamos:

Esta transformación de primera diferencia elimina 𝑐𝑖 . Perdemos el primer período para cada sección
cruzada, dejando 𝑇 − 1 períodos por cada 𝑖.
Modelo Transformado: La ecuación (10.63) aclara que los elementos de 𝑥𝑖𝑡 deben variar con el
tiempo; de lo contrario, ∆𝑥𝑖𝑡 tendrá elementos que sean idénticamente cero para todo 𝑡 e 𝑖. Además, se
elimina el intercepto original, y los cambios en las variables dummy de tiempo ∆𝑥𝑖𝑡 incluyen dichas
variables:

Estimación: Los parámetros 𝜃1 y 𝛾1 no están identificados porque desaparecen en la ecuación
transformada. El estimador de primera diferencia (FD), 𝛽̂𝐹𝐷 , es el estimador OLS agrupado de la
regresión:

Bajo la suposición FD.1, la estimación OLS agrupada de las ecuaciones en primera diferencia será
consistente porque:

Condición de Rango: La condición de rango para el estimador FD es:

Suposición FD.3: La suposición FD.3 es clave para la eficiencia del estimador de efectos fijos (FE),
asumiendo homocedasticidad y ausencia de correlación serial en 𝑢𝑖𝑡 . Esto asegura que los errores
idiosincráticos sean serialmente no correlacionados:

Varianza Asintótica: Bajo FD.1-FD.3, la varianza asintótica de 𝛽̂𝐹𝐷 es:

Conclusión: El estimador de primera diferencia (FD) es preferido sobre el estimador de efectos fijos
(FE) porque es más fácil de implementar sin software especializado. Bajo las suposiciones FD.1-FD.3,
el estimador FD es más eficiente y proporciona inferencias válidas al ajustar por variabilidad temporal
y eliminar efectos inobservados constantes en el tiempo. Las ecuaciones indican que los errores estándar
de la regresión en primera diferencia son asintóticamente válidos, asegurando la precisión y
confiabilidad en el análisis de datos de panel.
10.6.2 Matriz de Varianza Robusta
Si la suposición FD.3 es violada, entonces, como es habitual, podemos calcular una matriz de varianza
robusta. El estimador en la ecuación (7.26) aplicado en este contexto es:

Donde ∆𝑋 denota la matriz 𝑁(𝑇 − 1) × 𝐾 de primeras diferencias apiladas de 𝑥𝑖𝑡 .
Ejemplo 10.6: Estimación FD de los Efectos de las Subvenciones para la Capacitación Laboral:
Ahora estimamos el efecto de las subvenciones para la capacitación laboral en \(\log(\text{scrap})\)
utilizando la diferenciación de primera. Específicamente, usamos OLS agrupado en primeras
diferencias:

En lugar de diferenciar las variables dummy de los años y omitir el intercepto, simplemente incluimos
un intercepto y una variable dummy para 1989 para capturar los efectos de tiempo agregados. Si
estuviéramos específicamente interesados en los efectos de los años del modelo estructural (en niveles),
entonces deberíamos diferenciar esas también.
Resultados de la Estimación: La ecuación estimada es:

Donde los errores estándar habituales están entre paréntesis y los errores estándar robustos están entre
corchetes. Reportamos el 𝑅2 aquí porque tiene una interpretación útil: mide la cantidad de variación en
el crecimiento de la tasa de scrap que es explicada por ∆𝑔𝑟𝑎𝑛𝑡𝑖,𝑡−1 (y d89). Las estimaciones de 𝑔𝑟𝑎𝑛𝑡
y 𝑔𝑟𝑎𝑛𝑡𝑖,𝑡−1 son bastante similares a las estimaciones de efectos fijos, aunque 𝑔𝑟𝑎𝑛𝑡 es ahora
estadísticamente más significativo que 𝑔𝑟𝑎𝑛𝑡𝑖,𝑡−1 . La prueba F habitual para la significancia conjunta
de ∆𝑔𝑟𝑎𝑛𝑡𝑖,𝑡−1 es 1.53 con un valor p = 0.222.
Conclusión: El uso de una matriz de varianza robusta es esencial cuando la suposición de no correlación
serial es violada. En el ejemplo dado, se demuestra cómo aplicar OLS agrupado en primeras diferencias
y ajustar las inferencias utilizando una matriz de varianza robusta. Esto asegura que las estimaciones
sean precisas y confiables, incluso en presencia de heterocedasticidad y correlación serial.
10.6.3 Testeo para Correlación Serial
Bajo la suposición FD.3, los errores 𝑒𝑖𝑡 deben ser serialmente no correlacionados. Podemos probar esta
suposición fácilmente dado los residuos OLS agrupados de la regresión (10.65). Dado que la suposición
de exogeneidad estricta se mantiene, podemos aplicar la forma simple de la prueba en la Sección 7.8.
La regresión se basa en 𝑇 − 2 períodos de tiempo:

Procedimiento de Prueba: El estadístico de prueba es el estadístico t usual sobre 𝜌̂. Con 𝑇 = 2, esta
prueba no está disponible ni es necesaria. Con 𝑇 = 3, la regresión (10.71) es simplemente una regresión
de sección transversal porque perdemos los períodos de tiempo 𝑡 = 1 y 2.
Autocorrelación: Si los errores idiosincráticos {𝑢𝑖𝑡 : 𝑡 = 1, 2, … , 𝑇} no están correlacionados desde el
principio, {𝑒𝑖𝑡 : 𝑡 = 2, 3, … , 𝑇} estarán autocorrelacionados. De hecho, bajo la suposición FE.3, se
muestra fácilmente que 𝐶𝑜𝑟𝑟(𝑒𝑖𝑡 , 𝑒𝑖,𝑡−1 ) = −0.5. En cualquier caso, encontrar una correlación serial
significativa en los 𝑒𝑖𝑡 justifica calcular la matriz de varianza robusta para el estimador FD.
Ejemplo 10.6 (Continuación): Estimación de Correlación Serial AR(1):
Probamos la correlación serial AR (1) en la ecuación de primeras diferencias al hacer regresión de 𝑒𝑖𝑡
sobre 𝑒𝑖,𝑡−1 usando el año 1989. Obtenemos 𝜌̂ = 0.237 con un estadístico 𝑡 de 1.76. Hay evidencia
marginal de correlación serial positiva en las primeras diferencias ∆𝑢𝑖𝑡 . Además, (𝜌̂ = 0.237) es muy
diferente de (𝜌 = −0.5), lo que implica que los 𝑢𝑖𝑡 son no correlacionados en el supuesto estándar de
efectos aleatorios y fijos.
Alternativas a la Matriz de Varianza Robusta: Una alternativa a calcular errores estándar robustos y
estadísticas de prueba es usar un análisis FGLS bajo la suposición de que 𝐸(𝑒𝑖 𝑒𝑖′ |𝑥𝑖 ) es una matriz
constante (𝑇 − 1) × (𝑇 − 1). Omitimos los detalles, ya que son similares a los de FEGLS en la Sección
10.5.5. Al igual que con FEGLS, podríamos imponer una estructura en 𝐸(𝑢𝑖 𝑢𝑖’ ), como un modelo AR
(1) estable y homocedástico, y luego derivar 𝐸(𝑒𝑖 𝑒𝑖′ ) en términos de un conjunto pequeño de parámetros.
Conclusión: El testeo para correlación serial es crucial para validar las suposiciones de no correlación
en los errores de las primeras diferencias. Encontrar autocorrelación significativa justifica el uso de
matrices de varianza robusta o enfoques alternativos como FEGLS para asegurar inferencias válidas y
precisas en modelos de datos de panel.
10.6.4 Análisis de Política usando Primeras Diferencias
La diferenciación de primera de una ecuación estructural con un efecto no observado es un método
simple y efectivo para evaluar programas. Con datos de panel de dos años y grupos de control y
tratamiento en dos momentos, podemos abordar muchas preguntas. Al aplicar la diferenciación de
primera, se deben diferenciar todas las variables en la ecuación estructural, incluidas las variables
binarias de participación en el programa. Las estimaciones se interpretan en la ecuación original,
permitiendo comparaciones en la sección transversal en cualquier momento, donde una unidad recibe el
tratamiento y la otra no.
Caso Especial: Variable de Política Diferenciada: En un caso especial, no importa si la variable de
política es diferenciada. Supongamos que 𝑇 = 2, y 𝑝𝑟𝑜𝑔(𝑖𝑡) denota un indicador binario que se
establece en uno si la persona participó en el programa en el tiempo 𝑡. Para muchos programas, 𝑝𝑟𝑜𝑔𝑖𝑡 =
0 para todos 𝑖: nadie participó en el programa en el período inicial. En el segundo período, 𝑝𝑟𝑜𝑔𝑖2 es
uno para aquellos que participan en el programa y cero para aquellos que no lo hacen. En este caso,
∆𝑝𝑟𝑜𝑔𝑖 = 𝑝𝑟𝑜𝑔𝑖2 , y la ecuación diferenciada de primera puede escribirse como:

Estimador DID: El efecto de la política se puede obtener al hacer una regresión del cambio en 𝑦 sobre
el cambio en 𝑧 y el indicador de política. Cuando ∆𝑧𝑖2 se omite, la estimación de 𝛿1 en la ecuación
(10.72) es el estimador de diferencias en diferencias (DID) (𝛿1 = ∆𝑦𝑡𝑟𝑒𝑎𝑡 − ∆𝑦𝑐𝑜𝑛𝑡𝑟𝑜𝑙 ). Esto es similar
al estimador DID de la Sección 6.3 (ver ecuación 6.32), pero hay una diferencia importante: con datos
de panel, las diferencias en el tiempo son para las mismas unidades de sección transversal.
Consideraciones Adicionales: Si algunas personas participaron en el programa en el primer período, o
si hay más de dos períodos involucrados, la ecuación (10.72) puede dar respuestas engañosas. En
general, la ecuación que se debe estimar es:

Donde el indicador de participación en el programa se diferencia junto con todo lo demás, y los 𝜉𝑡 son
nuevos interceptos de período. El ejemplo 10.6 es un caso de este tipo.
Conclusión: La diferenciación de primera es una herramienta poderosa para el análisis de políticas,
permitiendo la evaluación de programas con datos de panel. Al diferenciar todas las variables, incluidas
las de participación en el programa, se obtienen estimaciones precisas y comparaciones válidas entre
unidades tratadas y de control. Es crucial considerar la estructura temporal y las características del
programa para evitar interpretaciones engañosas y asegurar resultados robustos.
10.7 COMPARACIÓN DE ESTIMADORES
10.7.1 Efectos fijos vs. Primeras Diferencias
Cuando solo tenemos dos períodos de tiempo, la estimación de efectos fijos (FE) y primeras diferencias
(FD) producen estimaciones e inferencias idénticas. Cuando 𝑇 > 2, la elección entre FE y FD depende
de las suposiciones sobre los errores idiosincráticos, 𝑢𝑖𝑡 . FE es más eficiente bajo la suposición FE.3
(los 𝑢𝑖𝑡 ) no están correlacionados serialmente), mientras que FD es más eficiente cuando 𝑢𝑖𝑡 sigue un
paseo aleatorio.
Suposición de Exogeneidad Estricta: Si las estimaciones de FE y FD difieren de maneras que no se
atribuyen al error de muestreo, el problema es la suposición de exogeneidad estricta. Si 𝑢𝑖𝑡 está
correlacionado con 𝑥𝑖𝑠 para cualquier 𝑠, FE y FD tienen límites de probabilidad diferentes. Los
problemas estándar de endogeneidad tienen el mismo efecto.
Prueba de Hausman: Podemos probar las suposiciones subyacentes a la consistencia de FE y FD
usando una prueba de Hausman. Usar una forma robusta de la prueba de Hausman que no mantenga la
suposición FE.3 ni la suposición FD.3 bajo la hipótesis nula.
Si 𝑇 = 2, en la ecuación 𝛥𝑦𝑖𝑡 = 𝛥𝑥𝑖𝑡 𝛽 + 𝛥𝑢𝑖𝑡 , ni 𝑥𝑖1 ni 𝑥𝑖2 deberían ser significativos como variables
explicativas adicionales en la ecuación de diferencias primeras. Una prueba de exogeneidad estricta
usando efectos fijos, cuando 𝑇 > 2, se obtiene especificando la ecuación:

donde 𝑤𝑖𝑡,𝑖+1 es un subconjunto de 𝑥𝑖𝑡,𝑖+1 (que excluiría las variables ficticias de tiempo). Bajo
exogeneidad estricta, 𝛿 = 0, y podemos realizar la prueba usando la estimación de efectos fijos.
Análisis de MCG: Bajo exogeneidad estricta, podemos usar un procedimiento de MCG en cualquiera
de las ecuaciones descontadas por tiempo o las ecuaciones de diferencias primeras. Im, Ahn, Schmidt y
Wooldridge (1999) muestran formalmente que los estimadores FEGLS y FDGLS son asintóticamente
equivalentes bajo FE.1 y FEGLS.3 y las condiciones de rango apropiadas.
Conclusión: La elección entre FE y FD depende de las suposiciones sobre los errores idiosincráticos y
la estructura temporal de los datos. Ambos métodos pueden producir estimaciones consistentes, pero su
eficiencia varía según las suposiciones específicas. La prueba de Hausman proporciona una herramienta
robusta para validar estas suposiciones y asegurar inferencias precisas en el análisis de datos de panel.
10.7.2 La relación entre los estimadores de efectos aleatorios y efectos fijos
Cuando las variables clave en 𝑥𝑡 no varían mucho con el tiempo, los métodos de efectos fijos (FE) y de
primeras diferencias (FD) pueden ser imprecisos. Si un análisis de efectos aleatorios (RE) es apropiado
si 𝑐𝑖 es ortogonal a 𝑥𝑖𝑡 , sus varianzas son mucho menores que los estimadores FE o FD. Ahora
obtenemos una expresión para el estimador de efectos aleatorios (RE) que nos permite compararlo con
el estimador de efectos fijos (FE).
Expresión de la Matriz de Varianza: Usando el hecho de que 𝑗𝑇′ 𝑗𝑇 = 𝑇, podemos escribir Ω bajo la
estructura de efectos aleatorios como:

Donde se transforma la ecuación y se obtiene:

Por lo tanto, la estimación MCO de (10.74) es la estimación MCO agrupada de:

para todos 𝑡 e 𝑖. Los errores en esta ecuación son no correlacionados en serie y homocedásticos bajo la
suposición RE.3; por lo tanto, cumplen con las condiciones clave para el análisis MCO agrupado.
Estimador RE Factible: El estimador RE factible reemplaza el 𝜆 desconocido por su estimador, 𝜆̂, de
modo que 𝛽̂𝑅𝐸 puede ser calculado a partir de la regresión MCO agrupada de:

Conclusión: La relación entre los estimadores de efectos aleatorios (RE) y efectos fijos (FE) se basa en
la estructura de la varianza y las suposiciones sobre la correlación entre las variables explicativas y los
efectos inobservados. El uso de RE puede ser más eficiente cuando 𝑐𝑖 es ortogonal a 𝑥𝑖𝑡 , permitiendo
una varianza menor en las estimaciones. La transformación y ajuste en los modelos aseguran que los
errores sean homocedásticos y no correlacionados, proporcionando estimaciones fiables y precisas en el
análisis de datos de panel.

10.7.3. La prueba de Hausman
La prueba de Hausman se utiliza con el fin de determinar si hay correlación entre los efectos no
observados (ci) y las variables explicativas (xit). La prueba compara los estimadores de efectos
aleatorios (RE) y efectos fijos (FE). Si los efectos fijos son consistentes cuando ci y xit están
correlacionados, pero los efectos aleatorios no lo son, una diferencia significativa entre ambos
estimadores sugiere que el supuesto de efectos aleatorios es incorrecto.
Condiciones para la prueba de Hausman: Exogeneidad Estricta (Se asume que se mantiene bajo la
hipótesis nula, lo que significa que la correlación entre 𝑥𝑖𝑠 y 𝑢𝑖𝑡 (para cualquier s y t) haría inconsistentes
tanto los estimadores FE como RE. También, la prueba se implementa asumiendo que la Suposición
RE.3 se mantiene bajo la hipótesis nula, lo que implica que el estimador RE es más eficiente que el FE.
Sin embargo, la prueba de Hausman no puede detectar fallos en RE.3, lo que puede llevar a una
distribución asintótica no estándar.
Estadísticas t y F: Para un solo parámetro, se puede usar una versión t de la prueba de Hausman,
mientras que para múltiples parámetros, se puede usar una versión F de la prueba de Hausman. Esto
implica extender el modelo e implementar la prueba usando análisis de regresión estándar con OLS.
Alternativas y Robustez: Si la suposición RE.3 falla, se necesita una versión robusta de la prueba de
Hausman. Una aproximación sencilla es usar una estadística Wald robusta en el contexto de la
estimación con OLS agrupado.
Consideraciones Prácticas: La prueba de Hausman puede rechazar las suposiciones RE con diferencias
pequeñas pero significativas entre los estimadores RE y FE, o puede no rechazar cuando las diferencias
son grandes pero las varianzas estándar también lo son. Esto puede llevar a errores de Tipo II (no
rechazar cuando es falso).
Conclusión: La prueba de Hausman es una herramienta crucial para decidir entre modelos de efectos
aleatorios y efectos fijos, basada en la correlación entre los efectos no observados y los regresores. Sin
embargo, su aplicación requiere cuidadosas consideraciones sobre las suposiciones subyacentes y la
robustez de los resultados, particularmente en presencia de correlación serial y heterocedasticidad.

FROM EDUCATION TO DEMOCRACY? (ACEMOGLU ET.AL. 2005)
La educación se considera crucial para la democracia porque fomenta una cultura democrática y está
asociada con mayor prosperidad, lo que a su vez se cree que impulsa el desarrollo político. Esta idea es
respaldada por la teoría de la modernización de Lipset, que destaca la educación y el crecimiento
económico como impulsores del desarrollo político y la democracia.
Lipset (1959 p.20) concluye que: Si bien un alto nivel de educación no es suficiente para la democracia,
la evidencia sugiere que se acerca a ser una condición necesaria.
Trabajos recientes de Barro (1999) y Przeworski et al. (2000) respaldan la idea de que la educación
influye en la democracia. Glaeser et al. (2004) argumentan que las diferencias en la educación son un
factor causal importante no solo para la democracia, sino también para otras instituciones políticas.
La investigación de User destaca que la correlación entre educación y democracia no implica causalidad.
Mostraron que, a pesar de un aumento en la educación en un país, no hay una tendencia clara hacia una
mayor democratización en ese país. Esto sugiere que otros factores podrían estar influyendo en la
relación entre educación y democracia.
La investigación muestra que la relación entre educación y democracia desaparece al controlar los
efectos fijos por país, indicando la influencia de otros factores. Además, se destaca la robustez de estos
hallazgos en diferentes técnicas y muestras.
El estudio de User indica que el impacto de la educación en la democracia, señalado por Glaeser et al.
(2004), desaparece cuando se consideran efectos temporales en las regresiones. Esto sugiere que el
aumento global en la educación y la democracia en las últimas décadas podría estar influyendo en los
resultados. Además, no se encuentra efecto de la educación en otras medidas de instituciones políticas.
User está investigando la relación entre ingreso y democracia, encontrando poca evidencia de un
efecto causal directo. Además, proponen una teoría respaldada por evidencia empírica para explicar las
diferencias en los factores que afectan la evolución de la educación, el ingreso y la democracia.
I. Educación y democracia
User utiliza el Índice de Derechos Políticos de Freedom House para medir la democracia, donde 7
representa la menor cantidad de libertad política y 1 la mayor. También utiliza la variable relacionada
de Bollen (1990), transformada entre 0 y 1. Prefiere observaciones quinquenales en lugar de promediar
los datos para evitar introducir una correlación serial adicional.
La variable principal, los años promedio de escolaridad en la población mayor de 25 años, proviene de
Barro y Lee (2000) y abarca intervalos de cinco años entre 1960 y 2000. En nuestra muestra base, esta
variable varía de 0.04 a 12.18 años, con una media de 4.44. Los resultados principales se presentan en
la Tabla 1, utilizando datos de Freedom House y estimando una relación agrupada de MCO entre
educación y democracia:
(1)

𝑑𝑖,𝑡 =∝ 𝑑𝑖,𝑡−1 + 𝛾𝑠𝑖,𝑡−1 + 𝜇 + 𝑣𝑖𝑡

donde 𝑑𝑖,𝑡 es el puntaje de democracia del país i en el período t. El valor rezagado de esta variable se
incluye en el lado derecho para capturar la persistencia en la democracia y también dinámicas
potencialmente de reversión a la media en la democracia. La principal variable de interés es 𝑠𝑖,𝑡−1 , el
valor rezagado de años promedio de escolaridad. El parámetro 𝛾 mide si la educación tiene un efecto
sobre la democracia. El parámetro 𝜇𝑡 denota un conjunto completo de efectos temporales, que
capturan los choques comunes a el puntaje de democracia de todos los países, y 𝑣𝑖𝑡 es un término de
error, que captura todos los otros factores omitidos.
La columna (i) muestra una correlación significativa entre educación y democracia, con una
estimación de γ de 0.027 y un error estándar de 0.004, significativo al 1%. Esto implicaría que un año
adicional de escolaridad aumenta el valor de "estado estacionario" de la democracia en 0.093. Esta es
una magnitud significativa en relación con la media de democracia en la muestra, que es 0.57. La

estimación incluye tanto el efecto directo como el indirecto de la educación sobre la democracia a
través del ingreso.
La Ecuación (1) de la investigación es similar a las regresiones previas al no considerar los efectos
fijos por país. Esto puede causar estimaciones incorrectas del efecto de la educación en la democracia
debido a factores omitidos que influyen en ambas variables a largo plazo.
La alternativa es permitir la presencia de tales factores omitidos (que no varían con el tiempo)
mediante la inclusión de efectos fijos por país, es decir, estimando un modelo de la forma:
(2)

𝑑𝑖,𝑡 =∝ 𝑑𝑖,𝑡−1 + 𝛾𝑠𝑖,𝑡−1 + 𝜇𝑡 + 𝛿𝑖 + 𝑢𝑖𝑡

, que difiere de (1) únicamente por el conjunto completo de variables Dummy de país, el 𝛿𝑖 ′𝑠.
El resto de la Tabla 1 presenta estimaciones de 𝛾 a partir de modelos similares a (2). La columna (ii) es
idéntica a la columna (i) excepto por los efectos fijos por país, los 𝛿𝑖 ′𝑠. Sin embargo, los resultados
son radicalmente diferentes. Ahora, 𝛾 se estima en -0.005 con un error estándar de 0.019; por lo tanto,
es altamente insignificante y tiene el signo opuesto al predicho por la hipótesis de la modernización [y
al encontrado en la regresión de MCO agrupada de la columna (i)].
En la regresión de la columna (ii), debido a que el regresor 𝑑𝑖,𝑡−1 está mecánicamente correlacionado
con 𝑢𝑖𝑡 para 𝑠 < 𝑡, la estimación de efectos fijos estándar no es consistente en paneles con una
dimensión temporal corta debido a la correlación mecánica entre el regresor y el error. Para abordar
esto, en la columna (iii) se utiliza el estimador GMM de Arellano y Bond (1991), que da una
estimación más negativa de -0.017 (SE = 0.022). Las pruebas AR(2) y de Hansen J no rechazan las
restricciones de sobreidentificación implícitas en este procedimiento.
Las columnas adicionales de la Tabla 1 examinan la relación entre educación y democracia al incluir
otras variables. Las columnas (iv) y (v) consideran la estructura de edad y la población, pero estas
variables no resultan significativas. Aunque las variables de estructura de edad son conjuntamente
significativas en OLS, no lo son en GMM, y el logaritmo de la población no es significativo. El efecto
de la educación sobre la democracia sigue siendo altamente insignificante en ambos casos.
Las columnas (vi) y (vii) incluyen el PIB per cápita. Tanto la educación como el PIB per cápita
resultan insignificantes, con coeficientes negativos. Esto sugiere que el efecto causal del ingreso sobre
la democracia, otro principio de la hipótesis de modernización, tampoco es robusto al considerar
efectos fijos por país. Las columnas (viii) y (ix) controlan simultáneamente el logaritmo de la
población, la estructura de edad y el PIB per cápita, con resultados similares.

Los resultados se mantienen al
excluir ciertos grupos de países
y al usar diferentes medidas de
democracia o periodos de
tiempo. En general, no hay una
relación empírica entre
educación y democracia una
vez que se controlan los efectos
fijos por país, lo que cuestiona
el efecto causal de la educación
sobre la democracia.

II. ¿De la educación a las instituciones?
El reciente artículo de Glaeser et al. (2004) argumenta que
hay un efecto causal de la educación sobre las instituciones.
Lo sustentan al reportar regresiones similares a nuestro
modelo en (2), pero con resultados muy diferentes,
mostrando en particular un efecto positivo de la educación
sobre la democracia. ¿Por qué sus resultados son diferentes
de los nuestros?
En la Tabla 2A, se replican los resultados utilizando
restricción ejecutiva, puntajes de autocracia y democracia de
Polity, y puntajes de autocracia de Przeworski et al. Los
resultados coinciden con las regresiones originales, pero
difieren de las de User al no incluir efectos temporales. Sin
efectos temporales, el parámetro γ se identifica a partir de la
variación a lo largo del tiempo, lo que no corresponde a un
efecto causal.
Los paneles B y C de la Tabla 2 presentan estimaciones con
y sin ingreso per cápita, pero incluyendo efectos temporales.
En todos los casos, el efecto de la educación es insignificante y tiene el signo incorrecto, al igual que
en los resultados básicos. Además, en la mayoría de las columnas, los efectos temporales son
significativos al nivel del 1% o menos, y en un caso al nivel del 10%. Curiosamente, en este último
caso, la educación sigue siendo insignificante incluso sin efectos temporales.
La evidencia en la Tabla 2 muestra, por lo tanto, que parece no haber efecto de la educación sobre la
democracia ni sobre otras instituciones políticas.
III. Conclusiones
La teoría de la modernización sugiere que la educación alta es crucial para la democracia, pero este
documento muestra que la evidencia existente no respalda firmemente esta idea, ya que factores no
considerados podrían explicar la aparente relación entre educación y democracia.
Esta evidencia plantea dos enunciados importantes:
(i) El documento no responde si hay una relación causal a largo plazo entre educación y democracia,
ya que se basa en variaciones quinquenales que podrían no capturar efectos a largo plazo que podrían
durar hasta 50 o 100 años.
(ii) Los factores omitidos que influyen en la educación y la democracia podrían relacionarse con la
evolución histórica del desarrollo económico y político. Investigaciones previas sugieren que los
efectos fijos por país están correlacionados con determinantes históricos del desarrollo institucional,
como las experiencias tempranas con la democracia y condiciones demográficas.

Universidad de Lima
Facultad de Ciencias Empresariales y Económicas
Carrera de Economía

Resumen Control 3: Lecturas semanas 5-9
Curso: Econometría 2

“He insistido en que la econometría debe tener relevancia en realidades
concretas, de lo contrario degenera en algo que no merece el nombre de
econometría, sino que más bien debería llamarse 'jugometría.”
-Ragnar Frisch.

Profesor
Jose Luis Nolazco Cama
Junio de 2024

Lima – Perú

COINTEGRATION AND ERROR-CORRECTION MODELS (ENDERS CAP 6)
1. LINEAR COMBINATIONS OF INTEGRATED VARIABLES
Se presenta una ecuación simple por la demanda del dinero
𝒎𝒕 = 𝜷𝟎 + 𝜷𝟏 𝒑𝒕 + 𝜷𝟐 𝒚𝒕 + 𝜷𝟑 𝒓𝒕 + 𝒆𝒕

(6.1. )

𝑝𝑡 = nivel de precios
𝑦𝑡 = ingreso real
𝑟𝑡 = tasa de interés
𝑒𝑡 = termino de perturbación estacionario
𝛽𝑖 = parámetros a estimar
y todas las variables son expresadas en logaritmos, excepto 𝑟𝑡
La hipótesis de que el mercado de dinero está en equilibrio permite a los investigadores recolectar data
de series de tiempo de la oferta monetaria (= demanda de dinero si el mercado siempre se vacía), el nivel
de precios, ingreso real, y una tasa de interés. Los supuestos de comportamiento requieren 𝛽1 = 1, 𝛽2 >
0 y 𝛽3 < 0. Si 𝑒𝑡 tiene una tendencia estocástica, los errores en el modelo serán acumulados tal que la
desviación del equilibrio no será eliminada. Es por ello que se necesita que 𝑒𝑡 sea estacionario.
El problema es que las demás variables pueden ser caracterizadas como variables no estacionarias I(1).
Como tal, cada variable puede serpentear sin tendencia a volver en el largo plazo. Sin embargo, la teoría
de (6.1) afirma que existe una combinación lineal de estas variables no estacionarias que es estacionaria.
𝑒𝑡 = 𝑚𝑡 − 𝛽0 − 𝛽1 𝑝𝑡 − 𝛽2 𝑦𝑡 − 𝛽3 𝑟𝑡
Dentro de un marco de equilibrio, las desviaciones de este deben ser estacionarias. Otros ejemplos
económicos importantes son:
1. Teoría de la función del consumo: La versión simple de la hipótesis del consumo permanente
𝑝
sostiene que el consumo total (𝑐𝑡 ) es la suma del consumo permanente (𝑐𝑡 ) y el transitorio
(𝑐𝑡𝑡 ). Ya que el consumo permanente es proporcional al ingreso permanente, tanto el consumo
como el permanente serán variables I(1), lo que requiere que el consumo transitorio sea
estacionario.
2. Hipótesis de la tasa a plazo imparcial: La teoria de los mercados eficientes sostienen que el
precio forward de un activo es el valor esperado del precio spot en el futuro. En este caso la
combinación lineal dependerá del error. Según esta hipótesis, se requiere que exista una
combinación lineal de tipos de cambio a plazo y al contado no estacionarios que sea estacionaria.
3. Mercado de arbitraje de comoddities y poder de paridad de compra: Los arbitrajistas evitan
que en el corto plazo los precios de los mismos activos no varíen incluso si los precios son no
estacionarios. Además, el PPP pone restricciones en el movimiento no estacionario de los
niveles de precios y tasas de tipo de cambio. Si et es el logaritmo del tipo de cambio extranjero
𝑝𝑡 y 𝑝𝑡∗ los logaritmos de los niveles de precios domésticos y extranjeros, la combinación lineal
del PPP en el largo plazo 𝑒𝑡 + 𝑝𝑡∗ − 𝑝𝑡 es estacionario.
Todos estos ejemplos ilustran el concepto de cointegración Engle y Granger (1987). Su análisis formal
es la combinación de variables económicas en equilibrio en el largo plazo
𝛽1 𝑥1𝑡 + 𝛽2 𝑥2𝑡 + ⋯ + 𝛽𝑛 𝑥𝑛𝑡 = 0
Donde los 𝛽t son los vectores y los x que está en equilibrio cuando 𝛽 x=0. La desviación del equilibrio
(conocida como error de equilibrio) es 𝑒𝑡 , tal que:
𝑒𝑡 = 𝛽𝑥𝑡
Si el equilibrio es significativo, debe ser que el proceso de error de equilibrio es estacionario. El termino
equilibrio es diferente para economistas y econometristas, los primeros se refieren a una igualdad entre

transacciones deseadas y las verdaderas; mientras que los segundos, hacen referencia a cualquier
relación de variables no estacionarias en el largo plazo. Engle y Granger (1987) definen la cointegración:
Los componentes del vector
denotado por
si:

se dicen que están cointegrados de orden d, b

1. Todos los componentes de 𝑥𝑡 , son integrados de orden 𝑑
2. Existe un vector beta, tal que la combinación lineal de 𝛽𝑥𝑡 = 𝛽1 𝑥1𝑡 + 𝛽2 𝑥2𝑡 + ⋯ + 𝛽𝑛 𝑥𝑛𝑡 x es
integrado de orden (𝑑 − 𝑏) donde 𝑏 > 0. (beta es el vector de cointegración)
En términos de la ecuación (6,1) (la del inicio) todas las variables que eran I(1) y la combinación lineal
de 𝑚𝑡 − 𝛽0 − 𝛽1 𝑝𝑡 − 𝛽2 𝑦𝑡 − 𝛽3 𝑟𝑡 = 𝑒𝑡 es estacionario, entonces las variables están cointegradas de
orden (1,1). El vector 𝑥𝑡 es (𝑚𝑡 , 1, 𝑝𝑡 , 𝑦𝑡 , 𝑟𝑡 )´ y el vector de cointegración 𝛽 es
(1, − 𝛽0 , − 𝛽1 , −𝛽2 , −𝛽3 ). La desviación del equilibrio del mercado monetario a largo plazo es et; dado
que {𝑒𝑡 } es estacionario, esta desviación es de naturaleza temporal.
Cuatro puntos a resaltar de la definición:
1. Si bien ya sabemos que es la cointegración. Teóricamente, es bastante probable que exista una
combinación no lineal de largo plazo entre variables integradas. También se debe notar que el
vector de cointegración no es único. Si existe un beta, si se lo multiplica por un landa (diferente
de cero) también es un vector de cointegración. Para normalizar el vector con respecto a 𝑥1𝑡 , se
elige 𝜆 = 1/ 𝛽.
2. En la definición, no todas las variables están son del mismo orden, usualmente habrá un número
de variables I(d) no cointegradas. Por lo que pasean arbitrariamente en el equilibrio. Dos
variables integradas de diferente orden, no pueden ser cointegradas. Sin embargo, si 𝑥1𝑡 y 𝑥2𝑡
son CI(2.1), existe una combinación lineal de la forma 𝛽1 𝑥1 + 𝛽2 𝑥2 la cual es I(1). Es posible
que esta combinación de 𝑥1𝑡 y 𝑥2𝑡 esta cointegrada con variables I(1). Lee y Granger (1990)
usan el termino multicointegración para esto.
3. Puede que halla más de un vector de cointegración independiente para un grupo de variables
I(1). El número de vectores de cointegración se llama el rango de cointegración de 𝑥𝑡 .
4. La mayor parte de la literatura de cointegración se concentra en el caso en que cada variable
contenga una única raíz unitaria. La razón es que tradicionalmente los análisis se aplican a
variables I(0) y pocas variables son integrados de orden superior a la unidad.
Caso 1: La serie {𝜇𝑡 } es un proceso de caminata aleatoria y
{𝜀𝑦𝑡 } y {𝜀𝑧𝑡 } son ruido blanco. Por lo tanto, las secuencias
{𝑦𝑡 } y {𝑧𝑡 } son ambas procesos de caminata aleatoria más
ruido. Aunque cada una es no estacionaria, las dos
secuencias tienen la misma tendencia estocástica; por lo
tanto, están cointegradas de manera que la combinación
lineal (𝑦𝑡 − 𝑧𝑡 ) es estacionaria. El término de error de
equilibrio (𝜀𝑦𝑡 − 𝜀𝑧𝑡 ) es un proceso I(0).
Caso 2: Las tres secuencias son procesos de caminata
aleatoria más ruido. Como están construidas, ninguna de las
dos están cointegradas. Sin embargo, la combinación lineal
(𝑦𝑡 + 𝑧𝑡 − 𝑤𝑡 ) es estacionaria; por lo tanto, las tres
variables están cointegradas. El error de equilibrio es un
proceso I(0).
En el caso 1, tanto la secuencia 𝑦𝑡 como 𝑧𝑡 fueron construidas como un random walk más un proceso
de ruido. Si bien cuando son 20 realizaciones se observa un declive, cuando se extiende la muestra se
eliminaría esta tendencia. En cualquier caso, ninguna serie muestra una tendencia a regresar a largo
plazo (dickey Fuller no es capaz de rechazar la 𝐻0 ). Aunque ambas series son no estacionarias, se ve

que se mueven juntas. De hecho, la diferencia de las series (𝑦𝑡 − 𝑧𝑡 ) es estacionario, con media cero y
varianza constante.
En el caso 2 ilustra la cointegración entre tres random walk con ruido. Como en el caso 1, ninguna serie
muestra tendencia a retorno en el largo plazo (dickey Fuller tampoco funciona). En contraste al caso
anterior, no hay dos series que aparenten estar cointegradas; cada serie parece deambular de la otra. Sin
embargo, existe la combinación lineal 𝑒𝑡 = 𝑦𝑡 + 𝑧𝑡 – 𝑤𝑡 . Por lo tanto, se deduce que el comportamiento
dinámico de al menos una variable debe estar restringido por los valores de las otras variables en el
sistema.
Se dispone de un gráfico de dispersión del caso 1 de 𝑦𝑡 con el valor
asociado 𝑧𝑡 , entonces se ve que hay una fuerte relación entre las
variables, de hecho, la recta de mínimos cuadrados revela que esta es la
relación de largo plazo de equilibrio de la serie, y las desviaciones de
la línea son las desviaciones estacionarias.
También se observa el caso de {𝑧𝑡 } y {𝑦𝑡 }, que son las sendas temporales de dos random walk con ruido
que no están cointegrados, siendo. En el gráfico (a), ambas parecen vagar sin mostrar ninguna tendencia
a acercarse. El gráfico (b) muestra el diagrama de dispersión de las dos secuencias y la línea de regresión
zt = 𝛽0 + 𝛽1yt. Sin embargo, esta línea de regresión es engañosa. Como se muestra en el gráfico (c), los
residuos de la regresión no son estacionarios.

2. COINTEGRATION AND COMMON TRENDS
La observación de Stock y Watson (1988) de que las variables cointegradas comparten tendencias
estocásticas comunes proporciona una manera muy útil de entender las relaciones de cointegración. Si
usamos un vector 𝑥𝑡 con dos variables 𝑦𝑡 y 𝑧𝑡 , podemos escribir cada variable como un paseo aleatorio
más un componente irregular:
𝑦𝑡 = 𝜇𝑦𝑡 + 𝑒𝑦𝑡 (6.4)
𝑧𝑡 = 𝜇𝑧𝑡 + 𝑒𝑧𝑡 (6.5)
Donde 𝜇 es un proceso de ruido blanco que representa la tendencia estocástica, y 𝑒 es el componente
estacionario de la variable
Si 𝑌𝑡 y 𝑍𝑡 son cointegradas de orden (1,1), debe de haber valores no nulos de 𝛽1 y 𝛽2 para los cuales la
combinación lineal 𝛽1 𝑌𝑡 + 𝛽2 𝑍𝑡 sea estacionaria. Por ello, considere la siguiente suma:

Para que 𝜷𝟏 𝒚𝒕 + 𝜷𝟐 𝒛𝒕 sea estacionario, el término (𝜷𝟏 𝝁𝒚𝒕 + 𝜷𝟐 𝝁𝒛𝒕 ) debe anularse, ya que si
cualquiera de las dos tendencias aparece en (6.6), la combinación lineal 𝛽1 𝑦𝑡 + 𝛽2 𝑧𝑡 lo tendrá. Por lo
tanto, dado que el segundo término entre paréntesis es estacionario, la condición necesaria y suficiente
para que {yt} y {zt} sean cointegradas de orden 1 es:
𝜷𝟏 𝝁𝒚𝒕 + 𝜷𝟐 𝝁𝒛𝒕 = 𝟎 (𝟔. 𝟕)

Claramente, 𝜇𝑦𝑡 y 𝜇𝑧𝑡 son variables cuyos valores realizados cambiarán continuamente con el tiempo.
Como excluimos que 𝛽1 y 𝛽2 sean iguales a 0, se sigue qué (6.7) se cumple para todo t si y sólo si
𝜇𝑦𝑡 = −𝛽2 𝜇𝑧𝑡 /𝛽1
Para valores no nulos de 𝛽1 y 𝛽2 , la única manera de asegurar la igualdad es que las tendencias
estocásticas sean idénticas hasta un escalar. Por lo tanto, hasta el escalar −𝛽2 ∕ 𝛽1, los dos procesos
estocásticos {𝑦𝑡 } y {𝑧𝑡 } que son I(1) deben tener la misma tendencia estocástica si están cointegrados de
orden (1, 1).
Siguiendo un caso donde los 𝜀 son ruido blanco y 𝜇 es un proceso de paseo aleatorio puro que representa
la misma tendencia estocástica para 𝑦𝑡 y 𝑧𝑡 :
𝑦𝑡 = 𝜇𝑡 + 𝜀𝑦𝑡
𝑧𝑡 = 𝜇𝑡 + 𝜀𝑧𝑡
𝜇𝑡 = 𝜇𝑡−1 + 𝜀𝑡
El valor de 𝜇0 se inicializó en 0 y se extrajeron 3 conjuntos de 20 números para representar las secuencias
{𝜀𝑦𝑡 }, {𝜀𝑧𝑡 } y {𝜀𝑡 }, y con estos errores y el valor inicial de 𝜇0 se construyen las secuencias {𝑦𝑡 , 𝑧𝑡 , 𝜇𝑡 }.
Luego, al restar 𝑧𝑡 de 𝑦𝑡 se consigue una secuencia estacionaria
𝑦𝑡 − 𝑧𝑡 = (𝜇𝑡 + 𝜀𝑦𝑡 ) − (𝜇𝑡 + 𝜀𝑧𝑡 ) = 𝜀𝑦𝑡 − 𝜀𝑧𝑡
Para plantear el punto usando la terminología de Engle y Granger, multiplicamos el vector cointegrante
𝛽 = (1,−1) por el vector 𝑥𝑡 = (𝑦𝑡 , 𝑧𝑡 )′ que produce la secuencia estacionaria =𝜀𝑡 = 𝜀𝑦𝑡 − 𝜀𝑧𝑡 . La idea
esencial de Stock y Watson (1988) es que los parámetros del vector de cointegración deben ser tales que
purguen la tendencia de la combinación lineal. Cualquier otra combinación lineal de las dos variables
contiene una tendencia de manera que el vector de cointegración es único hasta un escalar de
normalización. Por lo tanto, 𝛽3 𝑦𝑡 + 𝛽4 𝑧𝑡 no puede ser estacionario a menos que 𝛽3 ∕ 𝛽4 = 𝛽1 ∕ 𝛽2.
Luego, se construye una tendencia 𝑤𝑡 que es simplemente la sumatoria de tendencias en 𝑦𝑡 y 𝑧𝑡 :
𝜇𝑤𝑡 = 𝜇𝑦𝑡 + 𝜇𝑧𝑡 . En este caso, el vector 𝑥𝑡 = (𝑦𝑡 , 𝑧𝑡 , 𝑤𝑡 )′ tiene el vector de cointegración de (1,1,-1),
tal que la combinación lineal 𝑦𝑡 + 𝑧𝑡 − 𝑤𝑡 es estacionaria. Considerando:

Este ejemplo demuestra el punto general de que la cointegración ocurrirá siempre que la tendencia de
una variable pueda expresarse como una combinación lineal de las tendencias de las otras variables. En
tales circunstancias, siempre es posible encontrar un vector 𝛽 tal que la combinación lineal 𝛽1𝑦𝑡 + 𝛽2𝑧𝑡
+ 𝛽3𝑤𝑡 no contenga tendencia. El resultado se generaliza fácilmente al caso de n variables:
Considere la representación vectorial donde 𝑥𝑡 es el vector (𝑥1𝑡 , 𝑥2𝑡 , . . )′, 𝜇𝑡 es el vector de tendencias
estocásticas (𝜇1𝑡 , 𝜇2𝑡 , … )′ y 𝑒𝑡 es un vector de componentes estacionarios:
𝑥𝑡 = 𝜇𝑡 + 𝑒𝑡 (6.8)
Si una tendencia se puede expresar como una combinación lineal de las otras tendencias en el sistema,
significa que existe un vector 𝛽 tal que
𝛽1 𝜇1𝑡 + 𝛽2 𝜇2𝑡 + ⋯ + 𝛽𝑛 𝜇𝑛𝑡 = 0
Multiplicando por 𝛽𝑖 𝑠
𝛽𝑥𝑡 = 𝛽𝜇𝑡 + 𝛽𝑒𝑡
Si 𝛽𝜇𝑡 es 0, 𝛽𝑥𝑡 sería igual a 𝛽𝑒𝑡 , lo que garantiza su estacionariedad. Esto se generaliza para múltiples
vectores de cointegración

3. COINTEGRATION AND ERROR CORRECTION
Una característica principal de las variables cointegradas es que sus trayectorias temporales son
influenciadas por el grado de cualquier desviación del equilibrio de largo plazo. Después de todo, si el
sistema debe retornar al equilibrio de largo plazo, los movimientos de al menos algunas de las variables
deben responder a la magnitud del desequilibrio.
La relación entre tasas de interés a largo y corto plazo muestra cómo se ajustan las variables hacia un
equilibrio a largo plazo. Si la brecha entre estas tasas es grande, la tasa a corto plazo eventualmente debe
aumentar en relación con la tasa a largo plazo, así que las dinámicas a corto plazo se ven afectadas por
esta discrepancia con la relación a largo plazo. Por ello, es importante reducir la brecha, que se puede
hacer mediante (1) un aumento en la tasa a corto plazo y/o una disminución en la tasa a largo plazo, (2)
un aumento en la tasa a largo plazo, pero un aumento correspondientemente mayor en la tasa a corto
plazo, o (3) una disminución en la tasa a largo plazo, pero una disminución menor en la tasa a corto
plazo. Sin embargo, sin un modelo dinámico no se puede determinar cual de las posibilidades ocurrirá.
El modelo dinámico implícito en esta discusión es uno de corrección de errores. En un modelo de
corrección de errores, las dinámicas a corto plazo de las variables en el sistema son influenciadas por la
desviación del equilibrio. Si asumimos que ambas tasas de interés son I(1), un modelo simple de
corrección de errores que podría aplicarse a la estructura temporal de las tasas de interés es:

Donde: 𝜀𝑠𝑡 y 𝜀𝐿𝑡 son términos de perturbación de ruido blanco que pueden estar correlacionados, 𝑟𝐿𝑡 y
𝑟𝑆𝑡 son las tasas de interés a largo y corto plazo, 𝛼𝑆 𝛼𝐿 y 𝛽 son parámetros.
En este caso, las tasas a corto y largo plazo cambiarían por los choques estocásticos 𝜀𝑠𝑡 y 𝜀𝐿𝑡 y por las
desviaciones de periodos anteriores. Si la desviación resultara positiva (de modo que 𝑟𝐿𝑡−1 y 𝛽𝑟𝑆𝑡−1 >
0), la tasa de interés a corto plazo aumentaría y la tasa a largo plazo disminuiría. El equilibrio a largo
plazo se alcanza cuando 𝑟𝐿𝑡 = 𝛽𝑟𝑆𝑡 , de modo que el cambio esperado en cada tasa es cero.
Aquí se puede ver la relación entre los modelos de corrección de errores y las variables cointegradas.
Primero, Δ𝑟𝑆𝑡 es estacionario, de modo que el lado izquierdo de (6.9) es I(0). Para que (6.9) tenga
sentido, el lado derecho también debe ser I(0). Dado que 𝜀𝑠𝑡 es estacionario, se sigue que la combinación
lineal 𝑟𝐿𝑡−1 - 𝛽𝑟𝑆𝑡−1 también debe ser estacionaria. (Recordemos el mayor orden de integración en una
igualdad) Por lo tanto, las dos tasas de interés deben ser cointegradas con el vector de cointegración
(1,−𝛽). El mismo argumento se aplica a (6.10).
El punto esencial a tener en cuenta es que la representación de corrección de errores requiere que las
dos variables estén cointegradas de orden 𝐶𝐼(1,1). Este resultado no se altera si formulamos un modelo
más general introduciendo los cambios rezagados de cada tasa en ambas ecuaciones:

La inspección de (6.11) y (6.12) revela una notable similitud con los modelos VAR del capítulo anterior.
Este modelo de corrección de errores bivariado es un VAR bivariado en primeras diferencias aumentado
por los términos de corrección de errores 𝛼𝑆 (𝑟𝐿𝑡−1 − 𝛽𝑟𝑆𝑡−1 ) y −𝛼𝐿 (𝑟𝐿𝑡−1 − 𝛽𝑟𝑆𝑡−1 ). Observa que 𝛼𝑆
y 𝛼𝐿 tienen la interpretación de parámetros de velocidad de ajuste. Cuanto mayor sea 𝛼𝑆 , mayor será
la respuesta de 𝑟𝑆𝑡 a la desviación del período anterior del equilibrio a largo plazo. En el extremo opuesto,
valores muy pequeños de 𝜶𝑺 implican que la tasa de interés a corto plazo no responde al error de
equilibrio del período anterior.
Para que la secuencia {Δ𝑟𝑆𝑡 } no se vea afectada por la secuencia de tasas de interés a largo plazo, 𝛼𝑆 y
todos los coeficientes 𝑎12 (i) deben ser iguales a cero. Sin embargo, al menos uno de los términos de
velocidad de ajuste en (6.11) y (6.12) debe ser distinto de cero, ya que si tanto 𝛼𝑆 como 𝛼𝐿 son iguales

a cero, la relación de equilibrio a largo plazo no aparece y el modelo no es de corrección de errores o
cointegración.
El resultado puede ser fácilmente generalizado al modelo n-variable. Formalmente, el vector (n ⋅ 1) de
variables I(1) 𝑥𝑡 = (𝑥1𝑡 , 𝑥2𝑡 , … , 𝑥𝑛𝑡 )′ tiene una representación de corrección de errores si puede
expresarse en la forma:

donde 𝜋0 es un vector de términos de intercepto de tamaño (n ⋅ 1) con elementos 𝜋𝑖0 ; 𝜋𝑖 representa las
matrices de coeficientes de tamaño (n ⋅ n) con elementos 𝜋𝑗𝑘 (i); 𝜋 representa una matriz con elementos
𝜋𝑗𝑘 tales que uno o más de los 𝜋𝑗𝑘 ≠ 0; 𝜀𝑡 representa un vector de tamaño (n ⋅ 1) con elementos 𝜀𝑖𝑡 .
Observa que los términos de perturbación están dispuestos de modo que 𝜀𝑖𝑡 puede estar correlacionado
con 𝜀𝑗𝑡 . Supongamos que todas las variables en 𝑥𝑡 son I(1). Resolviendo (6.13) para 𝜋𝑥𝑡−1 obtenemos

Dado que cada expresión del lado derecho es estacionaria, 𝜋𝑥𝑡−1 también debe ser estacionaria. Dado
que 𝜋 solo contiene constantes, cada fila de 𝜋 es un vector de cointegración de 𝑥𝑡 .
La característica clave en (6.13) es la presencia de la matriz 𝜋. Hay dos puntos importantes a tener en
cuenta:
1. Si todos los elementos de 𝜋 son iguales a cero, (6.13) es un VAR tradicional en primeras
diferencias. En tales circunstancias, no hay una representación de corrección de errores ya que
Δ𝑥𝑡 no responde a la desviación del período anterior del equilibrio a largo plazo.
2. Si uno o más de los 𝜋𝑗𝑘 difieren de cero, Δ𝑥𝑡 responde a la desviación del período anterior del
equilibrio a largo plazo. Por lo tanto, estimar 𝑥𝑡 como un VAR en primeras diferencias no es
apropiado si 𝑥𝑡 tiene una representación de corrección de errores. Asimismo, la omisión de la
expresión 𝜋𝑥𝑡−1 conlleva un error de especificación si 𝑥𝑡 tiene una representación de corrección
de errores como en (6.13).
Una buena manera de examinar la relación entre cointegración y corrección de errores es estudiar las
propiedades del modelo VAR simple:

Usando operadores de retardo, se puede escribir como:

Y en forma de matriz seria:

Usando la regla de Cramer o matriz inversa, podemos obtener las soluciones

Hemos convertido el sistema de primer orden de dos variables representado por (6.14) y (6.15) en dos
ecuaciones de diferencia de segundo orden univariadas, y ambas variables tienen la misma ecuación
característica inversa (el denominador).

Al establecer (1 − 𝑎11 𝐿)(1 − 𝑎22 𝐿) − 𝑎12 𝑎21 𝐿2 = 0 y resolver para L, obtenemos las dos raíces de
la ecuación característica inversa. Para trabajar con las raíces características (en lugar de las raíces
características inversas), define 𝜆 = 1∕L y escribe la ecuación característica como
(6.18)
Dado que las dos variables tienen la misma ecuación característica, las raíces características de (6.18)
determinan las trayectorias temporales de ambas variables. Las siguientes observaciones resumen las
trayectorias temporales de {𝑦𝑡 } y {𝑧𝑡 }:
1. Si ambas raíces características (𝜆1 , 𝜆2 ) están dentro del círculo unitario, (6.16) y (6.17) proporcionan
soluciones estables para {yt} y {zt}. Si t es suficientemente grande o si las condiciones iniciales son
tales que la solución homogénea es cero, la condición de estabilidad garantiza que las variables son
estacionarias, y al haber estacionariedad las variables no pueden ser cointegradas de orden (1,1)
2. Si alguna de las raíces está fuera del círculo unitario, las soluciones son explosivas. Ninguna de las
variables es estacionaria en diferencias, por lo que no pueden ser CI(1,1). De la misma manera, si
ambas raíces características son unidad, la segunda diferencia de cada variable será estacionaria.
Dado que cada una es I(2), las variables no pueden ser CI(1,1).
3. Como se puede ver en (6.14) y (6.15), si 𝑎12 = 𝑎21 = 0, la solución es trivial. Para que {yt} y
{zt} sean procesos de raíz unitaria, es necesario que 𝑎11 = 𝑎22 = 1. Se sigue que 𝜆1 = 𝜆2 = 1 y
que las dos variables evolucionan sin ninguna relación de equilibrio a largo plazo; por lo tanto, las
variables no pueden ser cointegradas.
4. Para que {𝑦𝑡 } y {𝑧𝑡 } sean CI(1,1), es necesario que una raíz característica sea igual a la unidad y la
otra sea menor que la unidad en valor absoluto. En este caso, cada variable tendrá la misma tendencia
estocástica y la primera diferencia de cada variable será estacionaria. Por ejemplo, si 𝜆1 = 1, (6.16)
tendrá la forma:

Sería estacionario si |𝜆2 | es menor a 1
Por lo tanto, para asegurar que las variables sean CI(1,1), debemos establecer que una de las raíces
características sea igual a la unidad y la otra a un valor que sea menor que la unidad en valor absoluto.
Para que la raíz más grande de las dos sea igual a la unidad, la fórmula cuadrática indica que:

Después de una simplificación, los coeficientes satisfacen:
(6.19)
Ahora considera la segunda raíz característica. Dado que 𝑎12 y/o 𝑎21 deben diferir de cero si las variables
están cointegradas, la condición |𝜆2 | < 1 requiere
(6.20) y

(6.21)

Las ecuaciones (6.19), (6.20) y (6.21) son restricciones que debemos imponer en los coeficientes de
(6.14) y (6.15) si queremos asegurar que las variables están cointegradas de orden (1, 1). Para entender
cómo estas restricciones de coeficientes afectan la naturaleza de la solución, escribimos (6.14) y (6.15)
como:
(6.22) o

Estas ecuaciones forman un modelo de corrección de errores. Si tanto 𝑎12 como 𝑎21 difieren de cero,
podemos normalizar el vector de cointegración con respecto a cualquiera de las variables. Normalizando
con respecto a 𝑦𝑡 , obtenemos:

Podemos ver que 𝑦𝑡 y 𝑧𝑡 cambian en respuesta a la desviación del período anterior del equilibrio a largo
plazo 𝑦𝑡−1 − 𝛽𝑧𝑡−1 . Si 𝑦𝑡−1 = 𝛽𝑧𝑡−1 entonces, 𝑦𝑡 y 𝑧𝑡 cambian solo en respuesta a los choques 𝜀𝑦𝑡 y
𝜀𝑧𝑡 . Además, si 𝛼𝑦 < 0 y 𝛼𝑧 > 0, 𝑦𝑡 disminuye y 𝑧𝑡 aumenta en respuesta a una desviación positiva del
equilibrio a largo plazo. También, las condiciones (6.20) y (6.21) aseguran que 𝛽 ≠ 0 y que al menos
uno de los parámetros de velocidad de ajuste (es decir, 𝛼𝑦 y 𝛼𝑧 ) no es igual a cero. La secuencia {𝑧𝑡 }
realiza toda la corrección para eliminar cualquier desviación del equilibrio a largo plazo. Dado que {𝑦𝑡 }
no realiza ninguna corrección de errores, se dice que {𝑦𝑡 } es débilmente exógena.
Para resaltar algunas de las implicaciones importantes de este modelo simple, hemos mostrado lo
siguiente:
1. Las restricciones necesarias para garantizar que las variables sean CI(1, 1) garantizan que exista un
modelo de corrección de errores. En nuestro ejemplo, tanto {𝑦𝑡 } como {𝑧𝑡 } son procesos de raíz
unitaria pero la combinación lineal 𝑦𝑡 − 𝛽𝑧𝑡 es estacionaria; el vector de cointegración normalizado
es [1, −(1 − 𝑎22 )/𝑎21 ]. Las variables tienen una representación de corrección de errores con
coeficientes de velocidad de ajuste 𝛼𝑦 = −𝑎12 𝑎 21 ∕ (1 − 𝑎22 ) y 𝛼𝑧 = 𝑎21 .
También se demostró que un modelo de corrección de errores para variables I(1) implica
necesariamente cointegración. Este hallazgo ilustra el teorema de representación de Granger que
establece que para cualquier conjunto de variables I(1), la corrección de errores y la cointegración
son representaciones equivalentes.
2. La cointegración requiere restricciones en los coeficientes de un modelo VAR. Es importante darse
cuenta de que un sistema cointegrado puede ser visto como una forma restringida de un modelo
VAR general.
(6.25)
Claramente, estimar un VAR de variables cointegradas solo con primeras diferencias no es apropiado.
Eliminar la expresión 𝜋xt−1 de la estimación de (6.25) suprimiría la parte de corrección de errores del
modelo. Además, es esencial tener en cuenta que las filas de 𝜋 no son linealmente independientes si las
variables están cointegradas. Al multiplicar cada elemento en la fila 1 por −(1 − a22)∕a12, se obtiene el
elemento correspondiente en la fila 2, haciendo que el determinante de 𝜋 sea igual a cero, lo que indica
que yt y zt tienen una representación de corrección de errores según (6.23) y (6.24).
Este ejemplo de dos variables ilustra la importancia del trabajo de Johansen (1988) y Stock y Watson
(1988), quienes mostraron que el rango de 𝜋 puede utilizarse para determinar si dos variables {yt} y
{zt} están cointegradas. Si el determinante de 𝜋 es cero y la mayor raíz característica es igual a la unidad
(𝜆1 = 1), entonces 𝜋 tiene un rango igual a la unidad. Por otro lado, si el rango de 𝜋 es cero, significaría
que 𝑎11 = 1, 𝑎22 = 1 y 𝑎12 = 𝑎21 = 0, lo que implicaría que el VAR representado por (6.14) y (6.15)
sería solo una secuencia de primeras diferencias sin vector de cointegración.
En un sistema cointegrado, ambas variables responderán a una desviación del equilibrio a largo plazo.
Sin embargo, puede haber casos donde uno de los parámetros de velocidad de ajuste sea cero, lo que
significa que una de las variables no responde a la discrepancia del equilibrio a largo plazo. Esto se
conoce como una variable débilmente exógena. En tales casos, se puede estimar un modelo
econométrico para la otra variable sin tener en cuenta la primera.
Además, es necesario reconsiderar la causalidad de Granger en un sistema cointegrado. Si una variable
no responde a los rezagos de la otra en su ecuación de diferencias, entonces la segunda variable se

considera débilmente exógena y no es causada por Granger por la primera. Esto tiene implicaciones
importantes en el análisis causal en modelos cointegrados.
El caso de n variables
Poco cambia en el caso de n variables. La relación entre cointegración, corrección de errores y el rango
de la matriz 𝜋 es invariable al agregar variables adicionales al sistema. La característica interesante
introducida en el caso de n variables es la posibilidad de múltiples vectores de cointegración. Ahora
consideremos una versión más general de (6.25):
(6.26)

Restando 𝑥𝑡−1 de ambos lados de (6.26) y dejando que I sea una matriz identidad de tamaño (n ⋅ n),
obtenemos:
(6.27)

donde 𝜋 es la matriz de tamaño (n ⋅ n) dada por −(𝐼 − 𝐴1) y 𝜋𝑖𝑗 denota el elemento en la fila i y la
columna j de 𝜋. Como se puede observar, (6.27) es un caso especial de (6.13) donde todos los 𝜋𝑖 son
iguales a cero. La cuestión crucial para la cointegración sigue siendo el rango de la matriz 𝜋. La única
forma de que el rango de una matriz sea cero es que cada uno de sus elementos sea cero. Por lo tanto, si
el rango de 𝜋 es cero, cada elemento de 𝜋 debe ser cero, lo que implica que no hay vectores de
cointegración. En este caso, (6.27) es equivalente a un VAR de n variables en primeras diferencias:
𝛥𝑥𝑡 = 𝜀𝑡
Aquí, cada 𝛥𝑥𝑖𝑡 = 𝜀𝑖𝑡 , por lo que todas las secuencias {𝑥𝑖𝑡 } son procesos de raíz unitaria y no hay
ninguna combinación lineal de las variables que sea estacionaria.
En el otro extremo, supongamos que 𝜋 tiene rango completo. La solución a largo plazo de (6.27) está
dada por las n ecuaciones independientes:
(6.28)

Cada una de estas ecuaciones n es una restricción independiente en la solución a largo plazo de las
variables; las n variables en el sistema enfrentan n restricciones a largo plazo. En este caso, cada una de
las n variables contenidas en el vector 𝑥𝑡 debe ser estacionaria con los valores a largo plazo dados por
la solución a 6.28. Las variables no pueden ser CI(1,1) ya que todas son estacionarias. En casos
intermedios, en los que el rango de 𝜋 es igual a 𝑟 < 𝑛, hay 𝑟 vectores de cointegración. Con 𝑟
ecuaciones independientes y 𝑛 variables, hay 𝑛 − 𝑟 tendencias estocásticas en el sistema. Si 𝑟 = 1,
hay un único vector de cointegración dado por cualquier fila de la matriz 𝜋. Cada secuencia {𝛥𝑥𝑖𝑡 } se
puede escribir en forma de corrección de errores.
IMPORTANTE: El punto principal aquí es que hay tres formas importantes de probar la cointegración.
La metodología de Engle–Granger busca determinar si los residuos de la relación de equilibrio son
estacionarios. La metodología de Johansen (1988) determina el rango de 𝜋 y el método de corrección
de errores examina los coeficientes de velocidad de ajuste.

4. TESTING FOR COINTEGRATION: THE ENGLE–GRANGER METHODOLOGY
Para explicar el procedimiento de prueba de Engle-Granger, comencemos con el tipo de problema que
probablemente se encuentre en estudios aplicados. Supongamos que se cree que dos variables, digamos
𝑦𝑡 y 𝑧𝑡 , están integradas de orden 1 y queremos determinar si existe una relación de equilibrio entre las
dos. Engle y Granger (1987) proponen un procedimiento de cuatro pasos para determinar si dos variables
I(1) están cointegradas de orden CI(1, 1).
PASO 1: Preprueba las variables para su orden de integración.
Este paso es fundamental para establecer la base de la prueba de cointegración. La cointegración implica
que dos o más series temporales, aunque no sean estacionarias individualmente, pueden tener una
relación de equilibrio a largo plazo. Para verificar si las variables están cointegradas, primero debemos
determinar su orden de integración. Esto se hace a través de pruebas estadísticas como la prueba
aumentada de Dickey-Fuller, que examina la presencia de raíces unitarias en las series de tiempo. Si
ambas variables son estacionarias, no hay necesidad de proceder, ya que los métodos estándar de series
temporales pueden aplicarse. Sin embargo, si las variables tienen diferentes órdenes de integración, esto
sugiere que no están cointegradas.
PASO 2: Estima la relación de equilibrio a largo plazo.
El Paso 2 implica estimar la relación de equilibrio a largo plazo entre las variables. Después de verificar
en el Paso 1 que tanto {𝑦𝑡 } como {𝑧𝑡 } son I(1), procedemos a modelar su relación en términos de una
ecuación de regresión simple:
𝑦𝑡 = 𝛽0 + 𝛽1 𝑧𝑡 + 𝑒𝑡
Si las variables están cointegradas, una regresión de Mínimos Cuadrados Ordinarios (OLS) proporciona
un estimador "super-consistente" de los parámetros cointegrados 𝛽0 y 𝛽1 . Este método es
particularmente poderoso porque, como demostró Stock (1987), las estimaciones OLS de 𝛽0 y 𝛽1
convergen más rápidamente que en modelos OLS que utilizan variables
estacionarias.
Para entender mejor la idea detrás de este enfoque, podemos referirnos a
un diagrama de dispersión como el mostrado en la Figura 6.1. Aquí, se
puede observar que el efecto de la tendencia común domina sobre el
componente estacionario; ambas variables parecen subir y bajar en
tándem. Esto sugiere una fuerte relación lineal, como lo muestra la línea
de regresión trazada en la figura.

Para verificar si las variables están realmente cointegradas, calculamos la secuencia residual de esta
ecuación, denotada como εt (sombrerito). Esta secuencia contiene los valores estimados de las
desviaciones de la relación de largo plazo. Si estas desviaciones se encuentran estacionarias, lo cual se
puede determinar a través de una prueba de Dickey-Fuller sobre estos residuos, entonces concluimos
que las secuencias {yt} y {zt} están cointegradas de orden (1, 1).
𝛥𝑒̂𝑡 = 𝑎1 𝑒̂𝑡−1 + 𝜀𝑡
En este caso, si no podemos rechazar la hipótesis nula de 𝑎1 = 0, podemos afirmar que la serie tiene
una raíz unitaria y por consecuencia se concluye que {𝑦𝑡 } y {𝑧𝑡 } no están cointegradas. (Es necesario
que los errores estimados sean estacionarios)
En la mayoría de los estudios aplicados, no es posible utilizar directamente las tablas de Dickey-Fuller.
Esto se debe a que la secuencia 𝑒t(sombrerito) se genera a partir de una ecuación de regresión; el
investigador no conoce el error real 𝑒𝑡 , solo la estimación del error 𝑒𝑡 (sombrerito). La metodología
utilizada para ajustar la regresión en (6.30) selecciona valores de 𝛽0 y 𝛽1 que minimizan la suma de los
residuos al cuadrado. Por lo tanto, para realizar pruebas sobre la estacionariedad de los residuos,
necesitamos técnicas adecuadas que tengan en cuenta esta estructura de la regresión.

PASO 3: Estima el modelo de corrección de errores.
El Paso 3 implica estimar el modelo de corrección de errores cuando las variables están cointegradas, lo
que significa que se rechaza la hipótesis nula de no cointegración. Los residuos de la regresión de
equilibrio se utilizan para estimar este modelo de corrección de errores. Si {𝑦𝑡 } y {𝑧𝑡 } son CI(1, 1),
entonces las variables tienen la forma de corrección de errores:

donde 𝛽1 es el parámetro del vector cointegrado dado por (6.30), 𝜀yt y 𝜀zt son perturbaciones de ruido
blanco (que pueden estar correlacionadas entre sí), y 𝛼1 , 𝛼2 , 𝛼𝑦 , 𝛼𝑧 , 𝛼11 (𝑖), 𝛼12 (𝑖), 𝛼21 (𝑖), 𝛼22 (𝑖) son
todos parámetros.
Engle y Granger (1987) proponen una manera ingeniosa de evitar las restricciones entre ecuaciones
involucradas en la estimación directa de (6.33) y (6.34). La magnitud del residuo 𝑒̂𝑡−1 es la desviación
del equilibrio a largo plazo en el período t − 1. Por lo tanto, es posible utilizar los residuos guardados
{𝑒̂𝑡−1 } obtenidos en el Paso 2 como una estimación de la expresión 𝑦𝑡−1 − β1 zt−1 en (6.33) y (6.34).
Así, utilizando los residuos guardados de la estimación de la relación de equilibrio a largo plazo, se
estima el modelo de corrección de errores como:

Además del término de corrección de errores êt−1, (6.35) y (6.36) constituyen un VAR en primeras
diferencias.. Todos los procedimientos desarrollados para estimar un VAR se aplican al sistema
representado por las ecuaciones de corrección de errores. Es importante destacar:
1. La estimación por OLS es una estrategia eficiente ya que cada ecuación contiene el mismo
conjunto de regresores.
2. Dado que todos los términos en (6.35) y (6.36) son estacionarios [es decir, 𝛥𝑦𝑡 y sus rezagos,
𝛥𝑧𝑡 y sus rezagos, y 𝑒̂𝑡−1 son I(0)], las estadísticas de prueba utilizadas en el análisis VAR
tradicional son apropiadas para (6.35) y (6.36). Por ejemplo, las longitudes de rezago pueden
determinarse utilizando una prueba 𝜒2, y la restricción de que todos los 𝛼𝑗𝑘 (𝑖) = 0 puede
verificarse utilizando una prueba F. Si hay un solo vector cointegrado, las restricciones
relacionadas con 𝛼y o 𝛼z pueden realizarse utilizando una prueba t.
PASO 4: Evalúa la adecuación del modelo.
El Paso 4 implica evaluar la adecuación del modelo mediante varios procedimientos:
1. Se deben realizar controles diagnósticos para determinar si los residuos de las ecuaciones de
corrección de errores se aproximan al ruido blanco. Si los residuos están correlacionados en
serie, las longitudes de rezago pueden ser demasiado cortas, por lo que se debe reestimar el
modelo utilizando longitudes de rezago que produzcan errores serialmente no correlacionados.
Es posible que se necesite permitir rezagos más largos de algunas variables que de otras. Si es
así, se puede ganar eficiencia estimando el VAR cercano utilizando el método de regresiones
aparentemente no relacionadas (SUR). Los ejercicios de pronóstico fuera de la muestra
también son útiles para seleccionar entre modelos alternativos.
2. Los coeficientes de velocidad de ajuste 𝛼𝑦 y 𝛼𝑧 son de particular interés ya que tienen
importantes implicaciones para la dinámica del sistema. Como se muestra en la Sección 3, los
valores de 𝛼𝑦 y 𝛼𝑧 están directamente relacionados con las raíces características del sistema de
ecuaciones de diferencia. La convergencia directa implica que 𝛼𝑦 sea negativo y 𝛼𝑧 sea positivo.

Si nos centramos en (6.36), es claro que para cualquier valor dado de 𝑒̂𝑡−1 , un valor grande de
𝛼𝑧 está asociado con un valor grande de 𝛥𝑧𝑡 . Si 𝛼𝑧 es cero, el cambio en 𝑧𝑡 no responde en
absoluto a la desviación del equilibrio a largo plazo en (𝑡 − 1). Si 𝛼𝑧 es cero y si todos los
𝛼21 (𝑖) = 0, entonces se puede decir que {𝛥𝑦𝑡 } no causa Granger a {𝛥𝑧𝑡 }. Sabemos que 𝛼𝑦 y/o
𝛼𝑧 deben ser significativamente diferentes de cero si las variables están cointegradas. Después
de todo, si tanto 𝛼𝑦 como 𝛼𝑧 son cero, no hay corrección de error y (6.35) y (6.36) no
comprenden más que un VAR en primeras diferencias. Además, los valores absolutos de estos
coeficientes de velocidad de ajuste no deben ser demasiado grandes. Las estimaciones puntuales
deben implicar que 𝛥𝑦𝑡 y 𝛥𝑧𝑡 convergen a la relación de equilibrio a largo plazo.
3. Al igual que en un análisis VAR tradicional, Lutkepohl y Reimers (1992) muestran que el
análisis de innovación (es decir, respuestas a impulsos y análisis de descomposición de varianza)
puede utilizarse para obtener información sobre las interacciones entre las variables. Como
cuestión práctica, las dos innovaciones 𝜀𝑦𝑡 y 𝜀𝑧𝑡 pueden estar correlacionadas simultáneamente
si 𝑦𝑡 tiene un efecto simultáneo en 𝑧𝑡 y/o si 𝑧𝑡 tiene un efecto simultáneo en 𝑦𝑡 . Al obtener
funciones de respuesta a impulsos y descomposiciones de varianza, algún método, como una
descomposición de Choleski, debe usarse para ortogonalizar las innovaciones.
La forma de las funciones de respuesta a impulsos y los resultados de las descomposiciones de varianza
pueden indicar si las respuestas dinámicas de las variables se ajustan a la teoría. Dado que todas las
variables en (6.35) y (6.36) son I(0), las respuestas a impulsos de Δyt y Δzt deben converger a cero. Se
deben de revisar los resultados de cada paso si se obtiene una función de respuesta a impulsos que no
decae o es explosiva
Además, es importante mencionar que es muy tentador usar las t-estadísticas para realizar pruebas de
significancia en el vector cointegrado. Sin embargo, se debe evitar esta tentación ya que, en general, los
coeficientes no tienen una distribución t asintótica.
5. ILLUSTRATING THE ENGLE-GRANGER METHODOLOGY

El proceso comienza generando series de datos simuladas {yt}, {zt} y {wt}, cada una construida como
la suma de un componente de tendencia estocástica y un componente irregular autorregresivo. Esto se
logra utilizando secuencias de números aleatorios para representar los componentes estocásticos y los
componentes irregulares de cada serie.
Después de construir estas series, se presta atención a si las variables están integradas de orden 1 o si
tienen una raíz unitaria a través de pruebas de raíz unitaria y diferentes longitudes de rezago. Luego de
establecer la cointegración entre las variables, se estima la relación de equilibrio a largo plazo con la
metodología de Engle-Granger, estimando una regresión de cointegración que representa la relación de
largo plazo entre las variables:

Después de estimar la relación de equilibrio, se verifica si los residuos de esta regresión son estacionarios
con pruebas de raíz unitaria, lo que es crucial para confirmar la validez del modelo. Una vez confirmada
la cointegración y la estacionariedad de los residuos, se estima el modelo de corrección de errores. Este
modelo tiene en cuenta tanto los efectos de corto plazo como los de largo plazo en la relación entre las
variables, y se estima utilizando una metodología de ecuaciones en diferencias.
El modelo de corrección de errores se presenta como sigue:

donde 𝑒𝑤𝑡−1 = 𝑤𝑡−1 + 0.0852 − 0.9901𝑦𝑡−1 − 0.9535𝑧𝑡−1 es el valor rezagado del residuo de la
relación de equilibrio utilizando 𝑤𝑡 como la variable dependiente.
Finalmente, se llevan a cabo pruebas de diagnóstico para evaluar la adecuación del modelo estimado.
Estas pruebas incluyen la evaluación de la autocorrelación de los residuos, la determinación de la
presencia de efectos de corto plazo y la verificación de si el modelo captura adecuadamente la dinámica
de las variables a lo largo del tiempo.
El procedimiento para variables con integración de orden 2 – I(2)
La multicointegración se refiere a una situación en la que una combinación lineal de variables I(2) e I(1)
está integrada de orden cero. Esto significa que una combinación lineal de variables I(2) puede estar
cointegrada con una variable I(1), lo que implica una relación de equilibrio a largo plazo entre ellas.
Por ejemplo, supongamos que 𝑥1𝑡 y 𝑥2𝑡 son variables I(2) y que 𝑧𝑡 es I(1). Es posible que una
combinación lineal de 𝑥1𝑡 y 𝑥2𝑡 sea I(1) y que esta combinación esté cointegrada con 𝑧𝑡 . Esto se
representa mediante una relación de equilibrio a largo plazo de la forma:
Sin embargo, una especificación más rica permite una relación estacionaria:

Esto permite que la combinación lineal 𝑥1𝑡 − 𝛽2 𝑥2𝑡 sea I(1) y cointegrada con otras variables I(1) en el
sistema: 𝛥𝑥2𝑡 y 𝑧𝑡 . Es importante destacar que 𝛽2 no puede ser cero, ya que si lo fuera, la variable I(2)
𝑥1𝑡 no podría estar cointegrada con las variables I(1).
Para verificar la multicointegración, se puede utilizar un procedimiento de dos pasos. Primero, se busca
una relación de cointegración entre las variables I(2), y luego se utiliza esta relación para verificar una
posible cointegración con las variables I(1) restantes. Sin embargo, este procedimiento solo es efectivo
si se conoce el vector cointegrante para el primer paso.
Para realizar este análisis, se estima una ecuación de regresión con términos deterministas que pueden
incluir una tendencia cuadrática en el tiempo.

Como la clave es evaluar la estacionalidad de la serie {et}, se estima la siguiente ecuación:

Si es posible rechazar la hipótesis nula 𝜌 = 0, entonces se puede concluir que hay multicointegración.
Además del tamaño de la muestra, los valores críticos de la estadística t para la hipótesis nula 𝜌 = 0
dependen del número de regresores I(2) (𝑚2 = 1 𝑜 2), el número de regresores I(1) (𝑚1 = 0 𝑎 4) y
la forma de los regresores deterministas.

Tomemos como ejemplo las ecuaciones de demanda de dinero en el Reino Unido durante el período
de muestra de 1963Q1 a 1989Q2, estimadas por Haldrup (1994):

La preprueba de las variables indicó que 𝑚𝑡 (medida por el logaritmo de M1) y 𝑝𝑡 (el logaritmo del
deflactor implícito de precios) eran I(2), mientras que 𝑦𝑡 (el logaritmo del gasto final total) y 𝑟𝑡 (una
medida de la diferencia de tasas de interés) eran I(1). La presencia de 𝛥𝑝𝑡 en la función de demanda de
dinero se explica por la dependencia de la demanda de dinero de la tasa de inflación. Dado un total de
105 observaciones, un regresor I(2) (entonces m2 = 1) y tres regresores I(1), los valores críticos al 5%
para modelos sin y con tendencia lineal son -4.56 y -4.91, respectivamente. Utilizando los residuos de
las ecuaciones de demanda de dinero dadas por (6.41) y (6.42), Haldrup encontró que las estadísticas t
para la hipótesis nula 𝜌 = 0 fueron -2.35 y -2.66, respectivamente. Por lo tanto, es posible concluir que
las dos regresiones son espurias (es decir, no es posible rechazar la hipótesis nula de no
multicointegración).
A pesar del fracaso de la multicointegración, Haldrup continúa experimentando con varias estimaciones
del mecanismo de corrección de errores. Un modelo interesante es:

donde los regresores estacionarios pueden incluir valores rezagados de 𝛥2 𝑚𝑡 , así como valores actuales
y rezagados de 𝛥2 𝑝𝑡 , 𝛥𝑦𝑡 , 𝛥𝑝𝑡 y 𝛥𝑟𝑡 . La estimación puntual sugiere que 𝛥2 𝑚𝑡 disminuirá en respuesta
a una discrepancia positiva de la relación de equilibrio a largo plazo. La estadística t de −0.04/0.02 =
2 sugiere que el efecto es significativo justo al nivel del 5%.
6. COINTEGRATION AND PURCHASING POWER PARITY
𝑒𝑡 representa el logaritmo del tipo de cambio extranjero, 𝑝𝑡∗ representa el logaritmo del nivel de precios
extranjero y 𝑝𝑡 representa el logaritmo del nivel de precios domésticos. La PPP a largo plazo requieren
que 𝑒𝑡 + 𝑝𝑡∗ − 𝑝𝑡 sea estacionario. La cointegración ofrece un método alternativo para comprobar la
teoría; si la PPP se mantiene, la secuencia formada por 𝑒𝑡 + 𝑝𝑡∗ debería estar cointegrada con la secuencia
𝑝𝑡 .
Denomina al valor en dólares del nivel de precios extranjero 𝑓𝑡 = 𝑒𝑡 + 𝑝𝑡 . La PPP a largo plazo sostiene
que existe una combinación lineal de la forma 𝑓𝑡 = 𝛽0 + 𝛽1 𝑝𝑡 + 𝑢𝑡 tal que 𝑢𝑡 es estacionario y el vector
de cointegración es tal que 𝛽1 = 1.
El siguiente paso fue estimar la relación de equilibrio a largo plazo mediante la regresión de cada
𝑓𝑡 = 𝑒𝑡 + 𝑝𝑓𝑡 en 𝑝𝑡 de manera que: 𝑓𝑡 = 𝛽0 + 𝛽1 𝑝𝑡 + 𝑢𝑡
(6.43)
La PPP absoluta establece que 𝑓𝑡 = 𝑝𝑡 , requiriendo 𝛽0 = 0 y 𝛽1 = 1. El intercepto 𝛽0 es coherente con
la PPP relativa. Sin embargo, se recomienda incluir un término de intercepto en la regresión de
equilibrio, y las simulaciones de Monte Carlo de Engle y Granger (1987) confirman esto.
Los valores estimados de 𝛽1 y sus errores estándar
asociados se reportan en la Tabla 6.4. Cinco de los seis
valores se estiman bastante por debajo de la unidad.
No se deben de sobreestimar los resultados. O sea, no
se debe concluir que cada valor de 𝛽1 es
significativamente diferente de la unidad solo porque
(1 − 𝛽1 ) excede dos o tres desviaciones estándar. Las
suposiciones de este tipo de prueba t no son aplicables
porque no se asume que 𝑝𝑡 es la variable exógena y
𝑓𝑡 es la dependiente, o que 𝑢𝑡 es ruido blanco.

Se revisaron las raíces unitarias de los residuos de cada ecuación de regresión 𝑢𝑡 . Se estima las siguientes
2 ecuaciones utilizando los residuos de cada relación de equilibrio a largo plazo:
∆𝑢𝑡 = 𝑎1 𝑢̂𝑡−1 + 𝜀𝑡

(6.44)

∆𝑢𝑡 = 𝑎1 𝑢̂𝑡−1 + ∑ 𝑎1 ∆𝑢̂𝑡−1 + 𝜀𝑡

(6.45)

𝑝

𝑖=1

La Tabla 6.5 informa los valores estimados de 𝑎1 a partir de
(6.44) y (6.45) usando un rezago de cuatro períodos. Es
importante destacar que la no rechazación de la hipótesis
nula 𝑎1 = 0 significa que no podemos descartar la hipótesis
nula de no cointegración. Alternativamente, si −2 < 𝑎1 < 0,
es posible concluir que la secuencia 𝑢𝑡 no tiene una raíz
unitaria y que las secuencias 𝑓𝑡 y 𝑝𝑡 están cointegradas.
Bajo la hipótesis nula 𝑎1 = 0, los valores críticos para el
estadístico t dependen del tamaño de la muestra. Conforme
los resultados de la tabla 6.5, solo para Japón durante el
período de tipo de cambio fijo, se puede rechazar la hipótesis
nula de no cointegración. A un nivel de significancia del 5%,
el valor crítico de t es -3.398 para dos variables y T=100. Por
lo tanto, a este nivel de significancia, podemos rechazar la
hipótesis nula de no cointegración (aceptamos que las variables están cointegradas) y encontramos a
favor de la PPA. Para los demás países en cada período de tiempo, no podemos rechazar la hipótesis
nula de no cointegración y debemos concluir que la PPA falló en general.
El tercer paso en la metodología implica la estimación del modelo de corrección de errores. Solo el
modelo Japón/EE. UU. requiere estimación (mantienen la cointegración). Los modelos de corrección de
errores finales para los niveles de precios de Japón y EE. UU. durante el período de 1960-1971 fueron
estimados como:

Los resultados en (6.46) y (6.47) muestran una convergencia directa hacia el equilibrio a largo plazo.
Por ejemplo, si hay una desviación de una unidad de la PPP a largo plazo en el período 𝑡 − 1, el nivel
de precios japonés en dólares disminuye en 0.10548 unidades y el nivel de precios estadounidense
aumenta en 0.01114 unidades en el período t, lo que compensa la discrepancia previa, resaltando que el
coeficiente japonés es aproximadamente diez veces mayor que el estadounidense en valor absoluto.
Mientras que el nivel de precios de EE. UU. responde ligeramente a las desviaciones de la PPP, el de
Japón muestra una mayor sensibilidad. El término de corrección de errores para EE. UU. es cerca de 1/3
de una desviación estándar de cero, mientras que para Japón es aproximadamente 2.5 desviaciones
estándar de cero. Por lo tanto, a un nivel de significancia del 5%, concluimos que el término de
velocidad de ajuste no difiere significativamente de cero para Estados Unidos, pero sí para Japón.
Esto sugiere que Estados Unidos, siendo un país grande en comparación con Japón, tuvo movimientos
de precios independientes de Japón, mientras que los precios ajustados por tipo de cambio de Japón
respondieron a los eventos en Estados Unidos.
7. CHARACTERISTIC ROOTS, RANK, AND COINTEGRATION
Engle y Granger (1987): Se utiliza para analizar si hay una relación de equilibrio de largo plazo entre
dos o más series de tiempo. Se basa en la idea de que, si dos series están cointegradas, existe una relación
de equilibrio a largo plazo entre ellas, a pesar de que puedan mostrar comportamientos diferentes en el
corto plazo:
𝑌𝑡 = 𝛽10 + 𝛽11 𝑍𝑡 + 𝑒1𝑡

𝑜

𝑍𝑡 = 𝛽20 + 𝛽21 𝑌𝑡 + 𝑒2𝑡

Cuando la muestra es grande, la prueba de raíz unitaria puede ser problemática. Se puede encontrar que
las variables parecen estar cointegradas en una regresión, pero no en otra, lo cual es indeseable. Este
problema es más complejo con tres o más variables, ya que no se sabe que variable irá al lado izquierdo
y puede haber más de un vector cointegrante, y no existe un procedimiento para la estimación por
separado de los múltiples vectores cointegrantes.
Otro problema con el modelo es que se basa en un método de dos pasos para estimar los coeficientes.
En el primer paso, se calculan los errores de la regresión original. En el segundo paso, estos errores se
utilizan para estimar los coeficientes de la ecuación de corrección de errores. Esto significa que
cualquier error en el cálculo de los errores de la primera etapa afectará los resultados finales.
Los estimadores de máxima verosimilitud de Johansen (1998) y Stock y Watson son métodos que no
requieren el uso de estimadores de dos pasos. Estos métodos pueden estimar y probar si hay varios
vectores cointegrantes presentes en un conjunto de datos. Permiten al investigador probar diferentes
versiones restringidas del vector cointegrante y los parámetros de velocidad de ajuste.
El procedimiento de Johansen (1998) se basa en la relación entre el rango de una matriz y sus raíces
características. Es una generalización multivariada de la prueba de Dickey-Fuller, que se utiliza para
analizar la estacionariedad de {𝑌𝑡 } como dependiente de 𝑎1 en series de tiempo univariadas:
𝑌𝑡 = 𝑎1 𝑌𝑡−1 + 𝜀𝑡

𝑜

∆𝑌𝑡 = (𝑎1 − 1)𝑌𝑡−1 + 𝜀𝑡

Si (𝑎1 − 1) = 0, el proceso {𝑌𝑡 } tiene una raíz unitaria, si (a1 - 1) ≠ 0, la secuencia {𝑌𝑡 } es estacionaria.
Con Dickey-Fuller probamos formalmente la hipótesis nula (𝑎1 − 1) = 0.
Ahora consideremos la generalización simple a n variables:
𝑋𝑡 = 𝐴1 𝑋𝑡−1 + 𝜀𝑡 ,

𝑑𝑜𝑛𝑑𝑒 𝑋𝑡 𝑦 𝜀𝑡 𝑠𝑜𝑛 𝑣𝑒𝑐𝑡𝑜𝑟𝑒𝑠 𝑑𝑒 𝑑𝑖𝑚𝑒𝑛𝑠𝑖ó𝑛 𝑛 ∙ 1,

∆𝑋𝑡 = 𝐴1 𝑋𝑡−1 − 𝑋𝑡−1 + 𝜀𝑡 ,

𝐴1 𝑒𝑠 𝑢𝑛𝑎 𝑚𝑎𝑡𝑟𝑖𝑧 𝑑𝑒 𝑝𝑎𝑟á𝑚𝑒𝑡𝑟𝑜𝑠 𝑑𝑒 𝑑𝑖𝑚𝑒𝑛𝑠𝑖ó𝑛 𝑛 ∙ 𝑛

= (𝐴1 − 𝐼)𝑋𝑡−1 + 𝜀𝑡

𝐼 𝑒𝑠 𝑙𝑎 𝑚𝑎𝑡𝑟𝑖𝑧 𝑑𝑒 𝑖𝑑𝑒𝑛𝑡𝑖𝑑𝑎𝑑, 𝑦 𝜋 𝑒𝑠 (𝐴1 − 𝐼)

= 𝝅𝑿𝒕−𝟏 + 𝜺𝒕
El rango de (𝐴1 − 𝐼) es igual al número de vectores cointegrantes. Si (𝐴1 − 𝐼) consiste en todos ceros,
es decir, 𝑟𝑎𝑛𝑘(𝜋) = 0, todas las secuencias {𝑥𝑖𝑡 } son procesos de raíz unitaria y sus variables no están
cointegradas. Si todas las raíces características son menores que la unidad y si 𝒓𝒂𝒏𝒌(𝝅) = 𝒏, todas
las variables son estacionarias.
La ecuación 𝒓𝒂𝒏𝒌(𝝅) = 𝒏 se puede modificar fácilmente para permitir la presencia de un término de
tendencia:
∆𝑋𝑡 = 𝐴0 + 𝝅𝑋𝑡−1 + 𝜀𝑡 , 𝐴1 : 𝑚𝑎𝑡𝑟𝑖𝑧 𝑑𝑒 𝑑𝑖𝑚𝑒𝑛𝑠𝑖ó𝑛 𝑛 ∙ 1, 𝑑𝑒 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡𝑒𝑠 (𝑎10 , 𝑎20 , … , 𝑎𝑛0 )´
Se querría incluir el término de deriva si las variables muestran una clara tendencia a aumentar o
disminuir. A largo plazo, 𝜋𝑋𝑡−1 = 0, de modo que cada secuencia {𝛥𝑥𝑖𝑡 } tiene un valor esperado de
𝑎𝑖0 . Al agregar estos cambios a lo largo del tiempo, se obtiene la expresión 𝑎𝑖0𝑡 .
Manipulando adecuadamente los elementos de 𝐴0 , es posible incluir una constante en el vector o
vectores cointegrantes sin agregar una tendencia determinista en el tiempo al sistema.
Algunos prefieren incluir un intercepto en el vector cointegrante junto con un término de deriva si las
variables lo requieren según la teoría económica. Sin embargo, la intercepción en el vector cointegrante
no está identificada en presencia de un término de deriva, ya que parte de la deriva puede incluirse en el
vector de cointegración:
∆𝑋1𝑡 = (𝜋11 𝑋1𝑡−1 + 𝜋12 𝑋2𝑡−1 + ⋯ + 𝜋1𝑛 𝑋𝑛𝑡−1 + 𝑏10 ) + 𝑏11 + 𝜀1𝑡
⋮
∆𝑋𝑛𝑡 = 𝑆𝑛 (𝜋11 𝑋1𝑡−1 + 𝜋12 𝑋2𝑡−1 + ⋯ + 𝜋1𝑛 𝑋𝑛𝑡−1 + 𝑏10 ) + 𝑏𝑛1 + 𝜀1𝑡

Donde 𝑏𝑖1 se define como el valor que satisface 𝑠𝑖 𝑏10 + 𝑏𝑖1 = 𝑎10 . , dividiendo 𝑎10 en dos partes y
colocando una dentro de la relación cointegrante.
Es necesario un método de identificación porque la proporción de la deriva para incluir en el vector
cointegrante es arbitraria. Aunque se necesita un término de deriva fuera de la relación cointegrante para
capturar los efectos de una tendencia sostenida de las variables, la mayoría de los investigadores
incluyen términos de deriva solo si los datos lo justifican.
Es mejor evitar el uso de una tendencia como variable explicativa a menos que tenga una buena razón
para incluirla en el modelo. Johansen (1994) discute el papel de los regresores deterministas en una
relación cointegrante.
El modelo multivariado se puede generalizar para permitir un proceso autorregresivo de orden superior:
𝑋𝑡 = 𝐴1 𝑋𝑡−1 + 𝐴2 𝑋𝑡−2 + ⋯ + 𝐴𝑝 𝑋𝑡−𝑝 + 𝜀𝑡
𝑋𝑡 : 𝑒𝑠 𝑢𝑛𝑎 𝑚𝑎𝑡𝑟𝑖𝑧 𝑑𝑒 𝑑𝑖𝑚𝑒𝑛𝑠𝑖ó𝑛 𝑛 ∙ 1, 𝑑𝑒 𝑐𝑜𝑛𝑠𝑡𝑎𝑛𝑡𝑒𝑠 (𝑋1𝑡 , 𝑋2𝑡 , … , 𝑋𝑛𝑡 )´
𝜀𝑡 : 𝑒𝑠 𝑢𝑛 𝑢𝑛 𝑣𝑒𝑐𝑡𝑜𝑟 𝑛 − 𝑑𝑖𝑚𝑒𝑛𝑠𝑖𝑜𝑛𝑎𝑙 𝑖. 𝑖. 𝑑 𝑐𝑜𝑛 𝑚𝑒𝑑𝑖𝑎 𝑐𝑒𝑟𝑜 𝑦 𝑚𝑎𝑡𝑟𝑖𝑧 𝑑𝑒 𝑣𝑎𝑟𝑖𝑎𝑛𝑧𝑎 𝛴𝜀 .
Al realizar cálculos matemáticos, obtenemos:
𝑝−1

∆𝑋𝑡 = 𝜋𝑋𝑡−1 + ∑ 𝜋𝑖 ∆𝑋𝑡−𝑖 + 𝜀𝑡
𝑖=1

El rango de la matriz 𝜋 es clave; ya que indica el número de vectores cointegrantes independientes. Si
el rango(𝜋) = 0, la matriz es nula y la ecuación es el modelo VAR usual en primeras diferencias. Si 𝜋
tiene rango n, el proceso vectorial es estacionario. En casos intermedios, si 𝑟𝑎𝑛𝑔𝑜(𝜋) = 1, hay un solo
vector cointegrante y 𝜋𝑋𝑡−1 es el término de corrección de errores. Para otros casos en los que
1 < 𝑟𝑎𝑛𝑔𝑜(𝜋) < 𝑛, hay múltiples vectores cointegrantes. El número de vectores cointegrantes
distintos se puede obtener verificando la significancia de las raíces características de 𝜋.
En la práctica, solo podemos obtener estimaciones de 𝜋 y sus raíces características, y usamos dos
estadísticas de prueba para determinar el número de raíces características que no difieren
significativamente de la unidad:
𝑛

𝜆𝑡𝑟𝑎𝑐𝑒 (𝑟) = −𝑇 ∑ ln (1 − 𝜆̂𝑖 )
𝑖=𝑟+1

𝜆𝑚𝑎𝑥 (𝑟, 𝑟 + 1) = −𝑇 ln (1 − 𝜆̂𝑟+1 )
𝑑𝑜𝑛𝑑𝑒 𝜆̂𝑖 𝑠𝑜𝑛 𝑙𝑜𝑠 𝑣𝑎𝑙𝑜𝑟𝑒𝑠 𝑒𝑠𝑡𝑖𝑚𝑎𝑑𝑜𝑠 𝑑𝑒 𝑙𝑎𝑠 𝑟𝑎í𝑐𝑒𝑠 𝑐𝑎𝑟𝑎𝑐𝑡𝑒𝑟í𝑠𝑡𝑖𝑐𝑎𝑠 𝑜𝑏𝑡𝑒𝑛𝑖𝑑𝑜𝑠 𝑎 𝑝𝑎𝑟𝑡𝑖𝑟 𝑑𝑒 𝑙𝑎 𝑚𝑎𝑡𝑟𝑖𝑧 𝜋
La primera estadística prueba si el número de vectores cointegrantes es menor o igual a r, 𝜆𝑡𝑟𝑎𝑐𝑒 es igual
a cero cuando todas las 𝜆𝑖 = 0., mientras que la segunda prueba si es exactamente 𝑟 o 𝑟 + 1. Si el
valor estimado de la raíz característica está cerca de cero, 𝜆𝑚𝑎𝑥 será pequeño
Los valores críticos se obtienen mediante el enfoque de Monte Carlo y dependen:
1. Del número de componentes no estacionarias
2. De la forma de A0 en el modelo.
Un ejemplo en Dinamarca hecho por Johansen and Juselius (1990) ilustra que los resultados de las
pruebas 𝜆𝑡𝑟𝑎𝑐𝑒 𝑦 𝜆𝑚𝑎𝑥 pueden entrar en conflicto. La prueba 𝜆max tiene la hipótesis alternativa más
precisa y generalmente se prefiere para determinar el número de vectores cointegrantes.
8. HYPOTHESIS TESTING
En las pruebas de Dickey-Fuller y en el procedimiento de Johansen, es crucial determinar
correctamente la forma de los regresores determinísticos. Los valores críticos de las estadísticas 𝜆𝑡𝑟𝑎𝑐𝑒
y 𝜆𝑚𝑎𝑥 varían dependiendo de si se incluye un término de intercepción en el vector de cointegración.

En lugar de asumir la forma de A0 de manera arbitraria, se pueden probar formas restringidas del vector,
permitiendo examinar restricciones importantes en estudios como la demanda de dinero, como la
proporcionalidad a largo plazo entre el dinero y los precios, así como las elasticidades del ingreso y la
tasa de interés en la demanda de dinero. En la ecuación (6.1) 𝑚𝑡 = 𝛽0 + 𝛽1 𝑝𝑡 + 𝛽2 𝑦𝑡 + 𝛽3 𝑟𝑡 + 𝑒𝑡 , las
restricciones relevantes son 𝛽1 = 1, 𝛽2 > 0 𝑦 𝛽3 < 0.
En las pruebas de cointegración, sólo las combinaciones lineales estacionarias de variables son
relevantes. Si las restricciones en los parámetros de 𝜋 no afectan el número de vectores de cointegración,
este número se mantiene constante. Para determinar si hay una constante en lugar del sesgo no
restringido A0 en el vector de cointegración, se comparan las raíces características de dos formas del
modelo. La estadística resultante sigue una distribución 𝜒2 con (n - r) grados de libertad, donde n es el
número de raíces características y r es el número de vectores de cointegración.

La prueba compara las diferencias entre 𝑙𝑛(1 − 𝜆̂∗𝑖 ) y 𝑙𝑛(1 − 𝜆̂𝑖 ) para evaluar si las restricciones en
el vector de cointegración tienen un efecto significativo. Valores pequeños sugieren que incluir la
constante es aceptable, pero esto aumenta la probabilidad de encontrar una combinación lineal
estacionaria de las variables. Un valor alto de 𝜆̂∗𝑟+1 indica que las restricciones inflan artificialmente el
número de vectores de cointegración. Johansen (1991) demostró que, con una estadística de prueba
suficientemente grande, se puede rechazar la hipótesis nula de una constante en el vector de
cointegración, sugiriendo una tendencia lineal en las variables. Johansen y Juselius (1990) aplicaron
esta prueba a su modelo de demanda de dinero danesa y concluyeron que las variables no mostraban
una tendencia lineal, justificando así la inclusión de la constante en el vector de cointegración.
Para probar otras restricciones en el vector de cointegración, Johansen define las matrices 𝛼 y 𝛽, donde
𝜋 = 𝛼𝛽′. Estas matrices permiten estimar el modelo como un modelo de corrección de errores y
seleccionar los vectores de cointegración más significativos. Sin embargo, debido a las restricciones
entre ecuaciones, no es posible estimar 𝛼 y 𝛽 utilizando MCO. Empleando la estimación de máxima
verosimilitud, se puede estimar el modelo, determinar el rango de 𝜋 y seleccionar 𝛼 de manera que
𝜋 = 𝛼𝛽′.
En un modelo donde el rango de la matriz 𝜋 es igual a 1, las ecuaciones toman la forma:
𝜟𝒙𝒊𝒕 = 𝜶𝒊 (𝒙𝟏𝒕−𝟏 + 𝜷𝟐 𝒙𝟐𝒕−𝟏 +··· + 𝜷𝒏 𝒙𝒏𝒕−𝟏 ) +··· + 𝜺𝒊𝒕 (𝒊 = 𝟏, … , 𝒏)
o en forma matricial:

Para probar restricciones en los parámetros 𝛼 y 𝛽', se compara el número de vectores de cointegración
bajo la hipótesis nula y alternativa. La estadística de prueba es:

Asintóticamente, esta estadística tiene una distribución 𝜒2 con grados de libertad iguales al número de
restricciones impuestas en 𝛽. Los valores pequeños de 𝜆̂∗𝑖 en relación con 𝜆̂𝑖 (para i ≤ r) implican un
número reducido de vectores de cointegración. Por lo tanto, la restricción incrustada en la hipótesis nula
es vinculante si el valor calculado de la estadística de prueba excede el de una tabla 𝜒2. Por ejemplo,
Johansen y Juselius prueban la restricción de que el dinero y el ingreso se mueven proporcionalmente.
Su relación de equilibrio de largo plazo estimada es 𝑚2𝑡 = 1.03𝑦𝑡 − 5.21𝑖𝑡𝑏 + 4.22𝑖𝑡𝑑 + 6.06.
Restringen el coeficiente de ingreso a la unidad y encuentran los valores restringidos de los 𝜆̂∗𝑖 . Dado
que el modelo irrestricto tiene 𝑟 = 1 y −𝑇 𝑙𝑛(1 − 𝜆̂1 ) = 30.09, la ecuación (6.59) se convierte en
−30.04 + 30.09 = 0.05. Al tener solo una restricción en 𝛽, la estadística de prueba sigue una
distribución 𝜒2 con 1 grado de libertad. Sin embargo, dado que el valor obtenido, 0.05, no es
significativo, se concluye que la restricción no es vinculante.

Para probar restricciones en 𝛼, se sigue un procedimiento similar. Se restringe 𝛼 y se comparan las r
raíces características más significativas de los modelos restringido y no restringido utilizando la
ecuación (6.59). Si el valor calculado de esta ecuación supera el valor crítico de una tabla 𝜒2, con grados
de libertad igual al número de restricciones en 𝛼, entonces las restricciones pueden ser rechazadas. Por
ejemplo, Johansen y Juselius (1990) probaron la restricción de que solo la demanda de dinero responde
a las desviaciones del equilibrio de largo plazo, expresada formalmente como 𝛼2 = 𝛼3 = 𝛼4 = 0. Al
calcular la diferencia de la estadística de prueba con las raíces características restringidas y no
restringidas, encontraron un valor significativo, lo que sugiere un respaldo parcial a la hipótesis de que
la restricción no es vinculante. En resumen, si hay un solo vector de cointegración, tanto los métodos de
Engle-Granger como los de Johansen tienen la misma distribución asintótica. Además, se puede
emplear el modelo de corrección de errores estimado para probar restricciones en 𝛼, siendo la estadística
t equivalente a la prueba de Johansen en este caso.
Pruebas de longitud de rezago y causalidad
La prueba de longitud de rezago se puede entender considerando el sistema en la forma de (6.54), donde
todas las 𝛥𝑥𝑡−𝑖 son variables estacionarias. Esto permite usar la Regla 1 de Sims, Stock y Watson
(1990), que implica probar los coeficientes de interés en variables estacionarias con media cero
utilizando una distribución normal. Dado que la longitud del rezago depende solo de los valores de los
diversos 𝜋i, una distribución 𝜒2 es adecuada para probar cualquier restricción relacionada con la
longitud del rezago. La estadística de prueba
se
compara
con
una
distribución 𝜒2 con grados de libertad igual al número de restricciones en el sistema. Alternativamente,
se puede utilizar el AIC o SBC multivariado para determinar la longitud del rezago. Si se desea probar
las longitudes del rezago para una sola ecuación, un test F es apropiado.
La regla también implica que no se pueden realizar pruebas de causalidad de Granger en un sistema
cointegrado utilizando un test F estándar. Si el rango (𝜋) = 0, la causalidad de Granger implica sólo
variables estacionarias, lo que permite utilizar una distribución F estándar para las pruebas. Sin embargo,
si las variables están cointegradas, la causalidad de Granger involucra los coeficientes de 𝜋, que
multiplican variables no estacionarias. En este caso, no es apropiado utilizar un estadístico F para probar
la causalidad de Granger, ya que no se pueden escribir las restricciones de la prueba como restricciones
en un conjunto de variables I(0). Además, las pruebas de exogeneidad de bloque también están
descartadas en caso de que una variable esté cointegrada con otras, ya que no se puede utilizar una
prueba 𝜒2 estándar para determinar su inclusión en las ecuaciones correspondientes.
Diferenciar o no diferenciar
Diferenciar variables no estacionarias en un VAR puede ser problemático si están cointegradas, ya que
excluimos las relaciones de equilibrio de largo plazo entre ellas, lo que afecta negativamente a las
estimaciones y pruebas estadísticas al perder información importante. Por ello, es preferible usar
primeras diferencias si las variables I(1) no están cointegradas, ya que:
1. Evita la pérdida de poder en las pruebas al evitar estimar demasiados parámetros adicionales.
2. Permite pruebas de causalidad de Granger que siguen una distribución F estándar.
3. Ofrece respuestas de impulso más consistentes en horizontes de pronóstico largo.
Determinar la cointegración de variables I(1) es crucial. Se sugiere realizar pruebas de longitud de
rezago independientes de la cointegración y estimar un VAR irrestricto con ajuste estacional, seguido de
una prueba de cointegración. Si las variables no están cointegradas, se estiman en primeras diferencias;
si lo están, se utiliza el modelo de corrección de errores. Esto permite inferencias sobre cualquier
variable, excepto los vectores de cointegración, mediante estadísticas estándar, y proporciona
estimaciones consistentes mediante respuestas de impulso y descomposiciones de varianza.
Pruebas sobre Múltiples Vectores Cointegrantes
Cuando el rango de 𝜋 supera uno, no es sencillo interpretar los vectores de cointegración. Con múltiples
vectores de cointegración, cualquier combinación lineal de estos también es un vector de cointegración.

Afortunadamente, es posible identificar relaciones de comportamiento separadas al restringir
adecuadamente los vectores de cointegración individuales. La única complicación radica en ser claro
sobre el número de restricciones impuestas al sistema. Es importante destacar que si hay r relaciones de
cointegración en un sistema de n variables, existe un vector de cointegración para cada subconjunto de
(𝑛 − 𝑟 + 1) variables. Por ejemplo, si hay dos vectores de cointegración en un sistema de tres
variables, hay un vector de cointegración para cada par bilateral de variables (2 = 𝑛 − 𝑟 + 1). Por
tanto, hay un vector de cointegración para cada subconjunto de tres variables. En general, la matriz 𝛽′
será una matriz r ⋅ n de parámetros de cointegración, y cada subconjunto de 𝑛 − 𝑟 + 1 variables será
cointegrado. Las operaciones estándar de fila y columna en 𝛽′ no implican restricciones en los vectores
de cointegración; simplemente resultan en vectores de cointegración adicionales que son combinaciones
lineales de los originales.
EJEMPLO 1: EXCLUSIÓN DE VARIABLES DENTRO DE UNA ECUACIÓN
Con múltiples vectores de cointegración, no puedes probar si un 𝛽ij particular es igual a cero, ya que
esta suposición no restringe el espacio de cointegración. En el caso general donde 𝛽′ es una matriz r ⋅ n,
una restricción de exclusión comprobable implica la exclusión de r o más variables de un vector de
cointegración. Por lo tanto, excluir r variables de un vector de cointegración implica solo una restricción.
Si el valor de la muestra de la estadística 𝜒2 con un grado de libertad (ya que solo hay una restricción
involucrada) supera un valor crítico, se rechaza la hipótesis nula de que este conjunto de variables
contiene una relación de cointegración.
EJEMPLO 2: EXCLUSIÓN DE VARIABLES ENTRE ECUACIONES
Supongamos que quieres probar si x4t puede ser excluido del conjunto de relaciones de cointegración.
La restricción 𝛽14 = 𝛽24 = 0 implica solo una restricción en el espacio de cointegración. En el caso
general donde 𝛽′ es una matriz r ⋅ n, la prueba 𝛽1𝑗 = 𝛽2𝑗 =···= 𝛽𝑟𝑗 = 0 aún involucra solo una
restricción. Esto se debe a que 𝑥𝑖𝑡 puede ser eliminado de 𝑟 − 1 ecuaciones usando operaciones simples
de fila y columna.
EJEMPLO 3: RESTRICCIONES CONDICIONALES
También es posible restringir un vector de cointegración condicionalmente a los valores de todos los
otros vectores de cointegración. Por ejemplo, podrías querer determinar si (1, 0, 𝛽23 , 𝛽24 )′ es un vector
de cointegración para los valores normalizados dados de 𝛽12 , 𝛽13 𝑦 𝛽14 . Cortell, Davis y Smith (1999)
consideran el problema de identificación en detalle considerable. Examinan cuatro relaciones de
comportamiento en un sistema de siete variables, y concluyen que no pudieron rechazar las restricciones
a niveles de significancia convencionales.
La primera ecuación es la ecuación de demanda de dinero. Las siguientes tres
ecuaciones son una función simple de consumo, una función de inversión y una
función de demanda de importaciones, respectivamente. Se asume que el consumo,
la inversión y las importaciones son funciones solo del ingreso y la tasa de interés.
La Prueba en Presencia de Variables I(2)
También es posible probar la multicointegración utilizando la metodología de Johansen. Se considera
un sistema VAR donde el problema de la multicointegración se refiere a los rangos tanto de 𝜋 como de
𝛤. Para ilustrar el procedimiento, se comienza con un sistema de tres variables que son
multicointegradas. Se define r como el rango de 𝜋 y 𝑟1 como el rango de Γ, de modo que si r = r1 = 1,
hay una relación de equilibrio. Si r = 0, la multicointegración falla, y si 𝑟 = 1 y 𝑟1 = 0, la relación de
equilibrio tiene una forma específica. Sin embargo, la estimación de los rangos de 𝜋 y Γ puede ser
complicada. Por ejemplo, si las variables I(2) están cointegradas de cierta manera, se debe estimar su
rango adecuadamente.
Si tomas la primera diferencia, se sigue que 𝜋11 𝛥𝑥1𝑡 + 𝜋12 𝛥𝑥2𝑡 + 𝜋13 𝛥𝑥3𝑡 es I(0). Deberías ser capaz
de identificar el problema. Para cualquier vector de cointegración en 𝜋, es posible estimar un vector de
cointegración idéntico para las primeras diferencias de las variables. Sin embargo, una combinación

lineal de las dos relaciones no es estacionaria. Si consideras el resultado obtenido al restar la relación
I(0) de la relación I(1), observarás que lo que se ha hecho es cambiar el subíndice temporal para las
variables en la relación de cointegración. La clave está en encontrar vectores de cointegración en 𝛤 que
no sean combinaciones lineales de los de 𝜋.
En el caso más general considerado por Johansen (1995), donde 𝑟𝑎𝑛𝑘(𝜋) = 𝑟 y 𝑠 denota el número de
vectores de cointegración en Γ que son ortogonales a los de 𝜋:
1. Si r = 0, no hay relación entre las variables que sea estacionaria.
2. En un sistema con n variables, si 𝑟 + 𝑠 = 𝑛 − 1, hay un vector de multicointegración único. El
número de tendencias estocásticas I(2) en un sistema de n variables se calcula como 𝑛 − 𝑟 − 𝑠.
3. El valor de s debe ser tal que 𝑠 < 𝑛 − 𝑟. Para que el análisis de las variables I(2) sea adecuado, los
valores de r y s deben satisfacer la condición de 𝑠 + 𝑟 < 𝑛. Si 𝑠 = 𝑛 − 𝑟, entonces 𝑥𝑡 no contiene
variables I(2).
La prueba de cointegración de Johansen con variables I(2) implica un proceso de dos pasos. Primero,
se estima un modelo para determinar el rango de 𝜋, utilizando estadísticas como 𝜆𝑡𝑟𝑎𝑐𝑒 y 𝜆𝑚𝑎𝑥 . Luego,
se determina el valor de s condicional al valor de 𝑟, lo que implica encontrar vectores de cointegración
en 𝛤 que sean independientes de los de 𝜋.
∗
El procedimiento implica considerar la hipótesis nula de que 𝑠 = 𝑠0 y calcular una estadística 𝑄𝑟,𝑠

Por lo tanto, se construye de la misma manera que una estadística 𝜆𝑡𝑟𝑎𝑐𝑒 . Las diferencias principales
radican en que se prueba el rango de 𝛤 condicional al valor de 𝑟 y que se obtiene el número de vectores
de cointegración ortogonales a los de 𝜋. Los valores críticos necesarios para determinar el valor de s
∗
deben ser modificados en función de r. Si el valor muestral de 𝑄𝑟,𝑠
supera el valor crítico calculado por
Johansen, se rechaza la hipótesis nula 𝑠 = 𝑠0 a favor de la alternativa 𝑠 > 𝑠0.
9. ILLUSTRATING THE JOHANSEN METHODOLOGY
Siga los cuatro pasos siguientes al implementar el procedimiento de Johansen:
PASO 1: Es una buena práctica realizar pruebas previas de todas las variables para evaluar su orden de
integración. Represente los datos para ver si es probable que haya una tendencia temporal lineal en el
proceso de generación de datos. En la mayoría de los casos, tendrá variables integradas del mismo orden.
En otros casos, puede comprobar la multicointegración. Los resultados de la prueba pueden ser bastante
sensibles a la longitud del retraso, por lo que es importante tener cuidado. El procedimiento más común
es estimar una autorregresión vectorial utilizando los datos no diferenciados. A continuación, utilice las
mismas pruebas de longitud de retardo que en un VAR tradicional. Comience con la longitud de retardo
más larga que se considere razonable y pruebe si se puede acortar. Por ejemplo, si queremos probar si
los retrasos 2 a 4 son importantes, podemos estimar los siguientes dos VAR:
𝑥𝑡 = 𝐴0 + 𝐴1 𝑥𝑡−1 + 𝐴2 𝑥𝑡−2 + 𝐴3 𝑥𝑡−3 + 𝐴4 𝑥𝑡−4 + 𝑒1𝑡
𝑋𝑡 = 𝐴0 + 𝐴1 𝑥𝑡−1 + 𝑒2𝑡
Dónde 𝑥𝑡 es el vector de variables (𝑛 ∙ 1), 𝐴0 es el vector de términos de intercepto(𝑛 ∙ 1), 𝐴𝑖 es la
matriz de coeficientes (𝑛 ∙ 𝑛), y 𝑒1𝑡 y 𝑒2𝑡 son los vectores de términos de error (𝑛 ∙ 1),
Estime el primer sistema con cuatro rezagos de cada variable en cada ecuación y llame a la matriz de
varianza/covarianza de los residuos 𝛴4 . Ahora estime la segunda ecuación usando solo un retraso de
cada variable en cada ecuación y llame a la matriz de varianza/covarianza de los residuos 𝛴1 . A pesar de
que estamos trabajando con variables no estacionarias, podemos realizar pruebas de longitud de retardo
utilizando el estadístico de la prueba de razón de verosimilitud recomendado por Sims (1980):

Siguiendo a Sims, usa la distribución 𝜒2 con grados de libertad iguales al número de restricciones de
coeficiente. Dado que cada 𝐴𝑖 tiene coeficientes 𝑛2 , restringir 𝐴2 = 𝐴3 = 𝐴4 = 0 implica
restricciones de 3𝑛2 . Alternativamente, puede seleccionar la longitud del retardo 𝑝 utilizando las
generalizaciones multivariantes del AIC o SBC. En el modelo que nos ocupa, debería encontrar que el
método de general a específico y el AIC seleccionan una longitud de retraso de 2, mientras que el SBC
selecciona una longitud de retraso de 1.
PASO 2: Estime el modelo y determine el rango de π. Muchos paquetes de software estadístico de series
temporales contienen una rutina para estimar el modelo. Aquí, basta con decir que OLS no es apropiado
porque es necesario imponer restricciones de ecuaciones cruzadas en la matriz π. En la mayoría de las
circunstancias, puede optar por estimar el modelo de tres formas: (1) con todos los elementos de 𝐴0
iguales a cero, (2) con una deriva o (3) con un término constante en el vector de cointegración.
Si pretendemos que no conocemos la forma del proceso de generación de datos, es posible que queramos
incluir un término de intersección en los vectores de cointegración, y esto se puede probar. Si seguimos
el método de general a específico y usamos una longitud de retardo de 2, el modelo estimado tiene la
forma:
Cualquier evidencia de que los errores no son ruido blanco generalmente significa que las longitudes de
retraso son demasiado cortas.
En el ejemplo que se plantea en el libro, a pesar de que el proceso real de generación de datos pueda
contener solo un vector de cointegración, las realizaciones son tales que los investigadores dispuestos a
usar el nivel de significación del 10% concluirían incorrectamente que hay dos vectores de
cointegración. No rechazar una hipótesis nula incorrecta es siempre un peligro inherente al uso de
intervalos de confianza amplios.
PASO 3: Analice los vectores de cointegración normalizados y los coeficientes de velocidad de ajuste.
Considere las siguientes pruebas:
1. La prueba de que 𝛽0 = 0 implica una restricción en un vector de cointegración; Por lo tanto,
la prueba de razón de verosimilitud tiene una distribución 𝜒 2 con un grado de libertad. El valor
calculado de 𝜒 2 = 0,011234 no es significativo a niveles convencionales. Por lo tanto, no
podemos rechazar la hipótesis nula de que 𝛽0 = 0. Por lo tanto, es posible utilizar la forma del
modelo en la que no hay ni una deriva ni una intersección en el vector de cointegración. Por lo
tanto, para aclarar la cuestión relativa al número de vectores de cointegración, sería prudente
reestimar el modelo excluyendo la constante del vector de cointegración.
2. Restringir el vector de cointegración normalizado de modo que 𝛽2 = −1 𝑦 𝛽3 = 1 implica
dos restricciones en un vector de cointegración; Por lo tanto, la prueba de razón de verosimilitud
tiene una distribución 𝜒 2 con dos grados de libertad. El valor calculado de 𝜒 2 = 0,55350 no
es significativo a niveles convencionales. Por lo tanto, no podemos rechazar la hipótesis nula
de que 𝛽2 = −1 𝑦 𝛽3 = 1.
3. Para probar la restricción conjunta β = (0,−1,−1, 1) implica las tres restricciones 𝛽0 = 0, 𝛽2 =
−1 𝑦 𝛽3 = 1. El valor calculado de 𝜒 2 con tres grados de libertad es 1,8128, por lo que el
nivel de significación es 0,612. Por lo tanto, no podemos rechazar la hipótesis nula de que el
vector de cointegración es (0,−1,−1, 1).
PASO 4: Por último, la contabilidad de la innovación y las pruebas de causalidad en el modelo de
corrección de errores de (6.62) podrían ayudar a identificar un modelo estructural y determinar si el
modelo estimado parece ser razonable. Dado que los datos simulados no tienen significado
económico, aquí no se lleva a cabo la contabilidad de la innovación.

10. ERROR-CORRECTION AND ADL TESTS
En el método de Engle–Granger, se estima la relación de equilibrio de largo plazo a partir de una
regresión de zt sobre yt o de una regresión de yt sobre zt. En el método de Johansen, todas las variables
se tratan de manera simétrica. Los métodos pueden ser usados cuando no se quiera especificar una
variable "dependiente" y un conjunto de variables "independientes". Supongamos que y t y zt están
cointegradas de orden (1, 1) y el modelo de corrección de errores (ECM) está representado de forma
reducida por (i) y (ii), luego en la relación entre los términos de error y los shocks estructurales en (iii)
(i)
(iii)
(ii)
donde 𝜀yt y 𝜀zt son las innovaciones estructurales en Δyt y Δzt, y los cij son coeficientes. Los shocks
estructurales están incorrelacionados en el sentido de que E𝜀yt𝜀zt = 0. Por ahora, supongamos que los
valores de los cij son desconocidos. Ortogonalizando los dos errores e1t = pe2t + vt donde 𝜌 es el
coeficiente de regresión y vt es la innovación. Sustituimos en la ecuación anterior y añadimos que a =
a1 – pa2 obtenemos
(iv)
El problema general es que Δzt estará correlacionado con el término de error v t, lo que genera
simultaneidad, OLS no puede utilizarse. Para que los problemas de simultaneidad e identificación
desaparezcan y que OLS sea una estrategia eficiente de estimación y prueba se deben dar las
suposiciones de que que zt es débilmente exógeno (a2 = 0) y causalmente anterior a yt (c21 = 0).
Cointegración con Exogeneidad Débil
Siguiendo a Engle, Hendry y Richard (1983), una variable x it es débilmente exógena para el conjunto
de parámetros P si la distribución marginal de x it no contiene información útil para realizar inferencias
sobre P. En un sistema cointegrado, si una variable no responde a la discrepancia de la relación de
equilibrio de largo plazo, es débilmente exógena. Por lo tanto, si el parámetro de velocidad de ajuste 𝛼i
es cero, la variable en cuestión es débilmente exógena. Se estima la ecuación no restringida
(v)
donde a partir de (iv), los coeficientes estimados son tales que 𝛽1 = 𝛼1 − 𝜌𝛼2, 𝛽2 = (𝛼1 − 𝜌𝛼2)𝛽 y 𝛽3 = 𝜌.
Dado que los coeficientes son no restringidos, esta forma del modelo se llama a menudo un rezago
distribuido autorregresivo. Si zt es débilmente exógeno (es decir, si 𝛼2 = 0), tus estimaciones de
coeficientes deberían ser tales que 𝛽1 = 𝛼1, 𝛽2 = 𝛼1𝛽 y 𝛽3 = 𝜌. Así, puedes identificar 𝛼1, 𝛽 y 𝜌 a partir
de 𝛽1, 𝛽2 y 𝛽3.
(vi)
Aunque la exogeneidad débil permite identificar el modelo, todavía
existe el problema
de probar adecuadamente para la cointegración. Dado que {yt} y {zt} son I(1), las estadísticas de prueba
de la hipótesis nula 𝛽1 = 0 y 𝛽2 = 0 en (v) no son estándar y necesitan ser tabuladas. La forma habitual
de probar la cointegración es utilizar la t-estadística para la hipótesis nula 𝛽1 = 0 si esto ocurre no hay
corrección de errores, por lo que yt no está cointegrado con zt.
Si comparas (v) con (i), puedes ver el beneficio de emplear la exogeneidad débil. Dado que (v) tendrá
una varianza más pequeña que el término de error en (i), los coeficientes de(v) pueden estimarse con
más precisión que el coeficiente de (i). Los coeficientes de yt−1 y zt−1 no están restringidos.En los
enfoques de Engle–Granger y Johansen, la llamada Restricción de Factor Común obliga a que los
cambios a corto plazo en Δyt sean una proporción constante de la desviación del período anterior
respecto al equilibrio de largo plazo.
Inferencia sobre el Vector de Cointegración: Supongamos que asumimos que la exogeneidad débil se
cumple y concluimos que las variables están cointegradas (de modo que 𝛼1 < 0 y 𝛼2 = 0).
Como tal, es posible escribir (ii) y (v) como

(vii)
(viii)

Un problema de simultaneidad existe si los regresores que aparecen en (vii) dependen del término de
error vt. La variable I(0) yt−1 − 𝛽zt−1 está pre-determinada, por lo que no hay necesidad de preocuparse
por la influencia de 𝑣𝑡 en el término de corrección de errores. El problema clave concierne a la relación
contemporánea entre Δyt y Δzt. Si Δzt no se ve afectada por innovaciones en Δyt, es apropiado realizar
inferencia sobre (vii) utilizando pruebas t y F estándar.
La ortogonalización e1t = pe2t + vt donde e2t y vt no están correlacionados, es una descomposición de
Choleski en la que 𝛥𝑧𝑡 no responde a innovaciones en Δyt pero Δyt responde a innovaciones en Δzt. Si
c21 = 0, se cumple que a e1t = 𝜌e2t + 𝜀yt y e2t = 𝜀zt. Dado que Δzt = e2t no depende de 𝜀yt, no hay
retroalimentación de Δyt a Δzt, por lo que es posible utilizar inferencia estándar en (vi) o (vii).
Finalmente, nota que 𝜌 es el coeficiente de la variable estacionaria Δzt. Por lo tanto, es apropiado
construir intervalos de confianza para 𝜌 utilizando una distribución t. Dado que zt puede ser en realidad
un vector de variables I(1), puedes estimar (v) para y t y un conjunto de variables débilmente exógenas
zt. Por ejemplo, con dos variables débilmente exógenas, z 1t y z2t, el modelo de corrección de errores se
generaliza a

donde b1 = −𝛽1/𝛼1 y b2 = −𝛽2/𝛼1. Para probar la cointegración, utiliza el estadístico t para la hipótesis
nula 𝛼1 = 0. Dado que tienes tres variables I(1) en el modelo, obtén los valores críticos de la Tabla F con
k = 3. Por supuesto, si partimos de un proceso de orden superior, se deben agregar lags adicionales de
Δyt−i, Δz1t−i y Δz2t−i a la ecuación. Al igual que en el caso de dos variables, debes asumir que Δyt no tiene
efectos contemporáneos en ningún valor de Δzi.
11. COMPARING THE THREE METHODS
En esta sección se comparan las pruebas de cointegración de Engle-Granger, Johansen y ADL utilizando
letras del Tesoro a tres meses y tasas de interés a 10 años. Como ya hemos verificado que las tasas
individuales actúan como un proceso I(1), podemos omitir el paso preliminar de realizar pruebas previas
para raíces unitarias.
1. Metodología Engle-Granger:
Dado que cada tasa actúa como un proceso de raíz unitaria, podemos comenzar estimando la relación
de equilibrio de largo plazo
𝑟𝐿𝑡 = 1.642 + 0.915𝑟𝑆𝑡 (6.71)
A continuación, probamos la estacionariedad de los residuos. Al
hacer las pruebas de rezagos se sugiere 1 rezago o 3. Si adoptamos
el SBC y utilizamos un cambio rezagado, obtenemos_
Con los valores críticos, rechazamos la hipótesis nula de no cointegración.
Dado que no hacemos ninguna suposición sobre la exogeneidad débil,
podemos usar 𝑟𝑆𝑡 como la variable del lado izquierdo:
De esta forma, la prueba de Engle-Granger también respalda el hallazgo
de cointegración ya que la regresión de los residuos produce:
2. Metodología Johansen: Sea 𝑥𝑡 el vector [𝑟𝐿𝑡 , 𝑟𝑆𝑡 ]′ . Si se estima el
VAR no restringido, el SBC sugiere un rezago de 1, y el AIC un rezago de 8, pero se usará 1 rezago por
simplicidad. Dada esta longitud de rezago, es posible estimar el modelo. El valor estimado de la matriz
𝜋∗ es tal que:

Obtenemos las raíces características 𝜆1 = 0.1295 y 𝜆2 = 0.0136. Luego, aplicamos la formula
−T ln(1 − 𝜆1) + −T ln(1 − 𝜆2), donde 𝐻0 significa no cointegración, y 𝐻1 significa 1 o 2 vectores
cointegrantes. Si el valor de la formula excede el VC, rechazamos H0, y habría por lo menos 1 vector
cointegrante. Según los valores otorgado, se rechaza H0
Normalizando el vector de cointegración con respecto al rendimiento 𝑟𝐿𝑡
𝑟𝐿𝑡 = 0.912 + 1.051𝑟𝑆𝑡
Una diferencia clave entre esta estimación de la relación de equilibrio de largo plazo y las de la prueba
de Engle-Granger es que se puede realizar una inferencia estándar sobre los coeficientes del vector
de cointegración. Si reestima el modelo que impone la restricción, y luego prueba la presencia de la
intersección, encontrará que el término constante en el vector de cointegración es muy significativo.
El punto importante es que el estadístico t de los términos de corrección de errores implica que la tasa
de largo plazo se ajusta a la discrepancia de la relación de equilibrio de largo plazo, pero la tasa de corto
plazo no. En otras palabras, 𝑟𝑠𝑡 es débilmente exógena. Como tal, las desviaciones de la relación de largo
plazo son bastante duraderas.
3. Metodología de Corrección de Errores:
Para usar esta metodología es necesario asumir que una de las variables es débilmente exógena. Si
asumimos que la tasa de corto plazo lo es, podemos estimar una ecuación de la forma:

Como no estamos tratando todas las variables simétricamente, no hay necesidad de restringir la longitud
del rezago representada por el polinomio 𝐴1 (𝐿) para que sea la misma que la de 𝐴2 (𝐿). Y según ciertas
pruebas, para este caso un rezago 6 es apropiado. Luego, se estima la ecuación:

Con los valores críticos se rechaza la hipótesis nula de que hay cointregración, por lo que se concluye
que las variables están cointegradas
4. Comparación:
En este ejemplo particular, los tres enfoques encuentran que las variables están cointegradas. Sin
embargo, el enfoque de Engle-Granger no nos permite realizar fácilmente inferencia del vector de
cointegración, pero el enfoque de Johansen nos permite concluir que dos tasas se mueven 1:1 a largo
plazo.
Si asumimos que 𝛽2 ≠ 0, es posible realizar una inferencia sobre el coeficiente de 𝑟𝑆𝑡−1 en la relación
de equilibrio de largo plazo. Reestimamos:
∆𝑟𝐿𝑡 = −0.187(0.914𝑟𝐿𝑡−1 − 𝑟𝑆𝑡−1 − 0.604) + 0.612∆𝑟𝑆𝑡 + 𝐴1 (𝐿)∆𝑟𝐿𝑡−1 + 𝐴2 (𝐿)∆𝑟𝑆𝑡−1 + 𝑣𝑡
Donde 𝛽2 (= 0,187) es el coeficiente de una variable estacionaria que tiene una distribución t estándar,
si suponemos que 𝛽1 = 𝛽2 y reestimamos:
∆𝑟𝐿𝑡 = −0.175(𝑟𝐿𝑡−1 − 𝑟𝑆𝑡−1 − 2.01) + 0.604∆𝑟𝑆𝑡 + 𝐴1 (𝐿)∆𝑟𝐿𝑡−1 + 𝐴2 (𝐿)∆𝑟𝑆𝑡−1 + 𝑣𝑡
Podemos rastrear los efectos de un shock unitario en ∆𝑟𝑆𝑡
Se podría proceder a realizar la contabilidad de la innovación estimando una ecuación de la forma
∆𝑟𝑆𝑡 = 𝐴3 (𝐿)∆𝑟𝐿𝑡 + 𝐴4 (𝐿)∆𝑟𝑆𝑡 + 𝑒2𝑡
Dónde la ecuación está en primeras diferencias ya que la ecuación ∆𝑟𝑆𝑡 no contiene un término de
corrección de errores. Y el supuesto de que ∆𝑟𝑆𝑡 es débilmente exógeno implica un ordenamiento causal
de las innovaciones.

VECTOR AUTOREGRESSIONS (STOCK Y WATSON, 2001)
Christopher Sims (1980) proporcionó un nuevo marco macroeconométrico muy prometedor: las
autorregresiones vectoriales (VAR). Una autorregresión univariante es un modelo lineal de una sola
ecuación y una sola variable en el que el valor actual de una variable se explica por sus propios valores
rezagados. Un VAR es un modelo lineal de n-ecuación y n-variable en el que cada variable se explica a
su vez por sus propios valores rezagados, más los valores actuales y pasados de las restantes 𝑛 −
1 variables. Este sencillo marco proporciona una forma sistemática de capturar dinámicas enriquecidas
en múltiples series temporales, y el conjunto de herramientas estadísticas que venía con los VAR era
fácil de usar e interpretar. Como argumentaron Sims (1980) y otros en una serie de influyentes artículos
iniciales, los VAR ofrecían la promesa de proporcionar un enfoque coherente y creíble para la
descripción de datos, la predicción, la inferencia estructural y el análisis de políticas.
En este artículo, evaluamos qué tan bien los VAR han abordado cuatro tareas macroeconométricas.
1.
2.
3.
4.

Descripción de datos
Pronóstico
Inferencia Estructural
Análisis de Políticas

Nuestra respuesta es "depende". En la descripción y previsión de datos, los VAR han demostrado ser
herramientas potentes y fiables que ahora, con razón, son de uso diario. Sin embargo, la inferencia
estructural y el análisis de políticas son inherentemente más difíciles porque requieren diferenciar entre
correlación y causalidad; Este es el "problema de identificación", en la jerga de la econometría. Este
problema no puede ser resuelto por una herramienta puramente estadística, ni siquiera una poderosa
como un VAR, sino que se requiere teoría económica o conocimiento institucional para resolver el
problema de identificación (causalidad versus correlación).
Un vistazo al interior de la caja de herramientas del VAR:
Los VAR vienen en tres variedades: de forma reducida, recursivos y estructurales.
Una forma reducida VAR expresa cada variable como una función lineal de sus propios valores
pasados, considerando los valores pasados de todas las demás variables y un término de error no
correlacionado en serie. Cada ecuación se estima mediante regresión de mínimos cuadrados ordinarios.
El número de valores rezagados a incluir en cada ecuación se puede determinar mediante varios métodos
diferentes.
Los términos de error en estas regresiones son los movimientos "sorpresa" en las variables después de
tener en cuenta sus valores pasados. Si las diferentes variables están correlacionadas entre sí, como suele
ocurrir en las aplicaciones macroeconómicas, entonces los términos de error en el modelo de forma
reducida también se correlacionan entre ecuaciones.
Un VAR recursivo construye los términos de error en cada ecuación de regresión para que no estén
correlacionados con el error en las ecuaciones anteriores. Esto se hace incluyendo juiciosamente algunos
valores contemporáneos como regresores.
Considere un VAR de tres variables, ordenado como 1) la inflación, 2) la tasa de desempleo y 3) la tasa
de interés. En la primera ecuación del VAR recursivo correspondiente, la inflación es la variable
dependiente, y los regresores son los valores rezagados de las tres variables. En la segunda ecuación, la
tasa de desempleo es la variable dependiente, y los regresores son los rezagos de las tres variables más
el valor actual de la tasa de inflación. La tasa de interés es la variable dependiente en la tercera ecuación,
y los regresores son los rezagos de las tres variables, el valor actual de la tasa de inflación más el valor
actual de la tasa de desempleo. La estimación de cada ecuación por mínimos cuadrados ordinarios
produce residuos que no están correlacionados entre ecuaciones. Evidentemente, los resultados
dependen del orden de las variables: al cambiar el orden, cambian las ecuaciones, los coeficientes y los
residuos del VAR, y hay 𝑛! VAR recursivos que representan todos los ordenamientos posibles.

Un VAR estructural utiliza la teoría económica para clasificar los vínculos contemporáneos entre las
variables. Los VAR estructurales requieren "identificar supuestos" que permitan interpretar causalmente
las correlaciones. Estos supuestos de identificación pueden involucrar todo el VAR, de modo que se
expliquen todos los vínculos causales en el modelo, o solo una ecuación, de modo que solo se identifique
un vínculo causal específico. De esta manera se producen variables instrumentales que permiten estimar
los vínculos contemporáneos mediante la regresión de variables instrumentales. El número de VAR
estructurales está limitado únicamente por la inventiva del investigador.
Poner a prueba el VAR de tres variables:
La práctica estándar en el análisis VAR es informar los resultados de las pruebas de causalidad de
Granger, las respuestas a impulsos y las descomposiciones de la varianza del error de pronóstico. Estas
estadísticas son calculadas automáticamente (o casi) por muchos paquetes econométricos (RATS,
Eviews, TSP y otros). Debido a la complicada dinámica del VAR, estas estadísticas son más informativas
que los coeficientes de regresión VAR estimados o las estadísticas R2, que normalmente no se informan.
Los estadísticos de causalidad de Granger examinan si los valores rezagados de una variable ayudan a
predecir otra variable.
Las respuestas a impulsos trazan la respuesta de los valores actuales y futuros de cada una de las
variables a un aumento de una unidad en el valor actual de uno de los errores VAR, asumiendo que este
error vuelve a cero en períodos posteriores y que todos los demás errores son iguales a cero. El
experimento mental implícito de cambiar un error mientras se mantiene constantes los demás tiene más
sentido cuando los errores no están correlacionados entre ecuaciones, por lo que las respuestas de
impulso se calculan típicamente para VAR recursivos y estructurales.
¿Qué tan bien realizan los VAR las cuatro tareas?
Pasamos ahora a una evaluación de los VAR en la realización de las cuatro tareas macroeconométricas,
destacando tanto los éxitos como las deficiencias.
Descripción de los datos:
Debido a que los VAR involucran valores actuales y retrasados de múltiples series temporales, capturan
comovimientos que no se pueden detectar en modelos univariados o bivariados. Las estadísticas estándar
de resumen del VAR, como las pruebas de causalidad de Granger, las funciones de respuesta al impulso
y las descomposiciones de la varianza, son métodos bien aceptados y ampliamente utilizados para
representar estos comovimientos. Estas estadísticas resumidas son útiles porque proporcionan objetivos
para los modelos macroeconómicos teóricos.
Por supuesto, los métodos de VAR descritos aquí tienen algunas limitaciones. Una de ellas es que los
métodos estándar de inferencia estadística (como el cálculo de errores estándar para las respuestas a
impulsos) pueden dar resultados engañosos si algunas de las variables son muy persistentes. Otra
limitación es que, sin modificaciones, los VAR estándar pasan por alto no linealidades,
heterocedasticidad condicional y desviaciones o rupturas en los parámetros.
Pronóstico:
Los VAR pequeños se han convertido en un punto de referencia con el que se juzgan los nuevos sistemas
de predicción. Sin embargo, si bien son útiles como punto de referencia, los VAR pequeños de dos o tres
variables son a menudo inestables y, por lo tanto, malos predictores del futuro (Stock y Watson, 1996).
Los sistemas de predicción VAR de última generación contienen más de tres variables y permiten que
los parámetros variables en el tiempo capturen desviaciones importantes en los coeficientes (Sims,
1993).
Sin embargo, la adición de variables al VAR crea complicaciones, ya que el número de parámetros del
VAR aumenta a medida que aumenta el cuadrado del número de variables. Desafortunadamente, los
datos de series cronológicas macroeconómicas no pueden proporcionar estimaciones fiables de todos
estos coeficientes sin más restricciones.

Una forma de controlar el número de parámetros en los modelos VAR grandes es imponer una estructura
común a los coeficientes, ejemplo: métodos bayesianos.
Inferencia Estructural:
A continuación, se presentan tres críticas importantes al modelado estructural del VAR:
1. En gran parte, los shocks, al igual que los de la regresión convencional, reflejan factores
omitidos en el modelo. Si estos factores se correlacionan con las variables incluidas, entonces
las estimaciones del VAR contendrán el sesgo de las variables omitidas.
2. En segundo lugar, las reglas de política cambian con el tiempo, y las pruebas estadísticas
formales revelan una inestabilidad generalizada en los VAR de baja dimensión (Stock y Watson,
1996). Los VAR estructurales de parámetros constantes que pasan por alto esta inestabilidad se
identifican incorrectamente.
3. En tercer lugar, las convenciones de temporización de los VAR no reflejan necesariamente la
disponibilidad de datos en tiempo real, lo que socava el método común de identificación de
restricciones basado en supuestos de tiempo.
En esta discusión, hemos distinguido cuidadosamente entre VAR recursivos y estructurales: los VAR
recursivos utilizan un método mecánico arbitrario para modelar la correlación contemporánea en las
variables, mientras que los VAR estructurales utilizan la teoría económica para asociar estas
correlaciones con relaciones causales.
A pesar de estas críticas, creemos que es posible tener supuestos identificativos creíbles en un VAR. Un
enfoque consiste en explotar el conocimiento institucional detallado. Otro ejemplo es Bernanke y Mihov
(1998), quienes utilizan un modelo del mercado de reservas para identificar los shocks de política
monetaria. Un enfoque diferente de la identificación consiste en utilizar restricciones a largo plazo para
identificar las perturbaciones.
Análisis de políticas:
A través de un VAR se pueden analizar dos tipos de políticas: las innovaciones puntuales, en las que se
mantiene la misma regla; y cambios en la regla de política. El efecto estimado de las innovaciones
puntuales es una función de las respuestas impulsivas a una innovación política, y ya se han examinado
los posibles escollos asociados a ellas. Las cosas son más difíciles si se quiere estimar el efecto de
cambiar las reglas políticas. Si las verdaderas ecuaciones estructurales implican expectativas (por
ejemplo, una curva de Phillips expectativa), entonces las expectativas dependerán de la regla de política;
por lo que, en general, todos los coeficientes del VAR dependerán de la regla. Esta es sólo una versión
de la crítica de Lucas (1976). La importancia práctica de la crítica de Lucas para este tipo de análisis de
la política del VAR es un tema de debate.
Conclusión:
Los VAR son herramientas poderosas para describir datos y generar pronósticos de referencia
multivariantes confiables. Queda trabajo técnico por hacer, sobre todo extender los VAR a dimensiones
más altas y estructuras no lineales más ricas. Sin embargo, incluso sin estas importantes extensiones, los
VAR han hecho contribuciones duraderas al conjunto de herramientas del macroeconometrista para
abordar estas dos tareas.
Los VAR estructurales pueden capturar propiedades dinámicas ricas de múltiples series temporales, pero
sus implicaciones estructurales son tan sólidas como sus esquemas de identificación. Si bien hay algunos
ejemplos de tratamientos reflexivos de la identificación en los VAR, con demasiada frecuencia en la
literatura del VAR el tema central de la identificación se maneja ignorándolo. En algunos campos de la
economía, como la economía laboral y las finanzas públicas, la identificación puede obtenerse de manera
creíble utilizando experimentos naturales que permiten extraer alguna variación exógena de una relación
que de otro modo estaría cargada de sesgo de endogeneidad y variables omitidas.
Desafortunadamente, este tipo de experimentos naturales son raros en macroeconomía. Aunque los VAR
tienen limitaciones en lo que respecta a la inferencia estructural y el análisis de políticas, también las

tienen las alternativas. Los modelos macroeconómicos de equilibrio general estocástico dinámico
calibrado son explícitos en cuanto a los vínculos causales y las expectativas, y proporcionan un marco
intelectualmente coherente para el análisis de políticas. Pero la generación actual de estos modelos no
se ajusta bien a los datos. En el otro extremo, los modelos simples de una sola ecuación, por ejemplo,
las regresiones de la inflación frente a las tasas de interés rezagadas, son fáciles de estimar y, a veces,
pueden producir buenos pronósticos. Pero si es difícil distinguir correlación y causalidad en un VAR, lo
es aún más en los modelos de una sola ecuación, que pueden, en cualquier caso, verse como una ecuación
extraída de un VAR más grande.
Si se utilizan sabiamente y se basan en el razonamiento económico y los detalles institucionales, los
VAR pueden ajustarse a los datos y, en el mejor de los casos, pueden proporcionar estimaciones
razonables de algunas conexiones causales. El desarrollo y la combinación de una buena teoría y detalles
institucionales con métodos estadísticos flexibles como los VAR deberían mantener ocupados a los
macroeconomistas hasta bien entrado el nuevo siglo.
MODELACIÓN: VAR DEL PAPER
A muchos macroeconomistas les gusta pensar que conocen las respuestas a estas y otras preguntas
similares, tal vez con un modesto rango de incertidumbre. En las siguientes dos secciones, echamos un
vistazo cuantitativo a estas y otras preguntas relacionadas utilizando varios VAR de tres variables
estimados utilizando datos trimestrales de EE. UU. sobre la tasa de inflación de precios (𝜋𝑡 ), la tasa de
desempleo (𝑢𝑡 ) y la tasa de interés (𝑅𝑡 , específicamente, la tasa de fondos federales) desde 1960: 𝐼
hasta 2000: 𝐼𝑉.2 Primero, construimos y examinamos estos modelos como una forma de mostrar el
conjunto de herramientas del VAR
En nuestro ejemplo de tres variables, consideramos dos VAR estructurales relacionados. Cada uno
incorpora un supuesto diferente que identifica la influencia causal de la política monetaria sobre el
desempleo, la inflación y las tasas de interés. La primera se basa en una versión de la "regla de Taylor",
en la que se modela que la Reserva Federal establece la tasa de interés en función de las tasas pasadas
de inflación y desempleo.5 En este sistema, la Fed establece la tasa de fondos federales R de acuerdo
con la regla
En nuestro ejemplo de tres variables, consideramos dos VAR estructurales relacionados. Cada uno
incorpora un supuesto diferente que identifica la influencia causal de la política monetaria sobre el
desempleo, la inflación y las tasas de interés. La primera se basa en una versión de la "regla de Taylor",
en la que se modela que la Reserva Federal establece la tasa de interés en función de las tasas pasadas
de inflación y desempleo. En este sistema, la Fed establece la tasa de fondos federales R de acuerdo con
la regla:

𝑟 ∗ : tasa de interés real deseada
:valores promedio de la inflación
tasa de desempleo en los últimos cuatro trimestres
valor objetivo de inflación
valor objetivo de desempleo
Esta relación se convierte en la ecuación de la tasa de interés en el VAR estructural.
La regla de Taylor es "retrospectiva" en el sentido de que la Fed reacciona a la información pasada
y
son promedios de los últimos cuatro trimestres de inflación y desempleo), y varios investigadores
han argumentado que el comportamiento de la Fed se describe más apropiadamente por el

comportamiento prospectivo. Por ello, nos planteamos otra variante del modelo en el que la Fed
reacciona a las previsiones de inflación y desempleo a cuatro trimestres vista. Esta regla de Taylor tiene
la misma forma que la regla anterior, pero con
ty
trimestres calculados a partir de la forma reducida VAR.

reemplazados por pronósticos de cuatro

¿Qué tan sensibles son los resultados a la suposición de identificación específica utilizada en este VAR
estructural, a saber, que la Fed sigue la regla de Taylor retrospectiva?
Da la casualidad de que es muy sensible. Las respuestas impulsivas en las tasas de interés reales son
muy similares bajo cualquiera de las dos reglas. Sin embargo, en el modelo prospectivo, el choque
monetario produce un aumento de 0,5 puntos porcentuales en la tasa de desempleo en el plazo de un
año, y la tasa de inflación cae bruscamente al principio, fluctúa y luego deja una disminución neta de
0,5 puntos porcentuales después de seis años. Según la regla retrospectiva, esta subida de tipos de 100
puntos básicos produce una leve desaceleración económica y un modesto descenso de la inflación dentro
de varios años; bajo la regla prospectiva, con esta misma acción la Fed obtiene una gran victoria contra
la inflación a costa de una recesión rápida y aguda.
Si la intervención es un movimiento inesperado en la tasa de interés de los fondos federales, entonces el
efecto estimado de esta política sobre las tasas futuras de inflación y desempleo se resume mediante las
funciones de respuesta al impulso. Esto puede parecer una política un tanto extraña, pero se puede
utilizar la misma mecánica para evaluar una intervención más realista, como aumentar la tasa de fondos
federales en 50 puntos básicos y mantener este aumento durante un año. Esta política puede diseñarse
en un VAR utilizando la secuencia correcta de innovaciones de política monetaria para mantener la tasa
de interés de los fondos federales en este nivel sostenido durante cuatro trimestres, teniendo en cuenta
que en el VAR, las acciones sobre las tasas de interés en trimestres anteriores afectan a las de trimestres
posteriores (Sims, 1982; Waggoner y Zha, 1999). El análisis del segundo tipo de política —un cambio
en la propia regla monetaria— es más complicado.
El análisis del segundo tipo de política —un cambio en la propia regla monetaria— es más complicado.
Una forma de evaluar a un candidato a una nueva regla de política monetaria es preguntarse cuál sería
el efecto de los shocks monetarios y no monetarios en la economía bajo la nueva regla. Dado que esta
pregunta involucra todas las perturbaciones estructurales, responderla requiere un modelo
macroeconómico completo de la determinación simultánea de todas las variables, y esto significa que
se deben especificar todos los vínculos causales en el VAR estructural. En este caso, el análisis de la
política se lleva a cabo de la siguiente manera: se estima un VAR estructural en el que se identifican
todas las ecuaciones, luego se forma un nuevo modelo reemplazando la regla de política monetaria. La
comparación de las respuestas impulsivas en los dos modelos muestra cómo el cambio en la política ha
alterado los efectos de los shocks monetarios y no monetarios en las variables del modelo.

THE DYNAMIC EFFECTS OF AGGREGATE DEMAND AND SUPPLY DISTURBANCES
(BLANCHARD-QUAH, 1989)
Abstract: Interpretamos que las fluctuaciones en el Producto Interno Bruto (PIB) y el desempleo se
deben a dos tipos de perturbaciones: las que tienen un efecto permanente en la producción y las que no
lo tienen. Las primeras las vemos como perturbaciones en la oferta, mientras que las segundas las
consideramos perturbaciones en la demanda. Las perturbaciones en la demanda tienen un efecto en
forma de montículo en la producción y el desempleo, como una imagen especular. El efecto de las
perturbaciones en la oferta en la producción aumenta gradualmente con el tiempo, alcanzando su punto
máximo después de dos años y estabilizándose después de cinco años.
Introducción: En los últimos tiempos, el PBI ya ha sido aceptado ampliamente como un proceso de
raíz unitaria, aclarando que una innovación positiva en el PBI debería llevar a revisar al alza el
pronóstico del PBI para todos los horizontes.
Al considerar esto, si solo hay un tipo de perturbación, la interpretación es clara y se puede identificar y
entender fácilmente su efecto en la economía con una representación estimada de promedio móvil
univariante. Sin embargo, si hay múltiples tipos de perturbaciones, la interpretación se complica, ya que
la respuesta económica sería una combinación de los efectos dinámicos de cada perturbación, y en este
caso se podrían imponer restricciones a priori en la respuesta del PBI a cada una de las perturbaciones
o aprovechar la información de variables macro distintas al PBI
El enfoque de este paper se basa en suponer que existen dos tipos de perturbaciones independientes (sin
correlación), se asume que ninguna tiene efecto a largo plazo en el desempleo, pero la primera
perturbación sí tiene un efecto a largo plazo en la producción (que se conocerá como perturbaciones de
oferta) y la segunda no (se conocerá como perturbaciones de demanda).
Bajo estas restricciones, las perturbaciones en la demanda causan picos en la producción y el desempleo
que desaparecen después de uno a tres años, mientras que las perturbaciones en la oferta aumentan
gradualmente la producción y pueden aumentar temporalmente el desempleo, luego disminuye
gradualmente.
Hay que tener en cuenta que, aunque la descripción dinámica es precisa, la contribución exacta de las
perturbaciones en la demanda y la oferta a las fluctuaciones en la producción no está claramente
definida. La serie de producción basada en la demanda muestra similitudes con los ciclos económicos,
pero al realizar la descomposición de varianza, las contribuciones precisas de cada tipo de perturbación
no están bien estimadas
1. Identificación
Los supuestos son:
•
•
•
•
•

Dos perturbaciones que afectan al desempleo y la producción, ambas sin efectos a largo plazo
en el desempleo, pero difieren en la producción
Perturbación con efecto a largo plazo en la producción (permanente): De Oferta
Perturbación sin efecto a largo plazo en la producción (temporal): De Demanda
Ambas perturbaciones no están correlacionadas entre sí (pueden correlacionarse en serie, pero
bajo ciertas condiciones, cada una puede representarse como una serie de rezagos de
perturbaciones no correlacionadas)
Las innovaciones en el crecimiento de la producción y el desempleo son combinaciones lineales
de estas perturbaciones.

Ahora derivamos el proceso conjunto seguido por la producción y el desempleo implicado por nuestras
suposiciones. Siendo 𝑌 el logaritmo del PBI y 𝑈 el nivel de la tasa de desempleo, así como 𝑒𝑑 y 𝑒𝑠 serían
las dos perturbaciones (d de demanda, s de supply). X será el vector (∆𝑌, 𝑈)′ y 𝑒 el vector de las
perturbaciones (𝑒𝑑 , 𝑒𝑠 )′.

Las suposiciones anteriores implican que X sigue un proceso estacionario dado por:
∞

𝑋(𝑡) = 𝐴(0) 𝑒(𝑡) + 𝐴(1)𝑒(𝑡 − 1) + ⋯ = ∑ 𝐴(𝑗)𝑒(𝑡 − 𝑗)

(1)

𝑗=0

En esta ecuación, 𝑌 y 𝑈 son expresados como rezagos distribuidos de las dos perturbaciones, 𝑒𝑑 y 𝑒𝑠 , y
como ambas son no correlacionadas, su matriz de covarianza de varianza es diagonal, por lo que se
puede asumir que es una matriz identidad por conveniencia.
El efecto contemporáneo de 𝑒 en 𝑋 se describe con 𝐴(0); los efectos rezagados son 𝐴(𝑗), 𝑗 ≥ 1. Al ser
X estacionario, las perturbaciones no tienen efecto a largo plazo en el desempleo ni en el PIB, y se
agrega una restricción de que la suma de los elementos de la primera fila de A sea cero, lo que implica
que 𝑒𝑑 no afecta el nivel de 𝑌, pues la suma de sus efectos en 𝑌 es 0: ∑∞
𝑗=0 𝑎11 (𝑗) = 0
Ahora mostramos cómo recuperar esta representación a partir de los datos. Dado que X es estacionario,
tiene una representación de media móvil de Wold:
∞

𝑋(𝑡) = 𝜈(𝑡) + 𝐶(1)𝜈(𝑡 − 1) + ⋯ = ∑ 𝐶(𝑗) 𝜈(𝑡 − 𝑗) (2)
𝑗=0

Esta representación de media móvil es única y se puede obtener primero estimando y luego invirtiendo
la representación autorregresiva vectorial de X de la manera habitual.
Al comparar las ecuaciones (1) y (2), vemos que 𝜈, el vector de innovaciones, y 𝑒, el vector de
perturbaciones originales, están relacionados por 𝜈 = 𝐴(0)𝑒, y que 𝐴(𝑗) = 𝐶(𝑗)𝐴(0), para todo j. Así
que conocer A(0) permite recuperar 𝑒 a partir de 𝜈, y obtener A(j) a partir de C(j). Asimismo, se mención
que A(0) es identificable mediante un argumento matemático y el factor Q de Choleski
En resumen, el procedimiento que realizan es el siguiente: Primero estiman una representación
autorregresiva vectorial para 𝑋, y la invierten para obtener (2). Luego, construyen la matriz 𝐴(0); y se
usa para obtener 𝐴(𝑗) = 𝐶(𝑗)𝐴(0), y 𝑒𝑡 = 𝐴(0)−1 𝜈𝑡 . Esto proporciona la producción y el desempleo
como funciones de las perturbaciones de demanda y oferta actuales y pasadas.
2. Interpretación
Interpretar los residuos como perturbaciones "estructurales" en sistemas de baja dimensionalidad
siempre es arriesgado. La interpretación de perturbaciones de oferta y demanda de los investigadores se
basa en una perspectiva keynesiana tradicional. Se proporciona un modelo simple para ilustrar estas
ideas, basado en el trabajo de Stanley Fischer (1977):
𝑌: Logaritmo de la producción
𝑁: Empleo
𝜃: Productividad
̅: Representación del pleno empleo
𝑁
𝑃: Logaritmo de nivel de precios
𝑊: Salario nominal
𝑀: Oferta monetaria
La ecuación (3) dice que la demanda agregada es una función de los saldos reales y la productividad,
(4) es la función de producción al relacionar producción, empleo y productividad, y asume retornos
constantes a escala, (5) describe el comportamiento de fijación de precios y es función del salario
nominal y la productividad, (6) caracteriza el comportamiento de fijación de salarios en la economía: el
salario se elige un período por adelantado y se establece de manera que se logre el pleno empleo
(esperado).
También debemos especificar cómo evolucionan 𝑀 y 𝜃: (a través de shocks de demanda y oferta)

̅ − 𝑁, al resolver para desempleo y crecimiento de la producción
Definiendo el desempleo 𝑈 como 𝑁
obtenemos:
∆𝑌 = 𝑒𝑑 (𝑡) − 𝑒𝑑 (𝑡 − 1) + 𝑎 ∙ (𝑒𝑠 (𝑡) − 𝑒𝑠 (𝑡 − 1)) + 𝑒𝑠 (𝑡)
𝑈 = −𝑒𝑑 (𝑡) − 𝑎 ∙ 𝑒𝑠 (𝑡)
Ambas ecuaciones cumplen con las restricciones de la ecuación (1), ya que, por rigideces nominales,
las perturbaciones en la demanda tendrán efectos temporales y a corto plazo en la producción y el
desempleo, mientras que, a largo plazo solo habrá efecto con las perturbaciones de oferta. Asimismo,
ninguna perturbación afecta a largo plazo al desempleo
Cuestionamientos:
Aunque se podría cuestionar sobre la no correlación entre las perturbaciones de oferta y demanda, el
modelo sugiere que incluso al ser son independientes, las perturbaciones en la oferta podrían afectar
directamente la demanda agregada, por lo que la independencia no limita los posibles impactos de estas
perturbaciones en la producción y el desempleo.
Además, aunque se podría pensar que las perturbaciones de demanda pueden tener efectos permanentes,
los autores argumentan que estos efectos son mínimos en comparación con los de la oferta, por lo que
los consideran transitorios y los excluyen de su análisis.
Sin embargo, si hay múltiples fuentes de perturbaciones con diversos efectos en la producción y el
desempleo, la interpretación de solo dos perturbaciones puede ser insuficiente, lo que podría complicar
la estimación al tener que considerar múltiples factores.
Tendencia y ciclo:
Después de la estimación, podemos crear dos series de producción: una que muestre solo los efectos de
las perturbaciones en la oferta, que será no estacionaria y otra que muestre solo los efectos de las
perturbaciones en la demanda, que será estacionaria. Es tentador asociar la primera serie a la
"tendencia" de la producción y la segunda al "ciclo económico". Sin embargo, esto no está justificado,
ya que las perturbaciones en la oferta también afectan tanto al ciclo económico como a la tendencia.
3. Estimación
Antes de la estimación, nos encontramos con un problema final. Nuestra representación asume que el
desempleo y la primera diferencia del logaritmo del PIB son estacionarios. Sin embargo, los datos de
EE. UU. después de la guerra sugieren un aumento gradual del desempleo y una disminución del
crecimiento del PIB desde los años 70. Esto plantea dos problemas:
1. Nuestras suposiciones básicas podrían estar equivocadas: Por ejemplo, el desempleo podría ser
no estacionario y estar afectado por perturbaciones de demanda y oferta
2. Cómo manejar la tendencia temporal aparente en el desempleo y la desaceleración aparente en
el crecimiento desde mediados de la década de 1970: Al no haber una solución específica, se
adopta un enfoque ecléctico, donde se presentan los resultados de la estimación permitiendo un
cambio en la tasa de crecimiento del PBI, y se ajusta una línea de regresión de tendencia
temporal para capturar los cambios en el desempleo
Se estimó un sistema VAR en el crecimiento real del PIB y la tasa de desempleo, permitiendo ocho
rezagos, utilizando datos desde 1950 hasta 1987. Los datos del PIB son trimestrales; y los datos
mensuales de desempleo se promedian para obtener observaciones trimestrales. Antes de estimar el
VAR, se eliminan las medias de muestra diferentes para el crecimiento del PIB y la tasa de desempleo
cuando se permite un cambio en estas variables. Se analizará solo el caso base
4. Efectos dinámicos de las perturbaciones de demanda y oferta
Se verán estos efectos en las Figuras 1 y 2, los ejes verticales en estas figuras representan
simultáneamente el logaritmo del producto y la tasa de desempleo, mientras que el eje horizontal
representa el tiempo en trimestres. Las figuras 3-6 usan bandas y ofrecen la misma información

Perturbaciones en la demanda:
Las perturbaciones en la demanda generan un aumento temporal en
la producción y el desempleo, que alcanza su punto máximo después
de dos a cuatro trimestres. Sin embargo, estos efectos desaparecen
después de unos tres a cinco años, y la respuesta en la producción es
menor si no se consideran cambios en el desempleo a largo plazo.
Permitir un cambio en la tasa de crecimiento de la producción reduce
la importancia de los cambios a largo plazo en el desempleo en las
respuestas a las perturbaciones en la demanda. Estos efectos
dinámicos son consistentes con una visión tradicional en la que los
movimientos en la demanda agregada se acumulan hasta que el ajuste
de precios y salarios lleva a la economía de vuelta al equilibrio.
Perturbaciones en la oferta:
Las perturbaciones en la oferta aumentan gradualmente la producción
con el tiempo, alcanzando su punto máximo después de unos ocho
trimestres en el caso base. Este efecto disminuye y se estabiliza
eventualmente, aunque la estimación a largo plazo es imprecisa. En
cuanto al desempleo, una perturbación positiva inicialmente lo eleva
ligeramente, pero luego se revierte después de algunos trimestres,
regresando al valor original. Estos efectos en el desempleo cesan
aproximadamente cinco años después.

Los resultados son consistentes en diferentes enfoques de análisis, pero varían en la respuesta inicial del
desempleo a las perturbaciones en la demanda, y como las rigideces nominales y reales afectan el
desempleo y la producción:
•
•

Las rigideces nominales podrían justificar por qué, frente a un incremento en la oferta (como un
aumento en la productividad), la demanda total no se eleva inicialmente lo necesario como para
equilibrar el incremento en la producción y mantener estable el desempleo.
Las rigideces de los salarios reales pueden justificar por qué los incrementos en la productividad
pueden reducir el desempleo después de algunos trimestres, lo cual perdura hasta que los
salarios reales se adaptan al nuevo nivel de productividad más alto.

Las figuras 1 y 2 analizan la relación entre el desempleo y la producción, conocida como la ley de Okun.
Bajo diferentes perturbaciones, esta relación varía, ya que mientras que las perturbaciones en la demanda
muestran una relación cercana entre la producción y el desempleo, las perturbaciones en la oferta no la
muestran.
A largo plazo, la producción se mantiene alta, pero el desempleo regresa a su nivel inicial. Esto sugiere
un coeficiente de Okun mayor para las perturbaciones en la oferta que para las perturbaciones
en la demanda.

5. Contribuciones relativas de las perturbaciones de demanda y oferta
Después de analizar los efectos dinámicos de cada tipo de perturbación, evaluamos su contribución
relativa a las fluctuaciones en producción y desempleo de dos formas, a y b:
a) Perturbaciones de la demanda y los ciclos de negocios de NBER
Se compararán las series de tiempo históricas del componente de demanda de la producción con la
cronología de los ciclos económicos del NBER (Buró Nacional de Investigación Económica) de manera
informal. Podemos formar los componentes de demanda de la estimación del proceso conjunto para
producción y desempleo, los que representan los caminos temporales de producción y desempleo sin
perturbaciones en la oferta
Al establecer las innovaciones de demanda en cero, generamos las series temporales de los
"componentes de oferta". La restricción identificativa de que las perturbaciones de demanda no tienen
efecto a largo plazo en la producción hace que el componente de demanda en el nivel de producción sea
estacionario
Las series temporales de estos componentes se presentan en las Figuras 7 a 10. Superpuestas en estas
series temporales están los picos y valles del NBER. Los picos se dibujan como líneas verticales sobre
el eje horizontal, y los valles como líneas verticales debajo del eje.

Los altibajos del componente de demanda en la producción coinciden estrechamente con los ciclos
económicos del NBER. Las recesiones de 1974-1975 y 1979-1980 se atribuyen aproximadamente por
igual a perturbaciones adversas en la oferta y la demanda, según nuestras estimaciones. Las
perturbaciones negativas en la oferta preceden a las negativas en la demanda en la recesión de 1974-75,
mientras que en la de 1979-80, una gran perturbación negativa en la oferta es seguida por una en la
demanda un año después.
Es importante notar que el componente de oferta en la producción, presentado en la Figura 7, claramente
no es una tendencia determinista. Exhibe un crecimiento más lento a finales de la década de 1950, así
como en la década de 1970.
Las Figuras 9 y 10 revelan los componentes de oferta y demanda en el desempleo. Las fluctuaciones del
desempleo por demanda se asemejan a las de la demanda en el PIB, lo que confirma nuestro hallazgo
anterior sobre las respuestas opuestas del desempleo y el crecimiento de la producción a las
perturbaciones de la demanda. El modelo indica que las perturbaciones en la oferta contribuyen
significativamente a las variaciones en el desempleo, especialmente con aumentos en la década de 1950
y durante las crisis petroleras de la década de 1970.
b) Descomposición de Varianza
El análisis previo es empírico. En este caso, se usará una evaluación estadística formal a través de las
descomposiciones de varianza de la producción y el desempleo en perturbaciones de oferta y demanda
en varios momentos.
Las Tablas 2 y 2A-C muestran cómo se distribuye la variabilidad en diferentes casos. El error de
pronóstico se descompone en la contribución de la demanda y la oferta. Los números en las tablas
indican el porcentaje de varianza del error de pronóstico atribuible a la demanda, con la contribución de

la oferta representada por el complemento a 100. Los números entre paréntesis muestran el rango de un
desvío estándar. Nuestras restricciones identificativas solo limitan la contribución de las perturbaciones
en la oferta a medida que aumenta el horizonte, dejando otros aspectos sin restricciones.

Dos conclusiones importantes se desprenden de estas tablas:
Primero, los datos no brindan una respuesta precisa sobre la contribución relativa de las
perturbaciones en la demanda y la oferta a los cambios en la producción a corto y mediano plazo.
Esto se debe a que los resultados varían según diferentes tratamientos.
Por ejemplo, en el caso base, las perturbaciones en la demanda representan el 98% de las fluctuaciones
en la producción a cuatro trimestres, y esta contribución disminuye al 79% cuando no se permite un
cambio, pero hay una tendencia temporal en el desempleo, y disminuye al 39% si no hay cambios ni
tendencia. Además, los márgenes de error estándar son bastante grandes en cada caso, lo que indica
incertidumbre en las estimaciones.
En segundo lugar, las estimaciones sobre cómo contribuyen diferentes perturbaciones al desempleo
no muestran mucha variación en los distintos enfoques de cambio y tendencia. La influencia de las
perturbaciones en la demanda, a cuatro trimestres, en las fluctuaciones del desempleo varía del 80% al
96%. En todos los casos, parece que las perturbaciones en la demanda son bastante significativas para
las fluctuaciones en el desempleo en todos los horizontes.
6. Conclusiones y extensiones
Se asumieron dos tipos de perturbaciones en la producción y el desempleo, una permanente (oferta, se
acumula durante mucho tiempo) y otra transitoria (demanda, efecto de montículo que desaparece en
unos años). Las perturbaciones en la demanda contribuyen significativamente a las fluctuaciones de
corto y mediano plazo en la producción, pero no podemos cuantificar esto con precisión debido a la falta
de datos detallados.
Por el lado de las extensiones, se recomiendan dos cosas: Primero, analizar más variables macro junto
con nuestros componentes de oferta y demanda del PIB (y los resultados preliminares ya respaldan
nuestra interpretación de los choques). Segundo, ampliar el sistema para incluir desempleo, producción,
precios y salarios, lo que nos permitirá explorar diferentes preguntas desde otra perspectiva, y ayudará
a identificar mejor las perturbaciones de oferta y demanda

